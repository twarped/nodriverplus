diff --git a/README.md b/README.md
index b15700b..8d83461 100644
--- a/README.md
+++ b/README.md
@@ -17,34 +17,30 @@ this library **depends** on [`twarped/nodriver`](https://github.com/twarped/nodr
 this library is mostly just a working proof of concept as of now, and will need some work still to really make it what I want it to be.
 
 - [ ] make timed_out_navigating vs timed_out_loading configurable in `crawl()`
-- [ ] fix stealth patch leaks (native masking)
 - [ ] ensure that links are deduped when crawling
-- [ ] fix `NodriverPlusManager` issue:
+- [ ] fix `Manager` issue:
 	- [ ] doesn't stop on ctrl+c. you have to manually terminate the process
 - [ ] add more granular tests
 - [ ] solve cloudflare checkbox
 - [ ] solve datadome puzzle slider
 - [ ] migrate low level functions from `NodriverPlus` into separate files like `tab.py` or something
 	- [ ] then attach the high level `NodriverPlus` to those functions to make it more maintainable
-- [ ] turn stealth and `scrape_bytes` logic/functionality into sharks/strings of functions that will occur in each of these events:
-	- [ ] target creation/attachment
-	- [ ] request interception
-	- [ ] response interception
-	- [ ] make sure that it can still follow redirects and stuff though
-	- [ ] and stay perfectly timed and stuff too
-- [ ] add sharks: `ScrapeRequestInterceptor` and `ScrapeResponseInterceptor`
-- [ ] turn handwritten sharks into ones using the new api
-- [ ] add option to receive bytes as stream on `ScrapeResponse` instead of a cached var
-- [ ] update `CrawlResultHandler` and `NodriverPlusManager` to handle errors and stuff
+- [ ] add target_interceptors: `ScrapeRequestInterceptor` and `ScrapeResultInterceptor`
+- [ ] turn handwritten target_interceptors into ones using the new api
+- [ ] add option to receive bytes as stream on `ScrapeResult` instead of a cached var
+- [ ] update `CrawlResultHandler` and `Manager` to handle errors and stuff
 - [ ] make `pymupdf` optional
+
 # usage
 
 example if you just want the stealth:
+
+(basically just the `hide_headless` flag since `--headless=new` passes a `Headless` token to the user agent.)
 ```python
 from nodriverplus import NodriverPlus
 
-# stealth defaults to `True`
-ndp = NodriverPlus() # `stealth=True`
+# hide_headless defaults to on
+ndp = NodriverPlus() # `hide_headless=True`
 browser = await ndp.start() # headless or headful
 
 # for a graceful shutdown: (takes longer)
@@ -82,23 +78,24 @@ await ndp.stop()
 
 example if you want to crawl:
 ```python
-from nodriverplus import NodriverPlus, ScrapeResponseHandler
+from nodriverplus import NodriverPlus, ScrapeResultHandler
 
 ndp = NodriverPlus()
 await ndp.start() # headless or headful
 
-def handle_html(response):
-    print(response.html[:500])
-def handle_links(response):
-    links_to_crawl = []
-    for link in response.links:
-        links_to_crawl.append(link)
-    print(f"found {len(links_to_crawl)} links to crawl")
-    return links_to_crawl
+class MyCustomHandler(ScrapeResultHandler):
+
+    async def html(self, response):
+        print(response.html[:500])
 
-handler = ScrapeResponseHandler(html=handle_html, links=handle_links)
+    async def links(self, response):
+        links_to_crawl = []
+        for link in response.links:
+            links_to_crawl.append(link)
+        print(f"found {len(links_to_crawl)} links to crawl")
+        return links_to_crawl
 
-result = await ndp.crawl("https://example.com", depth=2, handler=handler)
+result = await ndp.crawl("https://example.com", depth=2, handler=MyCustomHandler())
 
 await ndp.stop()
 ```
@@ -109,42 +106,41 @@ handy if you want to run crawls and stuff from an http server
 ```python
 from nodriverplus import (
     NodriverPlus,
-    NodriverPlusManager,
-    ScrapeResponseHandler,
+    Manager,
+    ScrapeResultHandler,
     CrawlResultHandler,
 )
 
 ndp = NodriverPlus()
 await ndp.start() # again, headless or headful
 
-manager = NodriverPlusManager(ndp, concurrency=2)
+manager = Manager(ndp, concurrency=2)
 manager.start()
 
 # simple result handlers
-def handle_html(response):
-    print(response.html[:500])
-def handle_crawl_result(result):
-    print(f"successfully crawled {len(result.successful_links)}")
-
-# create a new `ScrapeResponseHandler`
-scrape_response_handler = ScrapeResponseHandler(html=handle_html)
-# create a new `CrawlResultHandler`
-# special handler only used by `NodriverPlusManager` as of now
-crawl_result_handler = CrawlResultHandler(handle=handle_crawl_result)
-
-# enqueue a crawl
-await manager.enqueue_crawl("https://example.com",
-    scrape_response_handler=scrape_response_handler,
-    crawl_result_handler=crawl_result_handler
+class ScrapeHandler(ScrapeResultHandler):
+    async def html(self, response):
+        print(response.html[:500])
+
+class CrawlHandler(CrawlResultHandler):
+    async def handle(self, result):
+        print(f"successfully crawled {len(result.successful_links)}")
+
+# enqueue a crawl (returns `None` immediately)
+manager.enqueue_crawl("https://example.com",
+    scrape_result_handler=ScrapeHandler(),
+    crawl_result_handler=CrawlHandler()
 )
-# enqueue another crawl
-await manager.enqueue_crawl("https://example.com",
-    scrape_response_handler=scrape_response_handler,
-    crawl_result_handler=crawl_result_handler
+
+# enqueue another crawl (same here)
+manager.enqueue_crawl("https://example.com",
+    scrape_result_handler=ScrapeHandler(),
+    crawl_result_handler=CrawlHandler()
 )
-# enqueue a scrape
-await manager.enqueue_scrape("https://example.com",
-    scrape_response_handler=scrape_response_handler
+
+# enqueue a scrape (returns `None` immediately)
+manager.enqueue_scrape("https://example.com",
+    scrape_result_handler=ScrapeHandler()
 )
 
 # optional:
@@ -159,13 +155,4 @@ unfinished_queue = await manager.stop()
 save_queue_somehow(unfinished_queue)
 
 await ndp.stop()
-```
-
-
-# usage theories
-
-you could probably execute some complicated crawls using the link handler.
-
-for example, you could set up a pre-picked list of links or link generator to crawl and feed it to the `links` method of the `ScrapeResponseHandler` with `crawl(depth=float('inf'), concurrency=1)` or something
-
-or maybe using `ScrapeResponse.tab` in an async `ScrapeResponseHandler` method to execute javascript and stuff in a tab before continuing the current link tree
\ No newline at end of file
+```
\ No newline at end of file
diff --git a/examples/basic_crawl.py b/examples/basic_crawl.py
index 80e1bc4..b438e19 100644
--- a/examples/basic_crawl.py
+++ b/examples/basic_crawl.py
@@ -1,10 +1,10 @@
 import asyncio
-from nodriverplus import NodriverPlus, ScrapeResponseHandler
+from nodriverplus import NodriverPlus, ScrapeResultHandler
 
 START_URL = "https://example.com"
 
 # simple handler that prints html preview and returns links unchanged
-class ExampleHandler(ScrapeResponseHandler):
+class ExampleHandler(ScrapeResultHandler):
     async def html(self, response):  # type: ignore[override]
         print(response.html[:500])
     async def links(self, response):  # type: ignore[override]
@@ -16,7 +16,7 @@ async def main():
     ndp = NodriverPlus()
     await ndp.start()
     handler = ExampleHandler()
-    result = await ndp.crawl(START_URL, depth=2, handler=handler)
+    result = await ndp.crawl(START_URL, depth=2, scrape_result_handler=handler)
     print(f"crawl finished pages={len(result.successful_links)} failed={len(result.failed_links)}")
     await ndp.stop()
 
diff --git a/examples/manager_concurrent.py b/examples/manager_concurrent.py
index e54b2fe..febd10a 100644
--- a/examples/manager_concurrent.py
+++ b/examples/manager_concurrent.py
@@ -2,19 +2,18 @@ import asyncio
 import logging
 from nodriverplus import (
     NodriverPlus,
-    NodriverPlusManager,
-    ScrapeResponseHandler,
+    ScrapeResultHandler,
     CrawlResultHandler,
 )
 
 START_URL = "https://example.com"
 
-class HtmlPreviewHandler(ScrapeResponseHandler):
-    async def html(self, response):  # type: ignore[override]
+class HtmlPreviewHandler(ScrapeResultHandler):
+    async def handle(self, response):
         print(response.html[:120])
 
 class ResultPrinter(CrawlResultHandler):
-    async def handle(self, result):  # type: ignore[override]
+    async def handle(self, result):
         print(f"successfully crawled {len(result.successful_links)} page(s)")
 
 async def main():
@@ -22,24 +21,22 @@ async def main():
     ndp = NodriverPlus()
     await ndp.start()
 
-    manager = NodriverPlusManager(ndp, concurrency=2)
-    manager.start()
-
     html_handler = HtmlPreviewHandler()
     result_handler = ResultPrinter()
 
     # enqueue two crawls and a scrape
-    await manager.enqueue_crawl(START_URL, scrape_response_handler=html_handler, crawl_result_handler=result_handler)
-    await manager.enqueue_crawl(START_URL, scrape_response_handler=html_handler, crawl_result_handler=result_handler)
-    await manager.enqueue_scrape(START_URL, scrape_response_handler=html_handler)
+    ndp.enqueue_crawl(START_URL, scrape_result_handler=html_handler, crawl_result_handler=result_handler)
+    ndp.enqueue_crawl(START_URL, scrape_result_handler=html_handler, crawl_result_handler=result_handler)
+    ndp.enqueue_scrape(START_URL, scrape_result_handler=html_handler)
 
     # wait for queue to drain
-    await manager.wait_for_queue()
+    await ndp.wait_for_queue()
     # stop manager and collect unfinished jobs (should be empty)
-    leftover = await manager.stop()
+    leftover = await ndp.stop_manager()
     if leftover:
         print(f"unfinished {len(leftover)} jobs persisted")
 
+    # ndp.stop() also stops the manager, but you get the idea
     await ndp.stop()
 
 if __name__ == "__main__":
diff --git a/examples/single_scrape.py b/examples/single_scrape.py
index cab7e77..0d68fdf 100644
--- a/examples/single_scrape.py
+++ b/examples/single_scrape.py
@@ -1,12 +1,12 @@
 import asyncio
 from nodriverplus import NodriverPlus
 
-TARGET = "https://example.com"
+URL = "https://example.com"
 
 async def main():
     ndp = NodriverPlus()
     await ndp.start()
-    resp = await ndp.scrape(TARGET)
+    resp = await ndp.scrape(URL)
     # print a preview of the document
     print(resp.html[:500])
     await ndp.stop()
diff --git a/examples/stealth_only.py b/examples/stealth_only.py
index 623e082..8cd0be9 100644
--- a/examples/stealth_only.py
+++ b/examples/stealth_only.py
@@ -3,7 +3,7 @@ from nodriverplus import NodriverPlus
 
 # basic stealth startup + shutdown
 async def main():
-    ndp = NodriverPlus()  # stealth defaults to True
+    ndp = NodriverPlus()  # hide_headless defaults to True
     # start browser (headless False by default for now) - adjust as needed
     await ndp.start()
     # do nothing - just demonstrate bring-up
diff --git a/pyproject.toml b/pyproject.toml
index 3d54d0b..9b45730 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -1,7 +1,7 @@
 [project]
 name = "nodriverplus"
-version = "0.1.0"
-description = "Add your description here"
+version = "0.2.0"
+description = "an advanced scraper and crawler framework built for nodriver"
 readme = "README.md"
 authors = [
     { name = "twarped", email = "samthebeastofall@gmail.com" }
@@ -11,10 +11,17 @@ dependencies = [
     "beautifulsoup4>=4.13.5",
     "html-to-markdown>=1.9.0",
     "nodriver",
-    "pymupdf>=1.26.4",
+    "opencv-python-headless>=4.12.0.88",
+    "urlcanon>=0.3.1",
+]
+
+[project.optional-dependencies]
+testing = [
     "pytest>=8.4.1",
     "pytest-asyncio>=1.1.0",
-    "urlcanon>=0.3.1",
+]
+pdfs = [
+    "pymupdf>=1.26.4",
 ]
 
 [build-system]
@@ -30,13 +37,13 @@ asyncio_mode = "auto"
 testpaths = ["tests"]
 pythonpath = "src"
 markers = [
-    "suite: tests that check real world scenarios",
+    "integration: tests that check real world scenarios",
     "browser: tests that run a nodriver browser instance",
     "network: tests that require network access",
     "manager: tests that require the nodriver manager",
     "crawl: tests that involve `crawl()`",
     "scrape: tests that involve `scrape()`",
-    "bytes_: tests that involve scraped bytes",
+    # "bytes_: tests that involve scraped bytes",
     "stealth: tests that involve stealth",
     "markdown: tests that involve Markdown conversion",
 ]
@@ -45,4 +52,4 @@ filterwarnings = [
     "ignore:.*SwigPyPacked has no __module__ attribute:DeprecationWarning",
     "ignore:.*SwigPyObject has no __module__ attribute:DeprecationWarning",
     "ignore:.*swigvarlink has no __module__ attribute:DeprecationWarning",
-]
\ No newline at end of file
+]
diff --git a/src/nodriverplus/__init__.py b/src/nodriverplus/__init__.py
index 2c9b4fa..ffd0c38 100644
--- a/src/nodriverplus/__init__.py
+++ b/src/nodriverplus/__init__.py
@@ -1,28 +1,76 @@
-from nodriverplus.core.nodriverplus import NodriverPlus
-from nodriverplus.core.manager import NodriverPlusManager
-from nodriverplus.core.user_agent import UserAgent, UserAgentMetadata
-from nodriverplus.core.scrape_response import (
+from .core.nodriverplus import NodriverPlus
+from .core.manager import Manager, ManagerJob
+from .core.user_agent import UserAgent
+from .core.scrape_result import (
     CrawlResult, 
     CrawlResultHandler, 
-    ScrapeResponse, 
-    ScrapeResponseHandler, 
-    ScrapeResponseIntercepted, 
-    ScrapeRequestIntercepted
+    ScrapeResult, 
+    ScrapeResultHandler, 
+    InterceptedResponseMeta, 
+    InterceptedRequestMeta
+)
+from .core.handlers import (
+    NetworkWatcher,
+    TargetInterceptor,
+    TargetInterceptorManager,
+    UserAgentPatch,
+    WindowSizePatch,
+    CloudflareSolver,
 )
 from . import utils
 import nodriver
+from nodriver import cdp
+from .core.browser import (
+    get,
+    get_with_timeout,
+    stop,
+)
+from .core.tab import (
+    wait_for_page_load,
+    get_user_agent,
+    crawl,
+    scrape,
+    click_template_image,
+)
+from .core.cdp_helpers import (
+    TARGET_DOMAINS,
+    assert_domain,
+    can_use_domain,
+    domains_for,
+    target_types_for
+)
 
 __all__ = [
     "CrawlResult",
     "CrawlResultHandler",
     "nodriver",
+    "cdp",
     "NodriverPlus",
-    "NodriverPlusManager",
+    "Manager",
+    "ManagerJob",
     "UserAgent",
-    "UserAgentMetadata",
-    "ScrapeResponse",
-    "ScrapeResponseHandler",
-    "ScrapeResponseIntercepted",
-    "ScrapeRequestIntercepted",
+    "ScrapeResult",
+    "ScrapeResultHandler",
+    "InterceptedResponseMeta",
+    "InterceptedRequestMeta",
+    "NetworkWatcher",
+    "TargetInterceptor",
+    "TargetInterceptorManager",
+    "UserAgentPatch",
+    "WindowSizePatch",
+    "CloudflareSolver",
     "utils",
+    "get",
+    "get_with_timeout",
+    "stop",
+    "wait_for_page_load",
+    "get_user_agent",
+    "crawl",
+    "scrape",
+    "click_template_image",
+    "TARGET_DOMAINS",
+    "assert_domain",
+    "can_use_domain",
+    "domains_for",
+    "target_types_for"
 ]
\ No newline at end of file
diff --git a/src/nodriverplus/core/browser.py b/src/nodriverplus/core/browser.py
new file mode 100644
index 0000000..6de348d
--- /dev/null
+++ b/src/nodriverplus/core/browser.py
@@ -0,0 +1,173 @@
+import logging
+import asyncio
+import nodriver
+import time
+from datetime import timedelta
+from .scrape_result import (
+    ScrapeResult, 
+)
+
+logger = logging.getLogger("nodriverplus.browser")
+
+async def get(
+    base: nodriver.Tab | nodriver.Browser,
+    url: str = "about:blank",
+    *,
+    new_tab: bool = False,
+    new_window: bool = True,
+    new_context: bool = True,
+    dispose_on_detach: bool = True,
+    proxy_server: str = None,
+    proxy_bypass_list: list[str] = None,
+    origins_with_universal_network_access: list[str] = None,
+) -> nodriver.Tab:
+    """central factory for new/reused tabs/windows/contexts.
+
+    honors combinations of `new_window`/`new_tab`/`new_context` on `base`.
+    
+    see https://github.com/twarped/nodriver/commit/1dcb52e8063bad359a3f2978b83f44e20dfbca68
+
+    - **`dispose_on_detach`** — (EXPERIMENTAL) (Optional) If specified, disposes this context when debugging session disconnects.
+    - **`proxy_server`** — (EXPERIMENTAL) (Optional) Proxy server, similar to the one passed to --proxy-server
+    - **`proxy_bypass_list`** — (EXPERIMENTAL) (Optional) Proxy bypass list, similar to the one passed to --proxy-bypass-list
+    - **`origins_with_universal_network_access`** — (EXPERIMENTAL) (Optional) An optional list of origins to grant unlimited cross-origin access to. Parts of the URL other than those constituting origin are ignored.
+
+    :param base: existing tab or browser (defaults to browser root).
+    :param url: initial navigation (about:blank by default).
+    :param new_window: request a separate window (may create context).
+    :param new_tab: request a new tab in existing window/context.
+    :param new_context: create an isolated context when opening window.
+    :param dispose_on_detach: (EXPERIMENTAL) (Optional) If specified, disposes this context when debugging session disconnects.
+    :param proxy_server: (EXPERIMENTAL) (Optional) Proxy server, similar to the one passed to --proxy-server
+    :param proxy_bypass_list: (EXPERIMENTAL) (Optional) Proxy bypass list, similar to the one passed to --proxy-bypass-list
+    :param origins_with_universal_network_access: (EXPERIMENTAL) (Optional) An optional list of origins to grant unlimited cross-origin access to. Parts of the URL other than those constituting origin are ignored.
+    :return: acquired/created tab
+    :rtype: Tab
+    """
+
+    # create new browser context/window with a proxy if specified
+    if (new_window and new_context or proxy_server):
+        if isinstance(base, nodriver.Tab):
+            base = base.browser
+        if proxy_server:
+            new_window = True
+        tab = await base.create_context(
+            url=url,
+            new_tab=new_tab,
+            new_window=new_window,
+            dispose_on_detach=dispose_on_detach,
+            proxy_server=proxy_server,
+            proxy_bypass_list=proxy_bypass_list,
+            origins_with_universal_network_access=origins_with_universal_network_access,
+        )
+        return tab
+    # new standalone window without context
+    if new_window and isinstance(base, nodriver.Browser):
+        return await base.get(url, new_tab=False, new_window=True)
+    # open new tab off the browser root
+    if new_tab and isinstance(base, nodriver.Browser):
+        return await base.get(url, new_tab=True)
+    # base is a tab:
+    return await base.get(url, new_tab=new_tab, new_window=new_window)
+
+
+async def get_with_timeout( 
+    target: nodriver.Tab | nodriver.Browser, 
+    url: str, 
+    *,
+    navigation_timeout = 30,
+    wait_for_page_load_ = True,
+    page_load_timeout = 60,
+    extra_wait_ms = 0,
+    new_tab = False,
+    new_window = False
+):
+    """navigate with separate navigation + load timeouts.
+
+    returns a partial ScrapeResult (timing + timeout flags + tab ref)
+
+    :param tab: existing tab or browser root.
+    :param url: target url.
+    :param navigation_timeout: seconds for navigation phase.
+    :param wait_for_page_load_: whether to wait for load event.
+    :param page_load_timeout: seconds for load phase.
+    :param extra_wait_ms: post-load wait for dynamic content.
+    :param new_tab: request new tab first.
+    :param new_window: request new window/context first.
+    :return: partial ScrapeResult (timing + timeout flags).
+    :rtype: ScrapeResult
+    """
+    result = ScrapeResult(url=url, tab=target, timed_out=True)
+
+    start = time.monotonic()
+    # prepare target first when we need a new tab/window/context
+    base = target
+    if new_tab or new_window:
+        try:
+            base = await get(
+                target,
+                "about:blank",
+                new_window=new_window,
+                new_tab=new_tab,
+                new_context=new_window,
+            )
+        except Exception:
+            logger.exception("failed acquiring tab; falling back to provided target")
+    nav_task = asyncio.create_task(base.get(url))
+    try:
+        # cancelling nav_task will cause throw an InvalidStateError
+        # if the Transaction hasn't finished yet
+        base = await asyncio.wait_for(asyncio.shield(nav_task), timeout=navigation_timeout)
+        result.tab = base
+    except asyncio.TimeoutError:
+        result.timed_out_navigating = True
+        result.elapsed = timedelta(seconds=time.monotonic() - start)
+        logger.warning("timed out getting %s (navigation phase) (elapsed=%.2fs)", url, result.elapsed.total_seconds())
+        return result
+
+    if wait_for_page_load_:
+        # avoid circular import at module import time by importing locally
+        from .tab import wait_for_page_load
+        load_task = asyncio.create_task(wait_for_page_load(base, extra_wait_ms))
+        try:
+            # same thing here
+            await asyncio.wait_for(
+                asyncio.shield(load_task), 
+                timeout=page_load_timeout + extra_wait_ms / 1000
+            )
+        except asyncio.TimeoutError:
+            result.timed_out_loading = True
+            result.elapsed = timedelta(seconds=time.monotonic() - start)
+            logger.warning("timed out getting %s (load phase) (elapsed=%.2fs)", url, result.elapsed.total_seconds())
+            # wait for task to actually cancel
+            if not load_task.done():
+                load_task.cancel()
+                try:
+                    await load_task
+                except asyncio.CancelledError:
+                    pass
+            return result
+
+    result.timed_out = False
+    result.elapsed = timedelta(seconds=time.monotonic() - start)
+    return result
+
+
+async def stop(browser: nodriver.Browser, graceful = True):
+    """stop browser process (optionally wait for graceful exit).
+
+    :param graceful: wait for underlying process to exit.
+    """
+    logger.info("stopping browser")
+    browser.stop()
+    if graceful and browser._process is not None:
+        logger.info("waiting for graceful shutdown")
+        await browser._process.wait()
+    logger.info("successfully shutdown browser")
+
+
+__all__ = [
+    "get",
+    "get_with_timeout",
+    "stop",
+]
\ No newline at end of file
diff --git a/src/nodriverplus/core/cdp_helpers.py b/src/nodriverplus/core/cdp_helpers.py
index 8e077f7..77be87e 100644
--- a/src/nodriverplus/core/cdp_helpers.py
+++ b/src/nodriverplus/core/cdp_helpers.py
@@ -32,6 +32,9 @@ TARGET_DOMAINS_RAW: dict[str, list[str] | str] = {
         "DOMDebugger", "DOMSnapshot", "LayerTree",
         "Debugger",
     ],
+    "dedicated_worker": [
+        "Runtime", "Debugger", "Log", "Profiler",
+    ],
     "worker": [
         "Runtime", "Debugger", "Log", "Profiler",
     ],
@@ -104,4 +107,12 @@ def target_types_for(domain: str) -> list[str]:
 def assert_domain(target_type: str, domain: str) -> None:
     """raise ValueError if `domain` isn't available on `target_type`."""
     if not can_use_domain(target_type, domain):
-        raise ValueError(f"{target_type!r} cannot call {domain}.enable()")
\ No newline at end of file
+        raise ValueError(f"{target_type!r} cannot call {domain}.enable()")
+    
+__all__ = [
+    "TARGET_DOMAINS",
+    "domains_for",
+    "can_use_domain",
+    "target_types_for",
+    "assert_domain",
+]
\ No newline at end of file
diff --git a/src/nodriverplus/core/cloudflare.py b/src/nodriverplus/core/cloudflare.py
deleted file mode 100644
index 93b9e82..0000000
--- a/src/nodriverplus/core/cloudflare.py
+++ /dev/null
@@ -1,18 +0,0 @@
-import re
-
-CHECKBOX_MARKERS: list[re.Pattern] = [
-    re.compile(r"/cdn-cgi/challenge-platform/h/", re.IGNORECASE),
-    re.compile(r"<title>Just a moment\.\.\.</title>", re.IGNORECASE),
-]
-
-def should_wait(html: str):
-    """quick heuristic: does the html look like a cloudflare waiting / challenge page?
-
-    returns True if we detect well-known marker strings that usually show up while
-    a browser is being presented the *"just a moment"* interstitial (e.g. hcaptcha / turnstile / js challenge).
-
-    :param html: raw html of the page we just loaded.
-    :return: bool indicating a probable cf challenge state.
-    :rtype: bool
-    """
-    return any(marker.search(html) for marker in CHECKBOX_MARKERS)
\ No newline at end of file
diff --git a/src/nodriverplus/core/handlers/__init__.py b/src/nodriverplus/core/handlers/__init__.py
new file mode 100644
index 0000000..8dcd123
--- /dev/null
+++ b/src/nodriverplus/core/handlers/__init__.py
@@ -0,0 +1,21 @@
+from .target_intercepted import (
+    TargetInterceptor,
+    TargetInterceptorManager,
+    NetworkWatcher,
+)
+from .stock import (
+    UserAgentPatch,
+    # ScrapeRequestPausedHandler
+    WindowSizePatch,
+    CloudflareSolver,
+)
+
+__all__ = [
+    "UserAgentPatch",
+    "TargetInterceptor",
+    "TargetInterceptorManager",
+    "NetworkWatcher",
+    "WindowSizePatch",
+    # "ScrapeRequestPausedHandler",
+    "CloudflareSolver",
+]
\ No newline at end of file
diff --git a/src/nodriverplus/core/handlers/request_paused.py b/src/nodriverplus/core/handlers/request_paused.py
new file mode 100644
index 0000000..ba7e50d
--- /dev/null
+++ b/src/nodriverplus/core/handlers/request_paused.py
@@ -0,0 +1,671 @@
+"""
+# **TODO/NOTE:**
+intercepting requests this way for some reason stops sandboxed iframes from
+being able to run scripts. with this enabled, Cloudflare will never solve if
+you intercept the main page. (definitely an issue for PDF scraping)
+
+fetch pause interception helpers (request + auth) layered over raw `nodriver` events.
+
+adds structured async hooks around `Fetch.requestPaused` / `Fetch.authRequired` so callers can:
+- inspect / mutate request + response phases (fail, fulfill, stream, continue)
+- stream binary bodies prior to chrome consumption (e.g. pdf) then re-serve via fulfill
+- inject auth challenge responses (default or credentials) before continuing
+- stage mutations directly on the event object (`ev`) which is threaded across the decision tree
+
+design notes:
+- `ev` is treated as shared mutable state (body, headers, auth_challenge_response, etc.)
+- predicates (`should_*`) are small override points to steer control flow
+- separation of streaming (`take_response_body_as_stream`) from fulfillment keeps memory + clarity
+- tasks set prevents racy shutdown by awaiting outstanding async handler work
+"""
+
+from __future__ import annotations
+
+import base64
+import logging
+import asyncio
+from dataclasses import dataclass, field, asdict
+
+import nodriver
+from nodriver import cdp
+
+logger = logging.getLogger("nodriverplus.RequestPausedHandler")
+
+
+@dataclass
+class NavigationContext:
+    """navigation context for a logical request chain.
+
+    attributes:
+    - current: the initial URL for this request chain (first hop)
+    - chain: redirect targets encountered (in order)
+    - final: final resolved URL once a non-redirect main response completes
+    - done: flag indicating the main navigation finished
+    """
+    current: str
+    chain: list[str] = field(default_factory=list)
+    final: str | None = None
+    done: bool = False
+
+
+@dataclass
+class RequestMeta:
+    """typed per-event interception metadata.
+
+    fields:
+    - request_url: url (may update across hops)
+    - nav_request_id: cdp request id
+    - is_main_candidate: set during request phase when it might become main nav
+    - is_main: resolved main navigation flag (response phase)
+    - is_redirect: this event is a redirect (3xx)
+    - redirect_target: resolved redirect location (absolute)
+    - final_main: final non-redirect main navigation completed
+    - streamed: response body captured via streaming path
+    """
+    request_url: str
+    nav_request_id: str
+    is_main_candidate: bool = False
+    is_main: bool = False
+    is_redirect: bool = False
+    redirect_target: str | None = None
+    final_main: bool = False
+    streamed: bool = False
+
+    @classmethod
+    def from_request(cls, ev: cdp.fetch.RequestPaused) -> "RequestMeta":
+        """create a RequestMeta initialized from a request-paused event.
+
+        copies the current `ev.request.url` and `ev.request_id` into a new
+        RequestMeta instance so callers can attach it to `ev.meta`.
+        """
+        return cls(request_url=ev.request.url, nav_request_id=ev.request_id)
+
+    def mark_main_candidate(self, flag: bool):
+        """mark whether this paused request is a main-navigation candidate.
+
+        flag - True when the URL appears to be the main navigation for this chain.
+        """
+        self.is_main_candidate = flag
+
+    def mark_redirect(self, target: str):
+        """record a redirect hop for this event and set the redirect target.
+
+        target - absolute redirect destination (typically urljoin result).
+        """
+        self.is_redirect = True
+        self.redirect_target = target
+
+    def mark_final_main(self):
+        """mark this event as the final non-redirect main navigation.
+
+        this flips both `final_main` and `is_main` so downstream logic can
+        decide to stream or purge context reliably.
+        """
+        self.final_main = True
+        self.is_main = True
+
+    def mark_streamed(self):
+        """indicate the response body has been captured via streaming.
+
+        used to avoid double-processing the body on races.
+        """
+        self.streamed = True
+
+    def to_dict(self):
+        """return a shallow dict representation for logging or serialization."""
+        return asdict(self)
+
+
+class RequestPausedHandler:
+    """
+    # **TODO/NOTE:**
+    for some reason, intercepting requests triggers this error in sandboxed iframes:
+    
+    `Blocked script execution in 'about:blank' because the document's 
+    frame is sandboxed and the 'allow-scripts' permission is not set.`
+
+    this triggers cloudflare so that you won't ever receive a cf_clearance token.
+    definitely needs fixed.
+
+    # overview:
+    orchestrates request/response interception for a single tab.
+
+    **note**: can remove the `Range` header from requests in `_annotate_request_navigation()`
+
+    lifecycle (request phase):
+    1. `on_request(ev)` - inspect / prep state
+    2. predicates in order: fail -> fulfill -> stream -> continue
+    3. optional stream capture mutates `ev.body` then fulfill
+
+    response phase:
+    - `on_response(ev)` then `continue_response(ev)` (override to rewrite headers/status)
+
+    mutation contract:
+    - `ev` is mutated in-place (body, headers, etc.) and passed downstream
+    - overrides should avoid replacing `ev` wholesale; adjust fields so later steps see changes
+
+    extension points:
+    - override `should_*` predicates for custom logic
+    - override `handle_response_body` for post-stream transformations
+
+    concurrency:
+    - each intercepted event scheduled as a Task; `wait_for_tasks()` drains them on shutdown
+    """
+    # TODO:
+    # i'm not sure how `binary_response_headers` work,
+    # so some of that might need refactoring later on.
+
+    tab: nodriver.Tab
+    tasks: set[asyncio.Task]
+    remove_range_header: bool
+
+    nav_contexts: dict[str, NavigationContext]
+
+    def __init__(self, tab: nodriver.Tab, remove_range_header = True):
+        self.tab = tab
+        self.remove_range_header = remove_range_header
+        self.tasks = set()
+        # per-requestId navigation contexts (one logical chain per active id)
+        # nav_contexts[request_id] = NavigationContext
+        self.nav_contexts = {}
+        # single lock is fine for tiny critical sections; keeps races away
+        self._nav_lock = asyncio.Lock()
+
+
+    # helper for tracking request redirects
+    async def _annotate_request_navigation(self, ev: cdp.fetch.RequestPaused):
+        async with self._nav_lock:
+            ctx = self.nav_contexts.get(ev.request_id)
+            if ctx is None:
+                # first hop for this logical chain
+                ctx = NavigationContext(current=ev.request.url)
+                self.nav_contexts[ev.request_id] = ctx
+        meta = getattr(ev, "meta", None)
+        if not isinstance(meta, RequestMeta):
+            meta = RequestMeta.from_request(ev)
+            ev.meta = meta
+        meta.request_url = ev.request.url
+        meta.nav_request_id = ev.request_id
+        meta.mark_main_candidate(ev.request.url == ctx.current and not ctx.done)
+        # remove Range header to avoid servers sending 206 partial responses
+        if self.remove_range_header:
+            for k in list(ev.request.headers.keys()):
+                if k.lower() == "range":
+                    ev.request.headers.pop(k, None)
+
+
+
+    # helper for tracking response redirects
+    async def _annotate_response_navigation(self, ev: cdp.fetch.RequestPaused) -> bool:
+        """annotate redirect + main flags for this requestId; True when redirect handled.
+
+        per-event meta keys:
+        - is_main
+        - is_redirect
+        - redirect_target
+        - final_main
+        - nav_request_id
+        also sets meta['purge_nav_ctx'] when final main completes so caller can delete context.
+        """
+        meta = getattr(ev, "meta", None)
+        if not isinstance(meta, RequestMeta):
+            meta = RequestMeta.from_request(ev)
+            ev.meta = meta
+        async with self._nav_lock:
+            ctx = self.nav_contexts.get(ev.request_id)
+            if ctx is None:
+                # late response without prior request phase (edge) create minimal context
+                ctx = NavigationContext(current=ev.request.url)
+                self.nav_contexts[ev.request_id] = ctx
+        status = ev.response_status_code or 0
+        location = None
+        if ev.response_headers:
+            for h in ev.response_headers:
+                if h.name.lower() == "location":
+                    location = h.value
+                    break
+        is_redirect = 300 <= status < 400
+        is_main = (ev.request.url == ctx.current and not ctx.done) or meta.is_main_candidate
+        meta.is_main = bool(is_main)
+        meta.is_redirect = bool(is_redirect)
+        meta.redirect_target = None
+        meta.final_main = False
+        meta.nav_request_id = ev.request_id
+
+        if is_main and is_redirect and location:
+            from urllib.parse import urljoin
+            redirect_target = urljoin(ev.request.url, location)
+            meta.mark_redirect(redirect_target)
+            async with self._nav_lock:
+                ctx.chain.append(redirect_target)
+            return True
+        if is_main and not is_redirect and not ctx.done:
+            async with self._nav_lock:
+                ctx.final = ev.request.url
+                ctx.done = True
+            meta.mark_final_main()
+        return False
+
+
+    async def on_request(self, ev: cdp.fetch.RequestPaused):
+        """invoked for every paused network request phase prior to a response.
+
+        purpose:
+        - inspect / mutate the intercepted request event (`ev`) before deciding a control path
+        - lightweight hook for logging / future header tweaks
+
+        mutation note:
+        `ev` is a live event object that may be mutated downstream across handler steps
+        (e.g. body injection, auth challenge response). we treat it as state passed through
+        the decision tree.
+
+        :param ev: cdp.fetch.RequestPaused interception event (MUTATED IN-PLACE ACROSS FLOW).
+        """
+        pass
+
+
+    async def should_fail_request(self, ev: cdp.fetch.RequestPaused) -> bool:
+        """predicate deciding whether to abort the request.
+
+        return True to short-circuit and send a fail_request; default False keeps it flowing.
+
+        :param ev: interception event (mutable) used for decision logic.
+        :return: bool flag to trigger fail_request.
+        """
+        return False
+    
+
+    async def fail_request(self, ev: cdp.fetch.RequestPaused):
+        """send a Fetch.failRequest CDP command for the paused request.
+
+        assumes `ev.response_error_reason` is populated / acceptable to chrome.
+
+        mutation note: `ev` not mutated here; we only read its fields.
+
+        :param ev: interception event slated for failure.
+        """
+        await self.tab.send(
+            cdp.fetch.fail_request(
+                ev.request_id,
+                ev.response_error_reason
+            )
+        )
+
+
+    async def should_fulfill_request(self, ev: cdp.fetch.RequestPaused) -> bool:
+        """predicate controlling full synthetic response injection.
+
+        return True when we intend to build / modify a response via fulfill_request.
+
+        :param ev: interception event (mutable) you may pre-populate with body / headers.
+        :return: bool flag to trigger fulfill_request.
+        """
+        return False
+    
+
+    async def fulfill_request(self, ev: cdp.fetch.RequestPaused):
+        """issue Fetch.fulfillRequest with fields extracted from `ev`.
+
+        mutation note: body / headers may have been set by earlier steps (e.g. stream capture).
+
+        :param ev: interception event containing response parameters (MUTATED PRIOR).
+
+        expected `ev` state:
+        - `request_id`: str
+        - `response_status_code`: int status to emit
+        - `response_status_text`: (optional) str status text
+        - `response_headers`: list[HeaderEntry]
+        - `binary_response_headers`: (optional)
+        - `body`: (optional base64) — mutated previously (e.g. by streaming helpers)
+        """
+        await self.tab.send(
+            cdp.fetch.fulfill_request(
+                ev.request_id,
+                ev.response_status_code,
+                ev.response_headers,
+                getattr(ev, 'binary_response_headers', None),
+                getattr(ev, 'body', None),
+                ev.response_status_text or None
+            )
+        )
+        logger.debug("successfully fulfilled request for %s", ev.request.url)
+
+
+    async def should_intercept_response(self, ev: cdp.fetch.RequestPaused) -> bool:
+        """predicate controlling whether to intercept the response phase.
+
+        return True to enable response phase interception (response_* fields populated);
+        default False lets the response flow through unmodified.
+
+        :param ev: interception event (mutable) used for decision logic.
+        :return: bool flag to trigger response phase interception.
+        """
+        return False
+
+
+    async def continue_request(self, ev: cdp.fetch.RequestPaused):
+        """allow the original request to proceed untouched (or lightly adjusted).
+
+        can optionally mutate `ev.request` before continuing.
+
+        :param ev: interception event referencing the paused network request.
+        """
+        # chrome expects postData param to be base64 per Fetch.continueRequest spec
+        post_data = ev.request.post_data
+        if post_data is not None:
+            try:
+                # try to detect if already valid base64; if not, encode
+                base64.b64decode(post_data, validate=True)
+            except Exception:
+                if isinstance(post_data, str):
+                    post_data = base64.b64encode(post_data.encode()).decode()
+                elif isinstance(post_data, (bytes, bytearray)):
+                    post_data = base64.b64encode(bytes(post_data)).decode()
+                else:
+                    # unknown type - skip encoding and let chrome handle (will likely fail)
+                    logger.debug("unexpected post_data type for %s: %r", ev.request.url, type(post_data))
+        header_entries = [cdp.fetch.HeaderEntry(name=key, value=value) for key, value in ev.request.headers.items()]
+        logger.info("continuing request for %s with headers:\n%s", ev.request.url, header_entries)
+        await self.tab.send(
+            cdp.fetch.continue_request(
+                ev.request_id,
+                ev.request.url,
+                ev.request.method,
+                post_data,
+                header_entries,
+                await self.should_intercept_response(ev)
+            )
+        )
+        logger.debug("successfully continued request for %s", ev.request.url)
+
+
+    async def should_take_response_body_as_stream(self, ev: cdp.fetch.RequestPaused) -> bool:
+        """predicate for streaming response bodies before chrome consumes them.
+
+        **NOTE**: this is only invoked when `should_intercept_response()` returns True.
+
+        use when you need raw bytes (pdf, media) or want to transform before fulfill.
+
+        happens during request interception
+
+        :param ev: interception event (mutable) used to decide streaming.
+        :return: bool to trigger take_response_body_as_stream.
+        :rtype: bool
+        """
+        return False
+
+
+    async def handle_response_body_stream(self,
+        ev: cdp.fetch.RequestPaused,
+        stream: cdp.io.StreamHandle
+    ):
+        """read an IO stream into memory and attach base64 body onto `ev`.
+
+        flow:
+        1. iteratively read chunks via IO.read
+        2. decode base64 when flagged (cdp returns a pair (b64flag,data,...))
+        3. aggregate, then assign `ev.body` (base64 re-encoded)
+
+        mutation note: sets `ev.body` which later fulfill_request reuses.
+
+        :param ev: interception event mutated with body.
+        :param stream: stream handle from Fetch.takeResponseBodyAsStream.
+        """
+        buf = bytearray()
+        while True:
+            b64, data, eof = await self.tab.send(cdp.io.read(handle=stream))
+            buf.extend(base64.b64decode(data) if b64 else bytes(data, "utf-8"))
+            if eof: break
+        ev.body = base64.b64encode(buf).decode()
+
+
+    async def on_stream_finished(self, ev: cdp.fetch.RequestPaused):
+        """optional override for managing `ev.body` before fulfillment
+        without having to modify `take_response_body_as_stream()` or
+        `handle_response_body_stream()`.
+
+        :param ev: interception event mutated with body.
+        """
+        pass
+
+
+    async def take_response_body_as_stream(self, ev: cdp.fetch.RequestPaused):
+        """wrapper performing takeResponseBodyAsStream + buffering + closure.
+
+        mutation note: attaches processed base64 body to `ev.body` for subsequent fulfill.
+
+        override `on_stream_finished()` to easily access or modify `ev.body` before
+        fulfillment without modifying this function or `handle_response_body_stream()`.
+
+        :param ev: interception event mutated with body.
+        """
+        stream = await self.tab.send(
+            cdp.fetch.take_response_body_as_stream(ev.request_id)
+        )
+        await self.handle_response_body_stream(ev, stream)
+        await self.on_stream_finished(ev)
+        await self.tab.send(cdp.io.close(stream))
+        # mark streamed so we never double-handle body on late races
+        meta = getattr(ev, "meta", None)
+        if not isinstance(meta, RequestMeta):
+            meta = RequestMeta.from_request(ev)
+            ev.meta = meta  # type: ignore[attr-defined]
+        meta.mark_streamed()
+
+
+    async def on_response(self, ev: cdp.fetch.RequestPaused):
+        """hook invoked after a response is available (response phase interception).
+
+        **NOTE**: only invoked when `should_intercept_response()` returns True.
+        
+        extend to inspect headers / status / decide transformation before continue_response.
+
+        :param ev: interception event (contains response_* fields; may be mutated).
+        """
+        pass
+
+
+    async def continue_response(self, ev: cdp.fetch.RequestPaused):
+        """resume the response flow with minimal interference.
+
+        can be overridden to rewrite headers or status before forwarding.
+
+        :param ev: interception event carrying response metadata.
+
+        expected `ev` state:
+        - `request_id`: str
+        - `response_status_code`: int status to emit
+        - `response_status_text`: optional status text
+        - `response_headers`: optional `list[HeaderEntry]`
+        - `binary_response_headers`: optional `bytes` for raw headers
+        """
+        await self.tab.send(
+            cdp.fetch.continue_response(
+                ev.request_id,
+                ev.response_status_code,
+                ev.response_status_text or None,
+                ev.response_headers,
+                getattr(ev, 'binary_response_headers', None),
+            )
+        )
+
+
+    async def _handle(self, ev: cdp.fetch.RequestPaused):
+        """internal dispatcher orchestrating request vs response phases.
+
+        decision tree:
+        - if no response yet: run on_request -> predicates (fail | fulfill | stream | continue)
+        - if response: on_response -> continue_response
+
+        mutation note: `ev` may receive added fields (body, binary_response_headers, etc.).
+
+        :param ev: interception event being processed (MUTATED ACROSS STEPS).
+        """
+        if ev.response_status_code is None:
+            logger.debug("successfully intercepted request for %s", ev.request.url)
+            # annotate request navigation state before predicates
+            await self._annotate_request_navigation(ev)
+            await self.on_request(ev)
+            if await self.should_fail_request(ev):
+                await self.fail_request(ev)
+                return
+            if await self.should_fulfill_request(ev):
+                await self.fulfill_request(ev)
+                return
+            await self.continue_request(ev)
+        else:
+            logger.debug("successfully intercepted response for %s", ev.request.url)
+            # annotate response navigation
+            redirect = await self._annotate_response_navigation(ev)
+            await self.on_response(ev)
+            meta = getattr(ev, "meta", None)
+            if redirect:
+                # just pass through redirects (no streaming / fulfill)
+                logger.debug("detected redirect: %s => %s", 
+                    self.nav_contexts[ev.request_id].chain[-1], 
+                    ev.request.url
+                )
+                await self.continue_response(ev)
+            elif (
+                isinstance(meta, RequestMeta) 
+                and meta.final_main 
+                and await self.should_take_response_body_as_stream(ev)
+            ):
+                await self.take_response_body_as_stream(ev)
+                await self.fulfill_request(ev)
+            else:
+                await self.continue_response(ev)
+            if isinstance(meta, RequestMeta) and meta.final_main:
+                async with self._nav_lock:
+                    self.nav_contexts.pop(ev.request_id, None)
+
+
+    def handle(self, ev: cdp.fetch.RequestPaused):
+        """public entry: schedule `_handle` as a task for async concurrency.
+
+        :param ev: interception event queued for processing.
+        """
+        task = asyncio.create_task(self._handle(ev))
+        self.tasks.add(task)
+        task.add_done_callback(lambda t: self.tasks.discard(t))
+
+
+    def __call__(self, ev: cdp.fetch.RequestPaused):
+        self.handle(ev)
+
+
+    async def wait_for_tasks(self):
+        """await all outstanding interception tasks."""
+        logger.info("waiting for pending tasks to finish")
+        await asyncio.gather(*self.tasks, return_exceptions=True)
+        logger.info("all pending tasks finished")
+
+
+    async def start(self):
+        """
+        start request interception on the current tab and config
+        """
+        await self.tab.send(cdp.fetch.enable())
+        await self.tab.send(cdp.network.enable())
+        # ensure chrome always loads fresh bytes
+        # await self.tab.send(cdp.network.set_cache_disabled(True))
+        self.tab.add_handler(cdp.fetch.RequestPaused, self.handle)
+
+
+    async def stop(self, remove_handler = True, wait_for_tasks = True):
+        """
+        stop the request interception and wait for pending tasks if specified
+
+        :param remove_handler: whether to remove the `RequestPaused` handler
+        :param wait_for_tasks: whether to wait for outstanding tasks to complete
+        """
+        if remove_handler:
+            self.tab.remove_handler(cdp.fetch.RequestPaused, self.handle)
+        if wait_for_tasks:
+            await self.wait_for_tasks()
+
+
+# TODO: make this actually work
+class AuthRequiredHandler:
+    """API/handler for `Fetch.authRequired` challenges.
+
+    flow:
+    1. `on_auth_required(ev)` attaches an auth_challenge_response (mutates `ev`)
+    2. `continue_with_auth(ev)` sends decision to chrome
+
+    mutation contract:
+    - `ev.auth_challenge_response` is set in-place; callers can override to provide credentials
+
+    concurrency mirrors RequestPausedHandler: events become Tasks tracked in `tasks`.
+    """
+
+    connection: nodriver.Tab | nodriver.Connection
+    tasks: set[asyncio.Task]
+
+    def __init__(self, connection: nodriver.Tab | nodriver.Connection):
+        self.connection = connection
+        self.tasks = set()
+
+
+    async def on_auth_required(self, ev: cdp.fetch.AuthRequired):
+        """invoked when a request triggers an authentication challenge.
+
+        default strategy: answer with a "Default" challenge response (let browser decide).
+
+        extend to inject credentials:
+        ev.auth_challenge_response = cdp.fetch.AuthChallengeResponse("ProvideCredentials", username=..., password=...)
+
+        mutation note: sets `ev.auth_challenge_response` consumed in continue_with_auth.
+
+        :param ev: auth challenge event (MUTATED with response object).
+        """
+        logger.info("auth required: %s", ev.request.url)
+        ev.auth_challenge_response = cdp.fetch.AuthChallengeResponse("Default")
+
+
+    async def continue_with_auth(self, ev: cdp.fetch.AuthRequired):
+        """issue continueWithAuth using the response prepared on `ev`.
+
+        precondition: on_auth_required must set ev.auth_challenge_response.
+
+        :param ev: auth challenge event carrying previously attached response.
+        """
+        await self.connection.send(
+            cdp.fetch.continue_with_auth(
+                ev.request_id,
+                # throw if auth_challenge_response is not present
+                ev.auth_challenge_response
+            )
+        )
+
+    
+    async def _handle(self, ev: cdp.fetch.AuthRequired):
+        """orchestrate auth flow: prepare challenge response then continue.
+
+        :param ev: auth event (MUTATED then forwarded).
+        """
+        await self.on_auth_required(ev)
+        await self.continue_with_auth(ev)
+
+
+    async def handle(self, ev: cdp.fetch.AuthRequired):
+        """public entry: schedule `_handle` as a task for async concurrency.
+
+        :param ev: auth challenge event queued for processing.
+        """
+        task = asyncio.create_task(self._handle(ev))
+        self.tasks.add(task)
+        task.add_done_callback(lambda t: self.tasks.discard(t))
+
+
+    async def wait_for_tasks(self):
+        """await all outstanding auth tasks."""
+        await asyncio.gather(*self.tasks, return_exceptions=True)
+
+
+    async def start(self):
+        pass
+
+
+    async def stop(self):
+        await self.wait_for_tasks()
diff --git a/src/nodriverplus/core/handlers/stock/__init__.py b/src/nodriverplus/core/handlers/stock/__init__.py
new file mode 100644
index 0000000..dfcd37f
--- /dev/null
+++ b/src/nodriverplus/core/handlers/stock/__init__.py
@@ -0,0 +1,11 @@
+from .user_agent_patch import UserAgentPatch
+# from .scrape_request_paused_handler import ScrapeRequestPausedHandler
+from .window_size_patch import WindowSizePatch
+from .cloudflare_solver import CloudflareSolver
+
+__all__ = [
+    "UserAgentPatch",
+    # "ScrapeRequestPausedHandler",
+    "WindowSizePatch",
+    "CloudflareSolver",
+]
\ No newline at end of file
diff --git a/src/nodriverplus/core/handlers/stock/cloudflare_dark__x-120__y0.png b/src/nodriverplus/core/handlers/stock/cloudflare_dark__x-120__y0.png
new file mode 100644
index 0000000..c518e6e
Binary files /dev/null and b/src/nodriverplus/core/handlers/stock/cloudflare_dark__x-120__y0.png differ
diff --git a/src/nodriverplus/core/handlers/stock/cloudflare_light__x-120__y0.png b/src/nodriverplus/core/handlers/stock/cloudflare_light__x-120__y0.png
new file mode 100644
index 0000000..b069f40
Binary files /dev/null and b/src/nodriverplus/core/handlers/stock/cloudflare_light__x-120__y0.png differ
diff --git a/src/nodriverplus/core/handlers/stock/cloudflare_solver.py b/src/nodriverplus/core/handlers/stock/cloudflare_solver.py
new file mode 100644
index 0000000..6d7b334
--- /dev/null
+++ b/src/nodriverplus/core/handlers/stock/cloudflare_solver.py
@@ -0,0 +1,444 @@
+"""
+**TODO**:
+
+- either move HangingServer to it's own API
+- or turn this into a generic `CaptchaSolver` that can be used
+to build custom captcha solvers, and wire `CloudflareSolver` to
+use the new `CaptchaSolver` API.
+"""
+
+import asyncio
+import re
+import os
+import sys
+import logging
+import http.server
+import socketserver
+import threading
+import time
+import nodriver
+from nodriver import cdp
+from ..target_intercepted import NetworkWatcher
+from ...tab import click_template_image
+
+logger = logging.getLogger("nodriverplus.CloudflareSolver")
+
+class HangingHandler(http.server.BaseHTTPRequestHandler):
+    """request handler for `HangingServer`
+    """
+    server: "HangingServer"
+    def do_GET(self):
+        """where the magic happens.
+
+        this method will block the request until the path
+        is released via `HangingServer.release_block()`
+        """
+        logger.debug("received hanging request: %s", self.path)
+        # single critical section to avoid race where release happens between initial check and append
+        event = threading.Event()
+        immediate = False
+        with self.server._lock:
+            if self.path in self.server.released_paths:
+                # was pre-released before arrival
+                self.server.released_paths.remove(self.path)
+                immediate = True
+            else:
+                # register active waiter
+                self.server.active_requests.append((self, event, self.path))
+
+        if immediate:
+            try:
+                self.send_response(200)
+                self.end_headers()
+                self.wfile.write(b"OK")
+                logger.debug("released hanging request: %s (immediate)", self.path)
+            except BrokenPipeError:
+                logger.debug("client closed before immediate release (BrokenPipeError): %s", self.path)
+            except ConnectionAbortedError:
+                logger.debug("client closed before immediate release (ConnectionAbortedError): %s", self.path)
+            except ConnectionResetError:
+                logger.debug("client closed before immediate release (ConnectionResetError): %s", self.path)
+            return
+
+        event.wait()
+        try:
+            self.send_response(200)
+            self.end_headers()
+            self.wfile.write(b"OK")
+            logger.debug("released hanging request: %s", self.path)
+        except BrokenPipeError:
+            logger.debug("client closed before gated release (BrokenPipeError): %s", self.path)
+        except ConnectionAbortedError:
+            logger.debug("client closed before gated release (ConnectionAbortedError): %s", self.path)
+        except ConnectionResetError:
+            logger.debug("client closed before gated release (ConnectionResetError): %s", self.path)
+
+    def log_message(self, format, *args):
+        # silence logs
+        return
+
+class HangingServer(socketserver.ThreadingMixIn, socketserver.TCPServer):
+    """simple HTTP server that hangs requests until explicitly released
+    
+    initially built for `CloudflareSolver`
+    """
+    # prefer immediate re-use during development
+    allow_reuse_address = True
+    daemon_threads = True
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        # list of tuples: (handler, event, path)
+        self.active_requests: list[tuple[HangingHandler, threading.Event, str]] = []
+        # set of paths that have been released before a request arrived
+        self.released_paths: set[str] = set()
+        # protect active_requests and released_paths
+        self._lock = threading.Lock()
+
+    def handle_error(self, request, client_address):
+        """overrides default error handling to quietly ignore benign
+        `ConnectionResetError` instances that happen during shutdown.
+
+        this prevents noisy tracebacks like `"ConnectionResetError: An
+        existing connection was forcibly closed by the remote host"` from
+        being printed to stderr while still delegating other exceptions to
+        the base implementation.
+        """
+
+        exc_type, exc_value, _ = sys.exc_info()
+        if isinstance(exc_value, ConnectionResetError):
+            logger.debug("ignored ConnectionResetError from %s", client_address)
+            return
+        # fallback to default for anything else
+        return super().handle_error(request, client_address)
+
+    def release_block(self, path: str):
+        """release a hanging request if it exists, otherwise pre-release the path
+
+        :param path: the path to release, e.g. "/abcdef1234567890"
+        """
+        with self._lock:
+            for handler, event, p in list(self.active_requests):
+                if p == path:
+                    event.set()
+                    self.active_requests.remove((handler, event, p))
+                    logger.debug("signalled release of hanging request: %s", path)
+                    return True
+            if path not in self.released_paths:
+                self.released_paths.add(path)
+                logger.debug("pre-released hanging request: %s", path)
+            return False
+
+class CloudflareSolver(NetworkWatcher):
+    """
+    stock `NetworkWatcher` for detecting and solving Cloudflare challenges
+
+    this handler works by detecting cloudflare challenge responses
+    and then injecting an image into the page that will hang while
+    it clicks the checkbox until `cf_clearance` is obtained.
+
+    hanging images causes a page to not finish loading, which
+    prevents a scrape from finishing until the challenge is solved.
+
+    **TODO**:
+    - this handler cannot detect login based turnstile challenges yet,
+    only generic checkbox challenges.
+    - move HangingServer to it's own API that users can easily use to:
+        - specify when and if to hang a site so it can't finish loading
+        - do stuff in the middle
+        - specify when to release the site hang
+        - and also work along side the stock `CloudflareSolver` handler
+    """
+
+    def __init__(self, 
+        save_annotated_screenshot: str | os.PathLike = None,
+        flash_point: bool = False,
+        hanging_server_port: int = 0
+    ):
+        """initialize the `CloudflareSolver`
+
+        :param save_annotated_screenshot: if set, saves an annotated screenshot
+        to the given path when the iframe is found. useful for debugging.
+        :param flash_point: if True, flashes the click point on the page. defaults
+        to `False` because the flash messes up the screenshot given to `opencv`.
+        :param hanging_server_port: port for the internal `HangingServer` to listen on.
+        defaults to `0` which means a random free port will be chosen.
+        """
+        self.save_annotated_screenshot = save_annotated_screenshot
+        self.flash_point = flash_point
+        # bind explicitly to the IPv4 loopback so the browser connects to the same
+        # address family the server listens on (avoids ::1 vs 127.0.0.1 mismatch)
+        self.server = HangingServer(("127.0.0.1", hanging_server_port), HangingHandler)
+        self.server_thread = threading.Thread(target=self.server.serve_forever, daemon=True)
+        self.server_thread.start()
+
+        self.gates: dict[cdp.target.TargetID, dict[str, str]] = {}
+        # track gate creation times to reap stale ones
+        self._gate_created: dict[cdp.target.TargetID, dict[str, float]] = {}
+        # signal to stop background reaper
+        self._stop_reaper = threading.Event()
+        # start background reaper to ensure gates are eventually released even if traffic stops
+        self._start_reaper()
+
+    async def stop(self):
+        """
+        stop `CloudflareSolver` and all background threads/servers
+        """
+        # print("stopping the solver")
+        # signal background thread to exit
+        self._stop_reaper.set()
+        # stop server so serve_forever returns
+        try:
+            self.server.shutdown()
+        except Exception:
+            logger.exception("error during server.shutdown()")
+        # wait for threads to finish (no timeout to ensure clean exit)
+        if self.reaper_thread.is_alive():
+            self.reaper_thread.join()
+        if self.server_thread.is_alive():
+            self.server_thread.join()
+        # print("stopped the solver")
+
+    def _start_reaper(self, interval: float = 5.0, max_age: float = 30.0):
+        """start background thread to reap stale gates"""
+        def loop():
+            while not self._stop_reaper.is_set():
+                try:
+                    now = time.time()
+                    for target_id, gates in list(self.gates.items()):
+                        created = self._gate_created.get(target_id, {})
+                        for request_id, path in list(gates.items()):
+                            ts = created.get(request_id)
+                            if ts and now - ts > max_age:
+                                logger.debug("reaper releasing stale gate %s -> %s", request_id, path)
+                                self.server.release_block(path)
+                                gates.pop(request_id, None)
+                                created.pop(request_id, None)
+                except Exception:
+                    logger.exception("error in gate reaper")
+                # wait with wake-up on stop
+                self._stop_reaper.wait(interval)
+        self.reaper_thread = threading.Thread(target=loop, daemon=True)
+        self.reaper_thread.start()
+
+    def _cleanup_stale_gates(self, tab: nodriver.Tab, max_age: float = 4.0):
+        """release any gates older than max_age seconds
+        
+        :param tab: the `Tab` whose gates to clean up
+        :param max_age: the maximum age of a gate in seconds before it is considered stale
+        """
+        target_id = tab.target_id
+        created = self._gate_created.get(target_id)
+        if not created:
+            return
+        now = time.time()
+        stale: list[tuple[str, str]] = []
+        for request_id, ts in list(created.items()):
+            if now - ts > max_age and request_id in self.gates.get(target_id, {}):
+                path = self.gates[target_id][request_id]
+                stale.append((request_id, path))
+        for request_id, path in stale:
+            logger.debug("releasing stale gate %s -> %s", request_id, path)
+            self.server.release_block(path)
+            self.gates[target_id].pop(request_id, None)
+            created.pop(request_id, None)
+
+    async def on_detect(self,
+        tab: nodriver.Tab,
+    ):
+        """called when a cloudflare challenge is detected
+        
+        :param tab: the `Tab` where the challenge was detected
+        """
+        logger.info("cloudflare challenge detected on %s", tab)
+        tab._has_cf_clearance = False
+        tab._cf_turnstile_detected = True
+        # memo browser tabs list for quick lookups (avoid O(n) every loop)
+        def tab_alive() -> bool:
+            return any(t.target_id == tab.target_id for t in tab.browser.tabs)
+
+        # try both light and dark template images
+        base_dir = os.path.dirname(os.path.abspath(__file__))
+        templates = [
+            os.path.join(base_dir, "cloudflare_light__x-120__y0.png"),
+            os.path.join(base_dir, "cloudflare_dark__x-120__y0.png"),
+        ]
+        while (not tab._has_cf_clearance
+               and not self._stop_reaper.is_set()
+               and tab_alive()):
+            
+            for template in templates:
+                if tab._has_cf_clearance:
+                    break
+                if not tab_alive():
+                    logger.debug("tab detached during cloudflare solve; aborting loop")
+                    break
+                await click_template_image(
+                    tab,
+                    template,
+                    flash_point=self.flash_point,
+                    save_annotated_screenshot=self.save_annotated_screenshot,
+                )
+            await asyncio.sleep(0.5)
+
+    async def on_clearance(self,
+        tab: nodriver.Tab,
+    ):
+        """called when cloudflare clearance is detected
+        
+        :param tab: the `Tab` where the clearance was detected
+        """
+        logger.debug("clearance detected on %s", tab)
+        await tab.reload()
+        tab._has_cf_clearance = True
+        tab._cf_turnstile_detected = False
+        logger.info("successfully solved cloudflare challenge for %s", tab)
+
+    def should_ignore(self, url, method="get"):
+        """whether to ignore a request/response for gating
+        
+        :param url: the request/response URL
+        :param method: the HTTP method if specified, defaults to "get"
+        """
+        address = self.server.server_address
+        return (
+            re.match(rf"^(blob:|data:|http://{address[0]}:{address[1]}/[\w]{{32}})", url)
+            or method.lower() != "get"
+        )
+
+    async def on_request(self, tab, ev, extra_info):
+        """called on each `RequestWillBeSent` event
+        
+        sets up hang gates for requests that shouldn't be ignored
+        """
+        if self.should_ignore(ev.request.url, ev.request.method):
+            logger.debug("ignoring request: %s", ev.request.url)
+            return
+        current_gates = self.gates.get(tab.target_id, {})
+        if current_gates.get(ev.request_id):
+            self.server.release_block(current_gates[ev.request_id])
+            self.gates[tab.target_id].pop(ev.request_id, None)
+            logger.debug("released gate for redirect: %s", ev.request_id)
+
+        unique_path = f"/{os.urandom(16).hex()}"
+        target_id = tab.target_id
+        if target_id not in self.gates:
+            self.gates[target_id] = {}
+            self._gate_created[target_id] = {}
+        self.gates[target_id][ev.request_id] = unique_path
+        self._gate_created[target_id][ev.request_id] = time.time()
+        unique_url = f"http://{self.server.server_address[0]}:{self.server.server_address[1]}{unique_path}"
+        logger.debug("gate %s -> %s (origin=%s)", ev.request_id, unique_url, ev.request.url)
+
+        await tab.evaluate(f"new Image().src = '{unique_url}';")
+        # opportunistically cleanup stale gates
+        self._cleanup_stale_gates(tab)
+
+    async def on_loading_failed(self, tab, ev, request_will_be_sent):
+        """called on each `LoadingFailed` event
+
+        releases gates on failed requests/responses
+        so that they don't hang forever unknown        
+        """
+        req_url = request_will_be_sent.request.url if request_will_be_sent else "unknown"
+        logger.debug("loading failed req=%s url=%s err=%s", ev.request_id, req_url, ev.error_text)
+        # release gate if a gated request failed
+        target_id = tab.target_id
+        gates = self.gates.get(target_id)
+        if not gates:
+            logger.debug("no gates found for %s", tab)
+            return
+        path = gates.get(ev.request_id)
+        if not path:
+            # no gate was created for this request id
+            logger.debug("no gate for failed request %s <%s>", ev.request_id, req_url)
+            return
+
+        # signal the hanging handler and remove gate/state
+        try:
+            self.server.release_block(path)
+        except Exception:
+            logger.exception("error releasing gate for failed request %s -> %s", ev.request_id, path)
+
+        gates.pop(ev.request_id, None)
+        # also clear creation timestamp if present
+        self._gate_created.get(target_id, {}).pop(ev.request_id, None)
+        logger.debug("releasing gate after loading failed %s <%s> err=%s", ev.request_id, path, ev.error_text)
+        self._cleanup_stale_gates(tab)
+
+    async def on_response(self, tab, ev, extra_info):
+        """called on each `ResponseReceived` event
+
+        handles cloudflare challenge detection and clearance
+
+        ### flow:
+        1. ignore irrelevant responses
+        ### then:
+        2. check response headers for the `cf-chl-gen` header
+        3. fallback check:
+            1. first check if the response has a `cf-ray` header
+            2. then, if the URL is a turnstile script:
+            3. check if we have a `cf_clearance` cookie.
+            4. if there's no `cf_clearance` cookie, and the URL 
+            matches the turnstile pattern, then assume a checkbox challenge.
+        4. if `2` or `3.4` is true, then solve the challenge in `on_detect()`
+        ### otherwise:
+        5. if we have instead received a `cf_clearance` cookie, 
+        then signal that the challenge is solved with 
+        `on_clearance()` and release gates.
+        ### or:
+        (we already checked this in `2`, but it 
+        makes more sense reading it like this:)
+
+        6. if the response has a `cf-chl-gen` header,
+        then delete any existing `cf_clearance` cookie
+        so that step `3` hopefully won't have any false positives.
+        """
+        if self.should_ignore(ev.response.url):
+            logger.debug("ignoring response: %s", ev.response.url)
+            return
+        headers = extra_info.headers.to_json() if extra_info else ev.response.headers.to_json()
+
+        cf_ray = headers.get("cf-ray")
+        cf_chl_gen = headers.get("cf-chl-gen")
+        set_cookie = headers.get("set-cookie", "")
+        cookies: list[cdp.network.Cookie] = await tab.send(cdp.network.get_cookies([tab.url]))
+        cookies_list = [c.name for c in cookies]
+        logger.debug("cookies(%s) %s", ev.request_id, cookies_list)
+        turnstile = re.match(r".*\/turnstile\/v0(\/.*)?\/api\.js*", ev.response.url)
+
+        logger.debug("headers ray=%s turnstile=%s", cf_ray, bool(turnstile))
+        if cf_chl_gen:
+            if "cf_clearance" in cookies_list:
+                await tab.send(cdp.network.delete_cookies(
+                    name="cf_clearance", 
+                    domain=cookies[cookies_list.index("cf_clearance")].domain,
+                ))
+            if not getattr(tab, "_cf_turnstile_detected", False):
+                await self.on_detect(tab)
+        elif (
+            cf_ray and turnstile
+            and not getattr(tab, "_cf_turnstile_detected", False)
+            and not "cf_clearance" in cookies_list
+        ):
+            await self.on_detect(tab)
+        elif "cf_clearance=" in set_cookie:
+            await self.on_clearance(tab)
+
+            gates = self.gates.get(tab.target_id, {}).copy()
+            for request_id, path in gates.items():
+                self.server.release_block(path)
+                self.gates[tab.target_id].pop(request_id, None)
+            logger.debug("cleared gates for target %s", tab.target_id)
+        else:
+            try:
+                path = self.gates[tab.target_id][ev.request_id]
+            except KeyError:
+                logger.debug("no gate for response %s %s", ev.request_id, ev.response.url)
+                return
+            self.server.release_block(path)
+            logger.debug("released gate for response %s -> %s", ev.request_id, path)
+            self.gates[tab.target_id].pop(ev.request_id, None)
+            self._gate_created.get(tab.target_id, {}).pop(ev.request_id, None)
+        # cleanup stale after each response
+        self._cleanup_stale_gates(tab)
diff --git a/src/nodriverplus/core/handlers/stock/scrape_request_paused_handler.py b/src/nodriverplus/core/handlers/stock/scrape_request_paused_handler.py
new file mode 100644
index 0000000..e062e1f
--- /dev/null
+++ b/src/nodriverplus/core/handlers/stock/scrape_request_paused_handler.py
@@ -0,0 +1,93 @@
+"""
+# TODO:
+
+figure out why intercepting the original request triggers cloudflare
+and prevents you from receiving a `cf_clearance` token.
+
+maybe it has to do with `Network.requestWillBeSentExtraInfo` having
+extra headers that aren't provided by `Network.RequestPaused`?
+"""
+
+import logging
+import nodriver
+
+from ..request_paused import RequestPausedHandler
+from ...scrape_result import ScrapeResult, InterceptedResponseMeta, InterceptedRequestMeta
+import base64
+
+logger = logging.getLogger("nodriverplus.ScrapeRequestPausedHandler")
+
+
+class ScrapeRequestPausedHandler(RequestPausedHandler):
+    """stock `RequestPausedHandler` created for `NodriverPlus.scrape()`
+
+    mutates `ScrapeResult` during request/response interception.
+
+    also mutates `ScrapeResult.bytes_` according to `should_take_response_body_as_stream()`.
+    """
+
+    def __init__(self, 
+        tab: nodriver.Tab, 
+        result: ScrapeResult, 
+        url: str,
+        scrape_bytes: bool = True
+    ):
+        super().__init__(tab)
+        self.result = result
+        self.url = url
+        self.scrape_bytes = scrape_bytes
+
+
+    async def on_response(self, ev):
+        result = self.result
+        headers = {h.name.lower(): h.value for h in (ev.response_headers or [])}
+        mime = None
+        ct = headers.get("content-type", "")
+        if ct:
+            mime = ct.split(";", 1)[0].lower().strip()
+
+        if ev.request.url == self.url:
+            result.headers = headers
+            result.mime = mime
+        
+        result.intercepted_responses[ev.request.url] = InterceptedResponseMeta(
+            ev.request.url,
+            mime,
+            headers,
+            ev.request.method
+        )
+        self.result = result
+
+
+    async def on_request(self, ev):
+        headers = ev.request.headers.to_json()
+        self.result.intercepted_requests[ev.request.url] = InterceptedRequestMeta(
+            ev.request.url,
+            headers,
+            ev.request.method
+        )
+
+
+    async def should_intercept_response(self, ev):
+        return ev.request.url == self.url
+
+
+    async def should_take_response_body_as_stream(self, ev):
+        text_types = { "text", "javascript", "json", "xml" }
+        headers = {k.lower(): v for k, v in ev.request.headers.items()}
+        ct = headers.get("content-type", "").lower()
+        if (
+            ev.request.url == self.url
+            and not any(t in ct for t in text_types)
+            and self.scrape_bytes
+        ):
+            return True
+        return False
+    
+
+    async def on_stream_finished(self, ev):
+        self.result.bytes_ = base64.b64decode(ev.body)
+
+
+# keep this disabled until the request interception issues are resolved.
+__all__ = []
\ No newline at end of file
diff --git a/src/nodriverplus/core/handlers/stock/user_agent_patch.py b/src/nodriverplus/core/handlers/stock/user_agent_patch.py
new file mode 100644
index 0000000..dd752a9
--- /dev/null
+++ b/src/nodriverplus/core/handlers/stock/user_agent_patch.py
@@ -0,0 +1,109 @@
+import logging
+import nodriver
+from nodriver import cdp
+from ..target_intercepted import TargetInterceptor
+from ...user_agent import UserAgent
+from ...cdp_helpers import can_use_domain
+from ....js.load import load_text as load_js
+
+logger = logging.getLogger("nodriverplus.UserAgentPatch")
+
+
+class UserAgentPatch(TargetInterceptor):
+    """
+    stock `TargetInterceptor` for patching the user agent
+    across relevant domains for a target.
+
+    call `UserAgentPatch.patch_user_agent()` directly—with
+    or without creating a new instance—to patch
+    an existing `Tab` or `Connection`.
+    """
+    user_agent: UserAgent
+    hide_headless: bool
+
+    def __init__(self, user_agent: UserAgent, hide_headless: bool = False):
+        self.user_agent = user_agent
+        self.hide_headless = hide_headless
+
+    async def on_attach(self, connection: nodriver.Connection, ev: cdp.target.AttachedToTarget):
+        await self.patch_user_agent(connection, ev, self.user_agent, self.hide_headless)
+
+    async def on_change(self, tab, ev):
+        await self.patch_user_agent(tab, ev, self.user_agent, self.hide_headless)
+
+    @staticmethod
+    async def patch_user_agent( 
+        connection: nodriver.Tab | nodriver.Connection,
+        ev: cdp.target.AttachedToTarget | None,
+        user_agent: UserAgent,
+        hide_headless: bool = False
+    ):
+        """static method for applying UA overrides
+        across relevant domains for a target.
+
+        removes "Headless" when `hide_headless=True`
+
+        sets Network + Emulation overrides and installs a runtime 
+        patch so navigator + related surfaces align. worker/page aware.
+
+        :param connection: `Tab` or `Connection` to apply the patch to.
+        :param ev: if `None`, `connection` must be of type `Tab`.
+        :param user_agent: prepared UserAgent instance.
+        :param hide_headless: whether to strip "Headless" from user_agent.
+        """
+
+        if hide_headless:
+            user_agent.user_agent = user_agent.user_agent.replace("Headless", "")
+            user_agent.app_version = user_agent.app_version.replace("Headless", "")
+
+        if isinstance(connection, nodriver.Tab) and ev is None:
+            target_type = "tab"
+            msg = f"{target_type} <{connection.url}>"
+            session_id = None
+        else:
+            target_type = ev.target_info.type_
+            msg = f"{target_type} <{ev.target_info.url}>"
+            session_id = ev.session_id
+
+        domains_patched = []
+
+        if can_use_domain(target_type, "Network"):
+            logger.debug("patching user agent for %s with domain: \"Network\"", msg)
+            await connection.send(cdp.network.set_user_agent_override(
+                user_agent=user_agent.user_agent,
+                accept_language=user_agent.accept_language,
+                platform=user_agent.platform,
+                user_agent_metadata=user_agent.metadata,
+            ), session_id)
+            domains_patched.append("Network")
+        if can_use_domain(target_type, "Emulation"):
+            logger.debug("patching user agent for %s with domain: \"Emulation\"", msg)
+            await connection.send(cdp.emulation.set_user_agent_override(
+                user_agent=user_agent.user_agent,
+                accept_language=user_agent.accept_language,
+                platform=user_agent.platform,
+                user_agent_metadata=user_agent.metadata,
+            ), session_id)
+            domains_patched.append("Emulation")
+            
+        js = load_js("patch_user_agent.js")
+        uaPatch = f"const uaPatch = {user_agent.to_json(True, True)};"
+        script = js.replace("//uaPatch//", uaPatch)
+        if can_use_domain(target_type, "Page"):
+            await connection.send(cdp.page.add_script_to_evaluate_on_new_document(
+                source=script,
+                include_command_line_api=True,
+            ), session_id)
+        if can_use_domain(target_type, "Runtime"):
+            await connection.send(cdp.runtime.evaluate(
+                expression=script,
+                include_command_line_api=True,
+                allow_unsafe_eval_blocked_by_csp=True
+            ), session_id)
+            domains_patched.append("Runtime")
+
+        if len(domains_patched) == 0:
+            logger.info("no domains available to patch user agent for %s", msg)
+        else:
+            logger.debug("successfully patched user agent for %s with domains %s", msg, domains_patched)
+
diff --git a/src/nodriverplus/core/handlers/stock/window_size_patch.py b/src/nodriverplus/core/handlers/stock/window_size_patch.py
new file mode 100644
index 0000000..5a2a6ac
--- /dev/null
+++ b/src/nodriverplus/core/handlers/stock/window_size_patch.py
@@ -0,0 +1,113 @@
+import logging
+import nodriver
+from nodriver import cdp
+from ..target_intercepted import TargetInterceptor
+from ...cdp_helpers import can_use_domain
+
+logger = logging.getLogger("nodriverplus.WindowSizePatch")
+
+
+class WindowSizePatch(TargetInterceptor):
+	"""stock interceptor for setting viewport metrics via emulation domain.
+
+	skips targets without emulation. mirrors width/height to screen metrics.
+
+	call `WindowSizePatch.patch_window_size()` directly—with
+	or without creating a new instance—to patch an existing `Tab` or `Connection`.
+	"""
+
+	width: int
+	height: int
+	device_scale_factor: float
+	mobile: bool
+	orientation: str | None
+
+	def __init__(
+		self,
+		width: int,
+		height: int,
+		*,
+		device_scale_factor: float = 1.0,
+		mobile: bool = False,
+		orientation: str | None = None,
+	):
+		self.width = width
+		self.height = height
+		self.device_scale_factor = device_scale_factor
+		self.mobile = mobile
+		self.orientation = orientation
+
+	async def on_attach(
+		self,
+		connection: nodriver.Tab | nodriver.Connection,
+		ev: cdp.target.AttachedToTarget | None,
+	):
+		await self.patch_window_size(
+			connection,
+			ev,
+			width=self.width,
+			height=self.height,
+			device_scale_factor=self.device_scale_factor,
+			mobile=self.mobile,
+			orientation=self.orientation,
+		)
+
+	@staticmethod
+	async def patch_window_size(
+		connection: nodriver.Tab | nodriver.Connection,
+		ev: cdp.target.AttachedToTarget | None,
+		*,
+		width: int,
+		height: int,
+		device_scale_factor: float = 1.0,
+		mobile: bool = False,
+		orientation: str | None = None,
+	):
+		"""static method for applying a device metrics
+		override if emulation domain is available.
+
+		keeps quiet when emulation is not exposed (workers/etc.). mirrors screen size
+		so detection scripts comparing inner/outer sizes see consistent values.
+
+		:param ev: may be None when called directly on a tab with no attach event.
+		:param width: viewport width css px.
+		:param height: viewport height css px.
+		:param device_scale_factor: dpr (usually 1.0 for desktop).
+		:param mobile: emulate mobile viewport if True.
+		:param orientation: optional orientation (landscapePrimary / portraitPrimary).
+		"""
+
+		if isinstance(connection, nodriver.Tab) and ev is None:
+			target_type = "tab"
+			session_id = None
+			msg = f"tab <{connection.url}>"
+		else:
+			target_type = ev.target_info.type_
+			session_id = ev.session_id
+			msg = f"{target_type} <{ev.target_info.url}>"
+
+		if not can_use_domain(target_type, "Emulation") or target_type not in ("page", "tab", "iframe"):
+			logger.debug("skipping window size patch for %s", msg)
+			return
+
+		try:
+			await connection.send(cdp.emulation.set_device_metrics_override(
+				width,
+				height,
+				device_scale_factor,
+				mobile,
+				screen_width=width,
+				screen_height=height,
+				screen_orientation={"type": orientation, "angle": 0} if orientation else None
+			), session_id)
+		except Exception:
+			logger.exception("failed patching window size for %s:", msg)
+		else:
+			logger.debug(
+				"successfully patched window size for %s (width=%d height=%d dpr=%s mobile=%s)",
+				msg,
+				width,
+				height,
+				device_scale_factor,
+				mobile,
+			)
diff --git a/src/nodriverplus/core/handlers/target_intercepted.py b/src/nodriverplus/core/handlers/target_intercepted.py
new file mode 100644
index 0000000..fa2a3d0
--- /dev/null
+++ b/src/nodriverplus/core/handlers/target_intercepted.py
@@ -0,0 +1,489 @@
+import asyncio
+import websockets
+import logging
+import traceback
+from typing import Callable, Awaitable
+from nodriver import cdp, Tab, Connection, Browser
+from ..cdp_helpers import can_use_domain
+import os
+
+logger = logging.getLogger("nodriverplus.TargetInterceptorManager")
+
+class NetworkWatcher:
+    """base class for a network watcher managed by `TargetInterceptorManager`
+
+    override methods to handle different CDP network events.
+
+    **NOTE**: this class is tied closely to `TargetInterceptorManager` and
+    is not useful on its own.
+    
+    **TODO**:
+    - add `on_loading_finished`?
+    - add `on_data_received`?
+    - make it less convoluted probably.
+    """
+
+    async def on_response(self, 
+        tab: Tab,
+        ev: cdp.network.ResponseReceived,
+        extra_info: cdp.network.ResponseReceivedExtraInfo | None,
+    ):
+        """
+        handle a `ResponseReceived` event
+        
+        :param connection: the `Tab` the event was received on.
+        :param ev: the `ResponseReceived` event.
+        :param extra_info: the `ResponseReceivedExtraInfo` event, if available.
+        """
+        pass
+
+    async def on_response_extra_info(self,
+        ev: cdp.network.ResponseReceivedExtraInfo,
+    ):
+        """
+        handle a `ResponseReceivedExtraInfo` event
+        
+        :param ev: the `ResponseReceivedExtraInfo` event.
+        """
+        pass
+
+    async def on_loading_failed(self,
+        tab: Tab,
+        ev: cdp.network.LoadingFailed,
+        request_will_be_sent: cdp.network.RequestWillBeSent,
+    ):
+        """
+        handle a `LoadingFailed` event
+
+        :param tab: the `Tab` the event was received on.
+        :param ev: the `LoadingFailed` event.
+        """
+        pass
+
+    async def on_request(self,
+        tab: Tab,
+        ev: cdp.network.RequestWillBeSent,
+        extra_info: cdp.network.RequestWillBeSentExtraInfo | None,
+    ):
+        """
+        handle a `RequestWillBeSent` event
+
+        :param connection: the `Tab` the event was received on.
+        :param ev: the `RequestWillBeSent` event.
+        :param extra_info: the `RequestWillBeSentExtraInfo` event, if available.
+        """
+        pass
+
+    async def on_request_extra_info(self,
+        ev: cdp.network.RequestWillBeSentExtraInfo
+    ):
+        """
+        handle a `RequestWillBeSentExtraInfo` event
+
+        :param ev: the RequestWillBeSentExtraInfo event.
+        """
+        pass
+
+    async def stop(self):
+        """hook for stopping/cleaning up the watcher if needed"""
+        pass
+
+
+class TargetInterceptor:
+    """base class for a target interceptor
+
+    you must provide a `on_attach()` method that takes a connection and an event.
+
+    called by `apply_target_interceptors()`
+    """
+
+    async def on_attach(
+        self,
+        connection: Tab | Connection,
+        ev: cdp.target.AttachedToTarget | None
+    ):
+        """hook for handling target attachment events
+
+        :param connection: the connection to the target.
+        :param ev: **may be `None`** depending on how you call it, so be aware.
+        :type ev: AttachedToTarget | None
+        """
+        pass
+
+    async def on_change(
+        self,
+        tab: Tab | Connection,
+        ev: cdp.target.TargetInfoChanged | None,
+    ):
+        """hook for handling target change events
+
+        :param tab: the Tab instance where the event was received.
+        :param ev: the target info changed event
+        :type ev: TargetInfoChanged | None
+        """
+        pass
+
+
+class TargetInterceptorManager:
+
+    def __init__(self, 
+        session: Tab | Connection | Browser = None, 
+        interceptors: list[TargetInterceptor] = None,
+        network_watchers: list[NetworkWatcher] = None
+    ):
+        """init TargetInterceptorManager
+
+        for now, each `TargetInterceptorManager` can only have one session
+
+        :param session: the session that you want to add `TargetInterceptor`'s to.
+        :param interceptors: a list of `TargetInterceptor`'s to add to the manager.
+        :param network_watchers: a list of `NetworkWatcher`'s to add to the manager.
+        """
+        self.connection = session.connection if isinstance(session, Browser) else session
+        self.interceptors = interceptors or []
+        self.network_watchers = network_watchers or []
+        self.request_ids_to_tab: dict[str, Tab] = {}
+        self.request_sent_events: dict[str, cdp.network.RequestWillBeSent] = {}
+        self.response_received_events: dict[str, cdp.network.ResponseReceived] = {}
+        self.responses_extra_info: dict[str, cdp.network.ResponseReceivedExtraInfo] = {}
+        self.requests_extra_info: dict[str, cdp.network.RequestWillBeSentExtraInfo] = {}
+        self.target_id_to_connection: dict[str, Tab | Connection] = {}
+        # lifecycle control
+        self._stopped = False
+        # mapping of CDP event types to manager handler callables
+        # allows central dispatch and simpler customization
+        self.handler_mappings: dict[type, Callable[[object], Awaitable[None]]] = {
+            cdp.target.AttachedToTarget: self.on_attach,
+            cdp.target.TargetInfoChanged: self.on_change_interceptors,
+            cdp.network.LoadingFailed: self.on_loading_failed,
+            cdp.network.RequestWillBeSent: self.on_request_will_be_sent,
+            cdp.network.RequestWillBeSentExtraInfo: self.on_request_will_be_sent_extra_info,
+            cdp.network.ResponseReceived: self.on_response_received,
+            cdp.network.ResponseReceivedExtraInfo: self.on_response_received_extra_info,
+        }
+
+
+    async def _dispatch_event(self, ev: object):
+        """central dispatcher that looks up the handler in `self.handler_mappings`
+        and calls it with consistent exception handling.
+
+        :param ev: the event instance
+        """
+        event_type = type(ev)
+        if self._stopped:
+            return
+        handler = self.handler_mappings.get(event_type)
+        if handler is None:
+            logger.debug("no handler mapped for %s", event_type)
+            return
+        msg = f"failed to run {handler.__name__} for {event_type}"
+        if hasattr(ev, "target_info"):
+            msg += f" on {ev.target_info.type_} <{ev.target_info.url}>:"
+        else:
+            msg += ":"
+        def _log_exc_debug(msg, *a):
+            logger.debug(msg, *a, exc_info=True)
+        def _log_exc_warning(msg, *a):
+            logger.warning(msg, *a, exc_info=logger.getEffectiveLevel() <= logging.DEBUG)
+        try:
+            try:
+                await handler(ev)
+            except Exception as e:
+                current_file = os.path.normcase(os.path.normpath(__file__))
+                extracted = traceback.extract_tb(e.__traceback__)
+                for frame in extracted:
+                    frame_file = os.path.normcase(os.path.normpath(frame.filename))
+                    if frame_file != current_file:
+                        msg += f"\n{frame.filename}:{frame.lineno} in {frame.name}:\n"
+                        break
+                raise e
+        except (
+            websockets.exceptions.ConnectionClosedOK, 
+            websockets.exceptions.ConnectionClosedError
+        ) as e:
+            _log_exc_debug("%s target already moved/closed.", msg)
+        except websockets.exceptions.InvalidStatus as e:
+            if e.response.body.startswith(b'No such target id:'):
+                _log_exc_debug("%s target already moved/closed.", msg)
+            else:
+                logger.exception(msg)
+        except (EOFError, websockets.exceptions.InvalidMessage):
+            _log_exc_debug("%s websocket handshake/parse already closed.", msg)
+        except Exception as e:
+            if "-32000" in str(e):
+                _log_exc_warning("%s execution context not created yet.", msg)
+            elif "-32001" in str(e):
+                _log_exc_warning("%s session not found. (potential timing issue?)", msg)
+            elif "-32601" in str(e):
+                _log_exc_debug("%s method not found.", msg)
+            else:
+                logger.exception(msg)
+
+
+    async def set_hook(
+        self,
+        ev: cdp.target.AttachedToTarget | None
+    ):
+        """enable Target.setAutoAttach recursively and attach target interceptors
+
+        attaches recursively to workers/frames and ensures target interceptor application
+
+        :param ev: optional original attach event (when recursively called).
+        """
+        connection = self.connection
+        filters = [
+            {"type": "tab", "exclude": True},
+            {"type": "iframe", "exclude": True} # can't figure this one out yet
+        ]
+
+        if ev:
+            msg = f"{ev.target_info.type_} <{ev.target_info.url}>"
+            session_id = ev.session_id
+            # service workers will show up twice if allowed to populate on Page events
+            if ev.target_info.type_ == "page":
+                filters.append({"type": "service_worker", "exclude": True})
+                # network can only be enabled on page targets
+                network_msg = f"failed to enable network for {msg}:"
+                try:
+                    await connection.send(cdp.network.enable(), session_id)
+                except Exception as e:
+                    if "-32001" in str(e):
+                        logger.warning("%s session not found. (potential timing issue?)", network_msg)
+                    else:
+                        logger.exception(network_msg)
+        else:
+            msg = connection
+            session_id = None
+        filters.append({})
+
+        attach_msg = f"failed to set auto attach for {msg}:"
+        try:
+            await connection.send(cdp.target.set_auto_attach(
+                auto_attach=True,
+                wait_for_debugger_on_start=True,
+                flatten=True,
+                filter_=cdp.target.TargetFilter(filters)
+            ), session_id)
+            logger.debug("successfully set auto attach for %s", attach_msg)
+        except websockets.exceptions.ConnectionClosedError:
+            logger.debug("%s browser already closed.", attach_msg)
+        except Exception as e:
+            if "-32001" in str(e):
+                logger.warning("%s session not found. (potential timing issue?)", attach_msg)
+            elif "-32601" in str(e):
+                logger.warning("%s method not found.", attach_msg)
+            else:
+                logger.exception(attach_msg)
+
+
+    # `target_id` and `frame_id` are the same for tabs
+    # so as long as we only enable network for page targets
+    # we can find the tab like this: 
+    # (if `None`, it probably doesn't matter)
+    async def on_response_received(self, ev: cdp.network.ResponseReceived):
+        """CDP handler fired when a network response is received.
+
+        passes the event to all `NetworkWatcher.on_response` handlers.
+        """
+        if self._stopped:
+            return
+        tab = next((t for t in self.connection.browser.tabs if t.target_id == ev.frame_id), None)
+        if tab is None:
+            logger.debug("no tab found for ResponseReceived <%s> with target_id %s", ev.response.url, ev.frame_id)
+            return
+        for handler in self.network_watchers:
+            await handler.on_response(tab, ev, self.responses_extra_info.get(ev.request_id))
+        self.responses_extra_info.pop(ev.request_id, None)
+
+
+    async def on_loading_failed(self, ev: cdp.network.LoadingFailed):
+        """CDP handler fired when a network request fails to load.
+
+        passes the event to all `NetworkWatcher.on_loading_failed` handlers.
+        """
+        if self._stopped:
+            return
+        tab = self.request_ids_to_tab.get(ev.request_id)
+        if tab is None:
+            logger.debug("no tab found for LoadingFailed with request_id %s", ev.request_id)
+            return
+
+        for handler in self.network_watchers:
+            await handler.on_loading_failed(tab, ev, self.request_sent_events.get(ev.request_id))
+        self.request_ids_to_tab.pop(ev.request_id, None)
+
+
+    async def on_response_received_extra_info(self, ev: cdp.network.ResponseReceivedExtraInfo):
+        """CDP handler fired when extra information about a network response is received.
+
+        passes the event to all `NetworkWatcher.on_response_extra_info` handlers.
+        """
+        if self._stopped:
+            return
+        self.responses_extra_info[ev.request_id] = ev
+        for handler in self.network_watchers:
+            await handler.on_response_extra_info(ev)
+
+
+    async def on_request_will_be_sent(self, ev: cdp.network.RequestWillBeSent):
+        """CDP handler fired when a network request is about to be sent.
+
+        passes the event to all `NetworkWatcher.on_request` handlers.
+        """
+        if self._stopped:
+            return
+        tab = next((t for t in self.connection.browser.tabs if t.target_id == ev.frame_id), None)
+        if tab is None:
+            logger.debug("no tab found for RequestWillBeSent <%s> with target_id %s", ev.request.url, ev.frame_id)
+            return
+        self.request_ids_to_tab[ev.request_id] = tab
+        # store the request event so LoadingFailed handlers can access original url
+        self.request_sent_events[ev.request_id] = ev
+        for handler in self.network_watchers:
+            await handler.on_request(tab, ev, self.requests_extra_info.get(ev.request_id))
+        self.requests_extra_info.pop(ev.request_id, None)
+
+    
+    async def on_request_will_be_sent_extra_info(self, ev: cdp.network.RequestWillBeSentExtraInfo):
+        """CDP handler fired when extra information about a network request is received.
+
+        passes the event to all `NetworkWatcher.on_request_extra_info` handlers.
+        """
+        if self._stopped:
+            return
+        self.requests_extra_info[ev.request_id] = ev
+        for handler in self.network_watchers:
+            await handler.on_request_extra_info(ev)
+
+
+    async def on_change_interceptors(
+        self,
+        ev: cdp.target.TargetInfoChanged,
+    ):
+        """execute a list of `TargetInterceptor.on_change` calls—(in order)—to
+        `self.connection` with the `TargetInfoChanged` event.
+
+        :param ev: the event to pass to the interceptors.
+        """
+        if self._stopped:
+            return
+        target_msg = f"{ev.target_info.type_} <{ev.target_info.url}>"
+        connection = self.target_id_to_connection.get(ev.target_info.target_id) or self.connection
+        for interceptor in self.interceptors:
+            ev.session_id = None
+            logger.debug("calling interceptor (on_change) %s to %s", interceptor, target_msg)
+            await interceptor.on_change(connection, ev)
+
+
+    async def on_attach_interceptors(
+        self,
+        ev: cdp.target.AttachedToTarget | None,
+    ):
+        """execute a list of `TargetInterceptor.on_attach` calls—(in order)—to
+        `self.connection` with the `AttachedToTarget` event if available.
+
+        :param ev: the event to pass to the interceptors.
+        """
+        if ev:
+            target_msg = f"{ev.target_info.type_} <{ev.target_info.url}>"
+
+        elif isinstance(self.connection, Tab):
+            target_msg = f"tab <{self.connection.url}>"
+        else:
+            target_msg = f"connection <{self.connection}>"
+        if self._stopped:
+            return
+        for interceptor in self.interceptors:
+            if ev:
+                msg = f"{interceptor} to {target_msg}"
+            else:
+                msg = f"{interceptor} to {self.connection}"
+            logger.debug("calling interceptor (on_attach) %s", msg)
+            await interceptor.on_attach(self.connection, ev)
+
+
+    async def on_attach(
+        self,
+        ev: cdp.target.AttachedToTarget | None,
+    ):        
+        """handler fired when a new target is auto-attached.
+
+        applies `TargetInterceptor`s and recursively attaches to child sessions/targets
+
+        :param ev: CDP AttachedToTarget event.
+        """
+        if self._stopped:
+            return
+        connection = self.connection
+
+        if ev is not None:
+            msg = f"{ev.target_info.type_} <{ev.target_info.url}>"
+            session_id = ev.session_id
+            tab = next((t for t in connection.browser.tabs if t.target_id == ev.target_info.target_id), None)
+            if tab:
+                self.target_id_to_connection[ev.target_info.target_id] = tab
+            else:
+                self.target_id_to_connection[ev.target_info.target_id] = connection
+        else:
+            msg = connection
+            session_id = None
+        logger.debug("successfully attached to %s", msg)
+
+        # call on_attach interceptors
+        await self.on_attach_interceptors(ev)
+        # recursive attachment and TargetInfoChanged handling
+        await self.set_hook(ev)
+        # continue like normal
+        try:
+            logger.debug("resuming %s (session_id=%s)", msg, session_id)
+            await connection.send(cdp.runtime.run_if_waiting_for_debugger(), session_id)
+        except ConnectionRefusedError:
+            logger.debug("%s browser already closed.", msg)
+        except Exception as e:
+            if "-32001" in str(e):
+                logger.warning("session for %s not found. (potential timing issue?)", msg)
+            else:
+                logger.exception("failed to resume %s:", msg)
+        else:
+            logger.debug("successfully resumed %s", msg)
+
+
+    async def start(
+        self,
+    ):
+        """
+        start the hook on `self.connection` to recursively apply `TargetInterceptor`s.
+        """
+        connection = self.connection
+
+        # avoid duping stuff
+        if getattr(connection, "_target_interceptor_manager_initialized", False):
+            return
+        setattr(connection, "_target_interceptor_manager_initialized", True)
+
+        # register all mapped handlers to route through the central dispatcher
+        for ev_type in self.handler_mappings.keys():
+            connection.add_handler(ev_type, self._dispatch_event)
+        await self.set_hook(None)
+
+    async def stop(self):
+        """stop all background tasks
+
+        remove handlers and stop all created 
+        subprocesses/threads if available.
+        """
+        if self._stopped:
+            return
+        self._stopped = True
+        # remove handlers we previously added to avoid further callback dispatch after websocket closes
+        for ev_type in self.handler_mappings.keys():
+            self.connection.remove_handler(ev_type, self._dispatch_event)
+        
+        for watcher in self.network_watchers:
+            try:
+                # support both sync and async `stop()` implementations
+                res = watcher.stop()
+                if asyncio.iscoroutine(res):
+                    await res
+            except Exception:
+                logger.exception("error stopping watcher %s", watcher)
\ No newline at end of file
diff --git a/src/nodriverplus/core/manager.py b/src/nodriverplus/core/manager.py
index e2fd6b6..f136a1b 100644
--- a/src/nodriverplus/core/manager.py
+++ b/src/nodriverplus/core/manager.py
@@ -4,50 +4,34 @@ import threading
 import queue as _thread_queue
 from typing import Callable
 
-from .nodriverplus import NodriverPlus
-from .scrape_response import ScrapeResponseHandler, CrawlResultHandler
+import nodriver
 
-logger = logging.getLogger(__name__)
+# from .pause_handlers import ScrapeRequestPausedHandler
+from .scrape_result import ScrapeResultHandler, CrawlResultHandler
+from .tab import crawl, scrape
+
+logger = logging.getLogger("nodriverplus.Manager")
 
 
 class ManagerJob:
     url: str
     type_: str
-    handler: ScrapeResponseHandler | CrawlResultHandler | None
     kwargs: dict
 
-    def __init__(self, url: str, type_: str, handler: ScrapeResponseHandler | CrawlResultHandler | None, kwargs: dict):
-        """lightweight container describing a pending unit of work.
-
-        jobs sit on a thread-safe queue so producer code (maybe running in a different
-        thread) can schedule crawls / scrapes without touching the event loop directly.
-
-        :param url: target url for the operation.
-        :param type_: either "crawl" or "scrape" (dispatcher uses this).
-        :param handler: optional handler instance whose .handle() will receive the result.
-        :param kwargs: params forwarded to NodriverPlus.crawl/scrape.
-        """
+    def __init__(self, url: str, type_: str, kwargs: dict):
+        # lightweight container describing a pending unit of work.
         self.url = url
         self.type_ = type_
-        self.handler = handler
         self.kwargs = kwargs
 
-    def from_dict(cls, kwargs: dict):
-        """alternate constructor kept for symmetry with export/import paths.
+class Manager:
+    """manage a queue of crawl/scrape jobs with concurrency control.
+
+    no longer bound to a specific `NodriverPlus` instance;
+    each job must carry its own `base` (browser or tab).
+    """
 
-        note: signature mirrors the shape returned by ManagerJob.__dict__ so persisted
-        queue snapshots can be rehydrated.
-        """
-        return cls(
-            url=kwargs["url"],
-            type_=kwargs["type_"],
-            handler=kwargs["handler"],
-            kwargs=kwargs["kwargs"]
-        )
-
-class NodriverPlusManager:
     queue: _thread_queue.Queue
-    ndp: NodriverPlus
     concurrency: int
     _running: bool
     _running_tasks: list[asyncio.Task]
@@ -56,21 +40,18 @@ class NodriverPlusManager:
     _done_event: threading.Event
     _runner_task: asyncio.Task | None
     _bound_task: asyncio.Task | None
-    _stop_handler: Callable[[list[dict]], any] | None
+    _stop_handler: Callable[[list[ManagerJob]], any] | None
 
-    def __init__(self, ndp: NodriverPlus, concurrency: int = 1):
-        """orchestrates queued scrape/crawl jobs with bounded concurrency.
+    def __init__(self, concurrency: int = 1):
+        """manage a queue of crawl/scrape jobs with concurrency control.
 
-        a single background coroutine drains a thread-safe queue (put from any thread)
-        and spins up asyncio tasks (limited by a semaphore) that call into the shared
-        NodriverPlus instance.
+        no longer bound to a specific `NodriverPlus` instance;
+        each job must carry its own `base` (browser or tab).
 
-        :param ndp: active NodriverPlus instance (already started).
-        :param concurrency: max number of simultaneous jobs.
+        :param concurrency: max number of simultaneous jobs (default 1).
         """
-        # use a thread-safe queue for cross-thread communication
+        # now stateless w.r.t browser context; per-job base passed in enqueue.
         self.queue = _thread_queue.Queue()
-        self.ndp = ndp
         self.concurrency = concurrency
         self._thread = None
         self._stop_event = threading.Event()
@@ -79,43 +60,51 @@ class NodriverPlusManager:
         self._bound_task = None
         self._stop_handler = None
 
-    async def enqueue_crawl(
-        self,
+    def enqueue_crawl(self,
+        base: nodriver.Browser | nodriver.Tab,
         url: str,
-        scrape_response_handler: ScrapeResponseHandler = None,
-        depth: int = 1,
+        scrape_result_handler: ScrapeResultHandler = None,
+        depth = 1,
         crawl_result_handler: CrawlResultHandler = None,
         *,
-        new_window: bool = False,
-        scrape_bytes: bool = True,
-        navigation_timeout: int = 30,
-        wait_for_page_load: bool = True,
-        page_load_timeout: int = 60,
-        extra_wait_ms: int = 0,
+        new_window = False,
+        # scrape_bytes = True,
+        navigation_timeout = 30,
+        wait_for_page_load = True,
+        page_load_timeout = 60,
+        extra_wait_ms = 0,
         concurrency: int = 1,
         max_pages: int | None = None,
         collect_responses: bool = False,
         delay_range: tuple[float, float] | None = None,
-        tab_close_timeout: float = 5.0,
+        # request_paused_handler: ScrapeRequestPausedHandler = None,
+        proxy_server: str = None,
+        proxy_bypass_list: list[str] = None,
+        origins_with_universal_network_access: list[str] = None,
     ):
-        """async wrapper that just enqueues a crawl up to `depth`.
+        """enqueue a `crawl()` job
+        
+        mirror of `crawl()`, except that it's sync
+        and sends the crawl to a background queue.
 
-        kept async so user code calling from coroutines reads naturally even though
-        the body only uses the thread-safe queue.
+        **note:** returns `None` immediately. handlers
+        must be provided to receive results.
 
-        :param crawl_result_handler: optional handler for when the enqueued crawl finishes.
+        :return: returns immediately; results delivered via
+        `scrape_result_handler` and `crawl_result_handler`.
+        :rtype: None
         """
-        # enqueue a crawl job (thread-safe queue)
         self.queue.put(ManagerJob(
             url=url,
             type_="crawl",
-            handler=crawl_result_handler,
             kwargs={
+                "base": base,
                 "url": url,
-                "handler": scrape_response_handler,
+                "scrape_result_handler": scrape_result_handler,
                 "depth": depth,
+                "crawl_result_handler": crawl_result_handler,
                 "new_window": new_window,
-                "scrape_bytes": scrape_bytes,
+                # "scrape_bytes": scrape_bytes,
                 "navigation_timeout": navigation_timeout,
                 "wait_for_page_load": wait_for_page_load,
                 "page_load_timeout": page_load_timeout,
@@ -124,43 +113,59 @@ class NodriverPlusManager:
                 "max_pages": max_pages,
                 "collect_responses": collect_responses,
                 "delay_range": delay_range,
-                "tab_close_timeout": tab_close_timeout,
+                # "request_paused_handler": request_paused_handler,
+                "proxy_server": proxy_server,
+                "proxy_bypass_list": proxy_bypass_list,
+                "origins_with_universal_network_access": origins_with_universal_network_access,
             }
         ))
-        
-    async def enqueue_scrape(self,
+
+    def enqueue_scrape(self, 
+        base: nodriver.Browser | nodriver.Tab,
         url: str,
-        scrape_bytes = True,
-        scrape_response_handler: ScrapeResponseHandler = None,
+        # scrape_bytes = True,
+        scrape_result_handler: ScrapeResultHandler | None = None,
         *,
         navigation_timeout = 30,
         wait_for_page_load = True,
         page_load_timeout = 60,
         extra_wait_ms = 0,
-        # solve_cloudflare = True, # not implemented yet
         new_tab = False,
         new_window = False,
+        # request_paused_handler = ScrapeRequestPausedHandler,
+        proxy_server: str = None,
+        proxy_bypass_list: list[str] = None,
+        origins_with_universal_network_access: list[str] = None,
     ):
-        """async wrapper that still just enqueues a single-page scrape.
+        """enqueue a `scrape()` job
+        
+        mirror of `scrape()`, except that it's sync
+        and sends the scrape to a background queue.
 
-        kept async so user code calling from coroutines reads naturally even though
-        the body only uses the thread-safe queue.
+        **note:** returns `None` immediately. a handler
+        must be provided to receive the result.
 
-        :param scrape_response_handler: optional handler for when the enqueued scrape finishes.
+        :return: returns immediately; result delivered via `scrape_result_handler`.
+        :rtype: None
         """
         self.queue.put(ManagerJob(
             url=url,
             type_="scrape",
-            handler=scrape_response_handler,
             kwargs={
+                "base": base,
                 "url": url,
-                "scrape_bytes": scrape_bytes,
+                # "scrape_bytes": scrape_bytes,
+                "scrape_result_handler": scrape_result_handler,
                 "navigation_timeout": navigation_timeout,
                 "wait_for_page_load": wait_for_page_load,
                 "page_load_timeout": page_load_timeout,
                 "extra_wait_ms": extra_wait_ms,
                 "new_tab": new_tab,
                 "new_window": new_window,
+                # "request_paused_handler": request_paused_handler,
+                "proxy_server": proxy_server,
+                "proxy_bypass_list": proxy_bypass_list,
+                "origins_with_universal_network_access": origins_with_universal_network_access,
             }
         ))
 
@@ -216,7 +221,7 @@ class NodriverPlusManager:
         try:
             loop = asyncio.get_running_loop()
         except RuntimeError:
-            raise RuntimeError("NodriverPlusManager.start() must be called from within an asyncio event loop")
+            raise RuntimeError("Manager.start() must be called from within an asyncio event loop")
 
         self._runner_task = loop.create_task(self._run_loop())
 
@@ -263,7 +268,7 @@ class NodriverPlusManager:
             try:
                 result = self._stop_handler(exported)
                 if asyncio.iscoroutine(result):
-                    await result  # type: ignore[arg-type]
+                    await result
             except Exception:
                 logger.exception("stop handler failed")
             finally:
@@ -283,18 +288,14 @@ class NodriverPlusManager:
         async def _handle_job(job: ManagerJob):
             msg = f"{job.type_} job <{job.url}>"
             try:
+                # each job carries its own base (browser/tab)
+                base = job.kwargs.pop("base", None)
+                if base is None:
+                    raise RuntimeError("manager job missing 'base'")
                 if job.type_ == "crawl":
-                    result = await self.ndp.crawl(**job.kwargs)
+                    await crawl(base, **job.kwargs)
                 elif job.type_ == "scrape":
-                    result = await self.ndp.scrape(**job.kwargs)
-                try:
-                    if job.handler is not None:
-                        if asyncio.iscoroutinefunction(job.handler.handle):
-                            await job.handler.handle(result)
-                        else:
-                            job.handler.handle(result)
-                except Exception:
-                    logger.exception("error running handler for %s", msg)
+                    await scrape(base, **job.kwargs)
             except asyncio.CancelledError:
                 raise
             except Exception:
diff --git a/src/nodriverplus/core/nodriverplus.py b/src/nodriverplus/core/nodriverplus.py
index a743d40..3cc9112 100644
--- a/src/nodriverplus/core/nodriverplus.py
+++ b/src/nodriverplus/core/nodriverplus.py
@@ -1,8 +1,7 @@
-"""stealth + crawl helpers layered over `nodriver`.
+"""scrape/crawl helpers layered over `nodriver`.
 
 wraps nodriver to add:
 - user agent acquisition + patch (network / emulation / runtime) with headless token scrub
-- stealth scripts (navigator / plugins / workers) auto-applied via Target.setAutoAttach
 - single page scrape (html + optional raw bytes + link extraction)
 - crawl (depth, concurrency, jitter, per-page handler, error + timeout tracking)
 - fetch interception to stream non-text main bodies (e.g. pdf) before chrome consumes them
@@ -11,60 +10,86 @@ wraps nodriver to add:
 keeps low-level access to the underlying nodriver Browser so callers can still drive CDP directly.
 definitely still needs work tho
 """
-import asyncio
-import time
-import random
-from datetime import datetime, UTC
-import json
 import logging
-from os import PathLike
-import asyncio, base64, re
-from urllib.parse import urlparse, urljoin
-import urlcanon
+import os
 import nodriver
-from nodriver import Config, cdp
-from .cdp_helpers import TARGET_DOMAINS, can_use_domain
+from types import CoroutineType
+from nodriver import Config
 from .user_agent import *
-from .scrape_response import *
-from ..utils import extract_links
-from ..js.load import load_text as load_js
-from . import cloudflare
-from datetime import timedelta
-
-logger = logging.getLogger(__name__)
-
+from .scrape_result import *
+from .tab import get_user_agent, scrape, crawl
+from .browser import get, get_with_timeout, stop
+from .manager import Manager
+from .handlers import TargetInterceptor, TargetInterceptorManager, NetworkWatcher
+from .handlers.stock import (
+    UserAgentPatch, 
+    # not working as intended currently
+    # ScrapeRequestPausedHandler,
+    WindowSizePatch,
+    CloudflareSolver,
+)
+
+logger = logging.getLogger("nodriverplus.NodriverPlus")
 
 
 class NodriverPlus:
     """high-level orchestrator for starting a stealthy browser and performing scrapes/crawls.
 
     lifecycle:
-    1. `start()`: launch chrome, fetch + patch user agent, install stealth auto-attach
+    1. `start()`: launch chrome and fetch + patch user agent
     2. `scrape()`: navigate + capture html / links / headers / optional bytes
     3. `crawl()`: customizable nodriver crawling API with depth + concurrency + handler-produced link expansion
     4. `stop()`: shutdown underlying process (optional graceful wait)
 
-    bytes capture: uses Fetch domain interception + fulfill flow to stream non-text main bodies.
     ua patching: applies Network + Emulation overrides + runtime JS patch to sync navigator.* / userAgentData.
-    stealth: early evaluate scripts for pages + workers (plugins, languages, canvas, webdriver flag, etc.).
     """
     browser: nodriver.Browser
     config: nodriver.Config
     user_agent: UserAgent
-    stealth: bool
+    interceptor_manager: TargetInterceptorManager
+
+    def __init__(self, 
+        user_agent: UserAgent = None, 
+        hide_headless: bool = True, 
+        solve_cloudflare: bool = True,
+        *,
+        save_annotated_screenshot: str | os.PathLike = None,
+        interceptors: list[TargetInterceptor] = None,
+        network_watchers: list[NetworkWatcher] = None,
+        manager_concurrency: int = 1,
+    ):
+        """initialize a `NodriverPlus` instance
+        
+        :param user_agent: `UserAgent` to patch browser with 
+        using stock interceptor: `UserAgentInterceptor`
 
-    def __init__(self, user_agent: UserAgent = None, stealth: bool = True):
+        :param interceptors: list of additional custom interceptors to apply
+        """
         self.config = Config()
         self.browser = None
         self.user_agent = user_agent
-        self.stealth = stealth
-
+        self.hide_headless = hide_headless
+        # init interceptor manager and add provided + stock interceptors
+        interceptor_manager = TargetInterceptorManager(interceptors, network_watchers)
+        if user_agent:
+            interceptor_manager.interceptors.append(UserAgentPatch(user_agent, hide_headless))
+        if solve_cloudflare:
+            interceptor_manager.network_watchers.append(CloudflareSolver(save_annotated_screenshot))
+        self.interceptor_manager = interceptor_manager
+        # dedicated queue manager (jobs provide their own per-job crawl/scrape concurrency)
+        self.manager = Manager(concurrency=manager_concurrency)
+
+
+    # TODO: make a `WindowSize` dataclass that can be passed
+    # so that users can specify other meta like
+    # `device_scale_factor`, `mobile`, and `orientation`
     async def start(self,
         config: Config | None = None,
         *,
-        user_data_dir: PathLike | None = None,
+        window_size: tuple[int, int] | None = (1920, 1080),
+        user_data_dir: os.PathLike | None = None,
         headless: bool | None = False,
-        browser_executable_path: PathLike | None = None,
+        browser_executable_path: os.PathLike | None = None,
         browser_args: list[str] | None = None,
         sandbox: bool | None = True,
         lang: str | None = None,
@@ -73,13 +98,18 @@ class NodriverPlus:
         expert: bool | None = None,
         **kwargs: dict | None,
     ) -> nodriver.Browser:
-        """launch a browser and prime stealth / ua state.
+        """launch a browser and prime stock interceptors.
+
+        **specify *`window_size`* to apply a global window size patch.**
+        - applies the correct `browser_args`
+        - and applies the stock `WindowSizePatch` interceptor to the browser
 
-        wraps nodriver.start then optionally fetches + patches the user agent and installs
-        stealth scripts (shadow root auto-attach, navigator tweaks, etc.). returns the raw
+        wraps nodriver.start then optionally fetches + patches the user agent. returns the raw
         nodriver Browser so callers can still use low-level APIs.
 
         :param config: optional pre-built Config.
+        :param window_size: optional window size to apply a global window size patch.
+        :type window_size: pixels: (width, height)
         :param user_data_dir: chrome profile dir.
         :param headless: run in headless mode (we still scrub Headless tokens later).
         :param browser_executable_path: custom chrome path.
@@ -93,6 +123,20 @@ class NodriverPlus:
         :return: started browser instance.
         :rtype: nodriver.Browser
         """
+        browser_args = browser_args or []
+        if window_size:
+            width, height = window_size
+            # remove any existing --window-size arg to avoid dupes
+            browser_args = [
+                arg for arg in browser_args
+                if not arg.startswith("--window-size=")
+            ]
+            browser_args.append(f"--window-size={width},{height}")
+            # add interceptor to manager
+            self.interceptor_manager.interceptors.append(
+                WindowSizePatch(width, height)
+            )
+
         self.browser = await nodriver.start(
             config,
             user_data_dir=user_data_dir,
@@ -107,980 +151,185 @@ class NodriverPlus:
             **kwargs
         )
 
+        # get user agent if none specified
         if not self.user_agent:
-            self.user_agent = await self.get_user_agent(self.browser.main_tab)
-        await self.patch_user_agent(self.browser.main_tab, self.user_agent)
-
-        if self.stealth:
-            await self.setup_stealth(self.browser)
-
-        self.config = self.browser.config
-        return self.browser
-
-
-    async def send_message(
-        self,
-        method: str,
-        params: dict = {},
-        session_id: str = None,
-        connection: nodriver.Tab | nodriver.Connection = None
-    ):
-        """thin helper/patch to send a raw devtools command.
-
-        yields a dict suitable for nodriver's generator-based send() made specifically for this `nodriver` patch: 
-        
-        https://github.com/twarped/nodriver/commit/bf1dfda6cb16a31d2fd302f370f130dda3a3413b
-
-        :param method: full method name (e.g. "Runtime.evaluate").
-        :param params: payload dict; omit if none.
-        :param session_id: target session if addressing a child target.
-        :param connection: override connection (tab or underlying connection).
-        :return: protocol response pair (varies by method) or None.
-        """
-        def c():
-            yield {"method": method, "params": params, "sessionId": session_id}
-
-        if connection:
-            return await connection.send(c())
-        return await self.browser.connection.send(c())
-
-
-    async def autohook_stealth(self, connection: nodriver.Tab | nodriver.Connection, ev: cdp.target.AttachedToTarget = None):
-        """enable Target.setAutoAttach with our filter + resume logic.
-
-        attaches recursively to workers/frames and ensures stealth patches + user agent
-        overrides are applied before scripts run. idempotent per connection.
-
-        :param connection: tab/connection/browsers connection.
-        :param ev: optional original attach event (when recursively called).
-        """
-        types = list(TARGET_DOMAINS.keys())
-        types.remove("tab")
-
-        # hangs on service workers if not deduped
-        if getattr(connection, "_stealth_auto_attached", False):
-            return
-        setattr(connection, "_stealth_auto_attached", True)
-
-        try:
-            await self.send_message("Target.setAutoAttach", {
-                "autoAttach": True,
-                "waitForDebuggerOnStart": True,
-                "flatten": True,
-                "filter": [{"type": t, "exclude": False} for t in types]
-            }, ev.session_id if ev else None)
-        except Exception:
-            logger.exception("auto attach failed for %s:", 
-                f"{ev.target_info.type_} <{ev.target_info.url}>" if ev else connection)
-
-
-    async def on_attach_stealth(self, ev: cdp.target.AttachedToTarget, session: nodriver.Tab | nodriver.Browser | nodriver.Connection):        
-        """handler fired when a new target is auto-attached.
-
-        applies stealth js + ua patch, then resumes the debugger
-
-        :param ev: CDP AttachedToTarget event.
-        :param session: object providing .connection (browser or tab or connection itself).
-        """
-        connection = session.connection if isinstance(session, nodriver.Browser) else session
-
-        logger.debug("successfully attached to %s", f"{ev.target_info.type_} <{ev.target_info.url}>")
-        # apply patches
-        await self.apply_stealth(ev)
-        await self.patch_user_agent(ev, self.user_agent)
-        # recursive attachment
-        await self.autohook_stealth(connection, ev)
-        # continue like normal
-        msg = f"{ev.target_info.type_} <{ev.target_info.url}>"
-        try:
-            await self.send_message("Runtime.runIfWaitingForDebugger", session_id=ev.session_id)
-        except Exception as e:
-            if "-3200" in str(e):
-                logger.warning("too slow resuming %s", msg)
-            else:
-                logger.exception("failed to resume %s:", msg)
-        else:
-            logger.debug("successfully resumed %s", msg)
-
-
-    async def apply_stealth(self, ev: cdp.target.AttachedToTarget):
-        """inject stealth patch into a target (runtime + early document script).
-
-        chooses worker/page variant based on target type.
-
-        :param ev: attach event describing the target.
-        """
-        # load and apply the stealth patch
-        name = "apply_stealth.js"
-        if ev.target_info.type_ in {"service_worker", "shared_worker"}:
-            name = "apply_stealth_worker.js"
-        js = load_js(name)
-        msg = f"{ev.target_info.type_} <{ev.target_info.url}>"
-
-        # try adding the patch to the page
-        try:
-            if can_use_domain(ev.target_info.type_, "Page"):
-                await self.send_message("Page.enable", session_id=ev.session_id)
-                await self.send_message("Page.addScriptToEvaluateOnNewDocument", {
-                    "source": js,
-                    "includeCommandLineAPI": True,
-                    "runImmediately": True
-                }, ev.session_id)
-                logger.debug("successfully added script to %s", msg)
-        except Exception:
-            logger.exception("failed to add script to %s:", msg)
-
-        try:
-            await self.send_message("Runtime.evaluate", {
-                "expression": js,
-                "includeCommandLineAPI": True,
-                "awaitPromise": True
-            }, session_id=ev.session_id)
-        except Exception as e:
-            if "-3200" in str(e):
-                logger.warning("too slow patching %s", msg)
-            else:
-                logger.exception("failed to patch %s:", msg)
-        else:
-            logger.debug("successfully applied patch to %s", msg)
-
-
-    async def setup_stealth(self, session: nodriver.Tab | nodriver.Browser | nodriver.Connection):
-        """one-time setup on a root connection to enable recursive stealth patching.
-
-        :param session: target session for the operation.
-        """
-        connection = session.connection if isinstance(session, nodriver.Browser) else session
-
-        # avoid duping stuff
-        if getattr(connection, "_stealth_initialized", False):
-            return
-        setattr(connection, "_stealth_initialized", True)
-
-        connection.add_handler(cdp.target.AttachedToTarget, self.on_attach_stealth)
-        await self.autohook_stealth(connection)
-
-
-    async def get_user_agent(self, tab: nodriver.Tab):
-        """evaluate helper script in a tab to extract structured user agent data.
-
-        converts returned json into UserAgent / UserAgentMetadata models.
-
-        :param tab: target tab for the operation.
-        :return: structured user agent data.
-        :rtype: UserAgent
-        """
-        js = load_js("get_user_agent.js")
-        ua_data: dict = json.loads(await tab.evaluate(js, await_promise=True))
-        ua_data["metadata"] = UserAgentMetadata(**ua_data["metadata"]) if ua_data.get("metadata") else None
-        user_agent = UserAgent(**ua_data)
-        logger.info("successfully retrieved user agent from %s", tab.url)
-        logger.debug("user agent data retrieved from %s: \n%s", tab.url, user_agent.to_json())
-        return user_agent
-
-
-    async def patch_user_agent(self, 
-        target: cdp.target.AttachedToTarget | nodriver.Tab,
-        user_agent: UserAgent,
-    ):
-        """apply UA overrides across relevant domains for a target.
-
-        removes "Headless" when `self.stealth=True`
-
-        sets Network + Emulation overrides and installs a runtime 
-        patch so navigator + related surfaces align. worker/page aware.
-
-        :param target: tab or AttachedToTarget event.
-        :param user_agent: prepared UserAgent instance.
-        """
-        if self.stealth:
-            user_agent.user_agent = user_agent.user_agent.replace("Headless", "")
-            user_agent.app_version = user_agent.app_version.replace("Headless", "")
-
-        # hacky, but it works
-        if isinstance(target, nodriver.Tab):
-            is_connection = True
-            target_type = "page"
-            msg = f"tab <{target.url}>"
-            target.session_id = None
-        else:
-            is_connection = False
-            target_type = target.target_info.type_
-            msg = f"{target_type} <{target.target_info.url}>"
-        domains_patched = []
-
-        if can_use_domain(target_type, "Network"):
-            await self.send_message(
-                "Network.setUserAgentOverride", 
-                user_agent.to_json(), 
-                target.session_id, 
-                target if is_connection else None
+            user_agent = await get_user_agent(self.browser.main_tab)
+            self.user_agent = user_agent
+        # just in case they added self.user_agent outside of `__init__()`
+        if not any(isinstance(i, UserAgentPatch) for i in self.interceptor_manager.interceptors):
+            self.interceptor_manager.interceptors.append(
+                UserAgentPatch(self.user_agent, self.hide_headless)
             )
-            domains_patched.append("Network")
-        if can_use_domain(target_type, "Emulation"):
-            await self.send_message(
-                "Emulation.setUserAgentOverride", 
-                user_agent.to_json(), 
-                target.session_id, 
-                target if is_connection else None
-            )
-            domains_patched.append("Emulation")
-        if can_use_domain(target_type, "Runtime"):
-            js = load_js("patch_user_agent.js")
-            uaPatch = f"const uaPatch = {user_agent.to_json(True, True)};"
-            await self.send_message("Runtime.evaluate", {
-                "expression": js.replace("//uaPatch//", uaPatch),
-                "includeCommandLineAPI": True,
-            }, target.session_id, target if is_connection else None)
-            domains_patched.append("Runtime")
 
-        if len(domains_patched) == 0:
-            logger.debug("no domains available to patch user agent for %s", msg)
-        else:
-            logger.debug("successfully patched user agent for %s with domains %s", msg, domains_patched)
+        await UserAgentPatch.patch_user_agent(
+            self.browser.main_tab, 
+            None,
+            self.user_agent,
+            self.hide_headless
+        )
 
+        if window_size:
+            width, height = window_size
+            await WindowSizePatch.patch_window_size(
+                self.browser.main_tab,
+                None,
+                width=width,
+                height=height,
+            )
 
-    async def acquire_tab(self,
-        *,
-        base: nodriver.Tab | nodriver.Browser | None = None,
-        new_window: bool = False,
-        new_tab: bool = False,
-        new_context: bool = True,
-        initial_url: str = "about:blank",
-    ) -> nodriver.Tab:
-        """central factory for new/reused tabs/windows/contexts.
+        # start the `TargetInterceptorManager`
+        self.interceptor_manager.connection = self.browser.connection
+        await self.interceptor_manager.start()
 
-        honors combinations of `new_window`/`new_tab`/`new_context` and falls back to `self.browser`.
+        self.config = self.browser.config
 
-        :param base: existing tab or browser (defaults to browser root).
-        :param new_window: request a separate window (may create context).
-        :param new_tab: request a new tab in existing window/context.
-        :param new_context: create an isolated context when opening window.
-        :param initial_url: initial navigation (about:blank by default).
-        :return: acquired tab
-        :rtype: nodriver.Tab
-        """
-        # central place to create/reuse tabs/windows/contexts
-        if base is None:
-            base = self.browser
-        # context+window: gives us isolated storage and a dedicated window
-        if new_window and new_context:
-            try:
-                tab = await self.browser.create_context(initial_url, new_window=True)
-                return tab
-            except Exception:
-                logger.exception("failed creating context window; falling back to plain window")
-        # new standalone window without context
-        if new_window and isinstance(base, nodriver.Browser):
-            return await base.get(initial_url, new_tab=False, new_window=True)
-        # open new tab off a browser root
-        if new_tab and isinstance(base, nodriver.Browser):
-            return await base.get(initial_url, new_tab=True)
-        # base is a tab: delegate; when asking for a new tab it will bubble to browser
-        if isinstance(base, nodriver.Tab):
-            return await base.get(initial_url, new_tab=new_tab, new_window=new_window)
-        # fallback: main tab
-        return self.browser.main_tab
+        return self.browser
 
 
     async def crawl(self,
         url: str,
-        handler: ScrapeResponseHandler = None,
-        depth = 1,
+        scrape_result_handler: ScrapeResultHandler = None,
+        depth: int | None = 1,
+        crawl_result_handler: CrawlResultHandler = None,
         *,
         new_window = False,
-        scrape_bytes = True,
+        # scrape_bytes = True,
         navigation_timeout = 30,
         wait_for_page_load = True,
         page_load_timeout = 60,
         extra_wait_ms = 0,
         concurrency: int = 1,
         max_pages: int | None = None,
-        collect_responses: bool = False,
+        collect_results: bool = False,
         delay_range: tuple[float, float] | None = None,
-        tab_close_timeout: float = 5.0,
-        wait_for_pending_fetch: bool = True,
+        # request_paused_handler: ScrapeRequestPausedHandler = None,
     ):
         """customizable crawl API starting at `url` up to `depth`.
 
         schedules scrape tasks with a worker pool, collects response metadata, errors,
         links, and timing. handler is invoked for each page producing optional new links.
 
+        if `crawl_result_handler` is specified, `crawl_result_handler.handle()` will 
+        be called and awaited before returning the final `CrawlResult`.
+
+        `crawl_result_handler` is nifty if you're crawling with a `Manager`
+        instance.
+
         :param url: root starting point.
-        :param handler: optional ScrapeResponseHandler (auto-created if None).
+        :param scrape_result_handler: optional `ScrapeResultHandler` to be passed to `scrape()`
         :param depth: max link depth (0 means single page).
+        :param crawl_result_handler: if specified, `crawl_result_handler.handle()` 
+        will be called and awaited before returning the final `CrawlResult`.
         :param new_window: isolate crawl in new context+window when True.
-        :param scrape_bytes: capture bytes stream when possible.
         :param navigation_timeout: seconds for initial navigation phase.
         :param wait_for_page_load: await full load event.
         :param page_load_timeout: seconds for load phase.
         :param extra_wait_ms: post-load settle time.
         :param concurrency: worker concurrency.
         :param max_pages: hard cap on processed pages.
-        :param collect_responses: store every ScrapeResponse object.
+        :param collect_results: store every ScrapeResult object.
         :param delay_range: (min,max) jitter before first scrape per worker loop.
         :param tab_close_timeout: seconds to wait closing a tab.
-        :param wait_for_pending_fetch: await outstanding fetch interception tasks.
         :return: crawl summary
         :rtype: CrawlResult
         """
-        if handler is None:
-            if not collect_responses:
-                logger.warning("no handler provided and collect_responses is False, only errors and links will be captured")
-            else:
-                logger.info("no handler provided, using default handler functions")
-            handler = ScrapeResponseHandler()
-
-        # normalize delay range if provided
-        if delay_range is not None:
-            a, b = delay_range
-            if a > b:
-                delay_range = (b, a)
-            if a < 0 or b < 0:
-                delay_range = None  # disallow negative
-        logger.info(
-            "crawl started for %s (depth=%d concurrency=%d max_pages=%s delay=%s)",
-            url, depth, concurrency, max_pages, delay_range,
-        )
-
-        root_url = self.fix_url(url)
-        depth = max(0, depth)
-        concurrency = max(1, concurrency)
-
-        time_start = datetime.now(UTC)
-
-        # queue: (url, remaining_depth)
-        queue: asyncio.Queue[tuple[str, int]] = asyncio.Queue()
-        await queue.put((root_url, depth))
-
-        # visited = fully processed
-        # all_links_set = discovered (even if not yet processed)
-        visited: set[str] = set()
-        all_links: list[str] = [root_url]
-        all_links_set: set[str] = {root_url}
-        successful_links: list[str] = []
-        failed_links: list[FailedLink] = []
-        timed_out_links: list[FailedLink] = []
-        # optional heavy list of every response captured
-        responses: list[ScrapeResponse] = [] if collect_responses else None  # type: ignore
-
-        pages_processed = 0
-        lock = asyncio.Lock()  # protects shared collections when needed
-        # map worker idx -> current url for runtime debugging
-        current_processing: dict[int, str] = {}
-
-        async def should_enqueue(link: str, remaining: int) -> bool:
-            # depth 0 should just be a single scrape
-            if remaining < 0:
-                return False
-            try:
-                link_canon = self.fix_url(link)
-            except Exception:
-                return False
-            if link_canon in visited or link_canon in all_links_set:
-                return False
-            parsed = urlparse(link_canon)
-            # throw interesting links away
-            if parsed.scheme not in ("http", "https"):
-                return False
-            async with lock:  # race-safe insertion into discovery sets
-                if link_canon not in all_links_set:
-                    all_links_set.add(link_canon)
-                    all_links.append(link_canon)
-            return True
-
-        instance: nodriver.Tab | nodriver.Browser = self.browser
-        if new_window:
-            # create a dedicated browser context + window; tabs we spawn will stay in this context
-            instance = await self.acquire_tab(new_window=True, new_context=True)
-
-        async def worker(idx: int):  # noqa: ARG001
-            nonlocal pages_processed
-            while True:
-                try:
-                    # wait for next target
-                    current_url, remaining_depth = await queue.get()
-                except asyncio.CancelledError:
-                    break
-                # register current work for debugging/monitoring
-                try:
-                    current_processing[idx] = current_url
-                except Exception:
-                    current_processing[idx] = str(current_url)
-                if current_url in visited:
-                    # clear current marker before finishing item
-                    current_processing.pop(idx, None)
-                    queue.task_done()
-                    continue
-                visited.add(current_url)
-                if max_pages is not None and pages_processed >= max_pages:
-                    current_processing.pop(idx, None)
-                    queue.task_done()
-                    continue
-                error_obj: Exception | None = None
-                scrape_response: ScrapeResponse | None = None
-                try:
-                    is_first_depth = remaining_depth + 1 == depth
-                    # simple delay / jitter if specified; skip if first scrape
-                    if delay_range is not None and is_first_depth:
-                        wait_time = random.uniform(*delay_range)
-                        logger.info("waiting %.2f seconds before scraping %s", wait_time, current_url)
-                        await asyncio.sleep(wait_time)
-                    scrape_response = await self.scrape(
-                        current_url,
-                        scrape_bytes,
-                        instance,
-                        navigation_timeout=navigation_timeout,
-                        wait_for_page_load=wait_for_page_load,
-                        page_load_timeout=page_load_timeout,
-                        extra_wait_ms=extra_wait_ms,
-                        new_tab=True,
-                        wait_for_pending_fetches=wait_for_pending_fetch,
-                    )
-                    pages_processed += 1
-                    links: list[str] = []
-                    try:
-                        # remember: the handler can mutate links
-                        links = await handler.handle(scrape_response) or []
-                    except Exception as e:
-                        error_obj = e
-                        logger.exception("failed running handler for %s:", current_url)
-                    finally:
-                        async def _safe_close():
-                            # run close in its own task so a hung protocol txn can't block join()
-                            if not scrape_response.tab:
-                                return
-                            # avoid closing the dedicated context's primary tab twice
-                            if new_window and scrape_response.tab is instance:
-                                return
-                            t = asyncio.create_task(scrape_response.tab.close())
-                            try:
-                                await asyncio.wait_for(t, timeout=tab_close_timeout)
-                            except asyncio.TimeoutError:
-                                # target likely crashed/detached before answering Target.closeTarget
-                                logger.warning("timeout closing tab for %s after %.1fs (continuing)", current_url, tab_close_timeout)
-                                t.cancel()
-                            except Exception:
-                                logger.exception("failed closing tab for %s:", current_url)
-                        try:
-                            await _safe_close()
-                        except Exception:
-                            # never let close issues block queue progress
-                            logger.exception("unexpected error in safe close for %s", current_url)
-
-                    # scrape timed out or failed
-                    if scrape_response.timed_out_navigating:
-                        timed_out_links.append(FailedLink(current_url, scrape_response.timed_out_navigating, error_obj))
-                    elif error_obj:
-                        failed_links.append(FailedLink(current_url, scrape_response.timed_out_navigating, error_obj))
-                    else:
-                        # scrape was a success
-                        # record the final URL if the page redirected
-                        final_url = getattr(scrape_response, "url", None) or (scrape_response.tab.url if scrape_response.tab else current_url)
-                        try:
-                            final_url = self.fix_url(final_url)
-                        except Exception:
-                            final_url = final_url or current_url
-
-                        async with lock:
-                            if final_url not in all_links_set:
-                                all_links_set.add(final_url)
-                                all_links.append(final_url)
-
-                        successful_links.append(final_url)
-
-                    if collect_responses and scrape_response:
-                        async with lock:
-                            responses.append(scrape_response)
-
-                    next_remaining = remaining_depth - 1
-                    # depth 0 should still be allowed because depth
-                    # 1 should be actually scraping with a depth
-                    if next_remaining > -1 and links:
-                        for link in links:
-                            if max_pages is not None and pages_processed >= max_pages:
-                                break
-                            if await should_enqueue(link, next_remaining):
-                                await queue.put((self.fix_url(link), next_remaining))
-                except Exception as e:
-                    failed_links.append(FailedLink(current_url, False, e))
-                    logger.exception("unexpected error during crawl for %s", current_url)
-                finally:
-                    # clear current marker and mark as done so join can finish
-                    current_processing.pop(idx, None)
-                    queue.task_done()
-
-        workers = [asyncio.create_task(worker(i)) for i in range(concurrency)]
-
-        # wait until everything is finished
-        await queue.join()
-        for w in workers:
-            w.cancel()
-        # we're trying to cancel everything
-        # so ignore CancelledError
-        for w in workers:
-            try:
-                await w
-            except asyncio.CancelledError:
-                pass
-
-        if new_window and isinstance(instance, nodriver.Tab):
-            # close the dedicated context tab with a timeout to avoid hangs
-            try:
-                t = asyncio.create_task(instance.close())
-                await asyncio.wait_for(t, timeout=5)
-            except asyncio.TimeoutError:
-                logger.warning("timeout closing dedicated context tab (continuing)")
-                t.cancel()
-            except Exception:
-                logger.debug("failed closing dedicated context tab (already closed?)")
-        time_end = datetime.now(UTC)
-        result = CrawlResult(
-            links=all_links,
-            successful_links=successful_links,
-            failed_links=failed_links,
-            timed_out_links=timed_out_links,
-            time_start=time_start,
-            time_end=time_end,
-            time_elapsed=time_end - time_start,
-            responses=responses,
-        )
-        logger.info(
-            "successfully finished crawl for %s (pages=%d success=%d failed=%d timed_out=%d elapsed=%.2fs)",
-            url,
-            len(all_links),
-            len(successful_links),
-            len(failed_links),
-            len(timed_out_links),
-            (time_end - time_start).total_seconds(),
+        # :param scrape_bytes: capture bytes stream when possible.
+        # :param request_paused_handler: custom fetch interception handler.
+        # **must be a type**—not an instance—so that it can be initiated later with the correct values attached
+        # :type request_paused_handler: type[ScrapeRequestPausedHandler]
+
+        return await crawl(
+            self.browser,
+            url=url,
+            scrape_result_handler=scrape_result_handler,
+            depth=depth,
+            crawl_result_handler=crawl_result_handler,
+            new_window=new_window,
+            # scrape_bytes=scrape_bytes,
+            navigation_timeout=navigation_timeout,
+            wait_for_page_load=wait_for_page_load,
+            page_load_timeout=page_load_timeout,
+            extra_wait_ms=extra_wait_ms,
+            concurrency=concurrency,
+            max_pages=max_pages,
+            collect_results=collect_results,
+            delay_range=delay_range,
+            # request_paused_handler=request_paused_handler,
         )
-        return result
 
 
-    # TODO: refactor NodriverPlus to pull from 
-    # dedicated `Tab` functions like this
     async def scrape(self, 
         url: str,
-        scrape_bytes = True,
-        existing_tab: nodriver.Tab | None = None,
+        # scrape_bytes = True,
+        scrape_result_handler: ScrapeResultHandler | None = None,
         *,
         navigation_timeout = 30,
         wait_for_page_load = True,
         page_load_timeout = 60,
         extra_wait_ms = 0,
-        # solve_cloudflare = True, # not implemented yet
         new_tab = False,
         new_window = False,
-        wait_for_pending_fetches: bool = True,
+        # request_paused_handler = ScrapeRequestPausedHandler,
+        proxy_server: str = None,
+        proxy_bypass_list: list[str] = None,
+        origins_with_universal_network_access: list[str] = None,
     ):
         """single page scrape (html + optional bytes + link extraction).
 
         handles navigation, timeouts, fetch interception, html capture, link parsing and
         cleanup (tab closure, pending task draining).
 
+        if `scrape_result_handler` is provided, `scrape_result_handler.handle()` will
+        be called and awaited before returning the final `ScrapeResult`.
+
+        - **`proxy_server`** — (EXPERIMENTAL) (Optional) Proxy server, similar to the one passed to --proxy-server
+        - **`proxy_bypass_list`** — (EXPERIMENTAL) (Optional) Proxy bypass list, similar to the one passed to --proxy-bypass-list
+        - **`origins_with_universal_network_access`** — (EXPERIMENTAL) (Optional) An optional list of origins to grant unlimited cross-origin access to. Parts of the URL other than those constituting origin are ignored.
+
+        ### not implemented yet:
+        - `scrape_result_handler` could be useful if you want to execute stuff on `tab`
+        after the page loads, but before the `RequestPausedHandler` is removed
+
         :param url: target url.
-        :param scrape_bytes: capture non-text body bytes.
+        :param scrape_result_handler: if specified, `scrape_result_handler.handle()` will be called 
         :param existing_tab: reuse provided tab/browser root or create fresh.
+        and awaited before returning the final `ScrapeResult`.
         :param navigation_timeout: seconds for initial navigation.
         :param wait_for_page_load: await full load event.
         :param page_load_timeout: seconds for load phase.
         :param extra_wait_ms: post-load wait for dynamic content.
         :param new_tab: request new tab.
         :param new_window: request isolated window/context.
-        :param wait_for_pending_fetches: await in-flight fetch interception tasks.
+        :param proxy_server: (EXPERIMENTAL) (Optional) Proxy server, similar to the one passed to --proxy-server
+        :param proxy_bypass_list: (EXPERIMENTAL) (Optional) Proxy bypass list, similar to the one passed to --proxy-bypass-list
+        :param origins_with_universal_network_access: (EXPERIMENTAL) (Optional) An optional list of origins to grant unlimited cross-origin access to. Parts of the URL other than those constituting origin are ignored.
         :return: html/links/bytes/metadata
-        :rtype: ScrapeResponse
+        :rtype: ScrapeResult
         """
-        start = time.monotonic()
-        pending_tasks: set[asyncio.Task] = set()
-        target = existing_tab or self.browser
-        url = self.fix_url(url)
-
-        scrape_response = ScrapeResponse(url)
-        parsed_url = urlparse(url)
-
-        if target is None:
-            raise ValueError("browser was never started! start with `NodriverPlus.start(**kwargs)`")
-        # central acquisition
-        tab = await self.acquire_tab(
-            base=target, 
-            new_window=new_window, 
-            new_tab=new_tab, 
-        )
-        scrape_response.tab = tab
-        logger.info("scraping %s", url)
-
-        # network prep:
-        # cache is disabled so we can actually get bytes
-        await tab.send(cdp.network.enable())
-        await tab.send(cdp.network.set_cache_disabled(True))
-        await tab.send(
-            cdp.network.set_extra_http_headers(
-                headers=cdp.network.Headers({
-                    "Referer": f"{parsed_url.scheme}://{parsed_url.netloc}",
-                })
-            )
+        # :param scrape_bytes: capture non-text body bytes.
+        # :param request_paused_handler: custom fetch interception handler.
+        # **must be a type**—not an instance—so that it can be initiated later with the correct values attached
+        # :type request_paused_handler: type[ScrapeRequestPausedHandler]
+        
+        return await scrape(
+            base=self.browser,
+            url=url,
+            # scrape_bytes=scrape_bytes,
+            scrape_result_handler=scrape_result_handler,
+            navigation_timeout=navigation_timeout,
+            wait_for_page_load=wait_for_page_load,
+            page_load_timeout=page_load_timeout,
+            extra_wait_ms=extra_wait_ms,
+            new_tab=new_tab,
+            new_window=new_window,
+            # request_paused_handler=request_paused_handler,
+            proxy_server=proxy_server,
+            proxy_bypass_list=proxy_bypass_list,
+            origins_with_universal_network_access=origins_with_universal_network_access
         )
 
-        # we want the headers for the ScrapeResponse
-        async def on_response(ev: cdp.network.ResponseReceived):
-            if ev.response.url != url:
-                return
-            scrape_response.headers = {k.lower(): v for k, v in (ev.response.headers or {}).items()}
-            scrape_response.mime = ev.response.mime_type.lower().split(";", 1)[0].strip()
-            # one-shot: we just need the main response
-            tab.remove_handler(cdp.network.ResponseReceived, on_response)
-        tab.add_handler(cdp.network.ResponseReceived, on_response)
-
-        # catch those bytes and keep a reference to the handler so we can remove it
-        fetch_handler = None
-        if scrape_bytes:
-            # per-scrape dedupe state so concurrent scrapes don't collide
-            active_fetch_interceptions: set[str] = set()
-            active_fetch_lock: asyncio.Lock = asyncio.Lock()
-            fetch_handler = await self.scrape_bytes(
-                url, tab, scrape_response, pending_tasks,
-                active_fetch_interceptions=active_fetch_interceptions,
-                active_fetch_lock=active_fetch_lock,
-            )
-
-        error_obj: Exception = None
-        try:
-            nav_response = await self.get_with_timeout(tab, url, 
-                navigation_timeout=navigation_timeout, 
-                wait_for_page_load=wait_for_page_load, 
-                page_load_timeout=page_load_timeout, 
-                extra_wait_ms=extra_wait_ms
-            )
-            scrape_response.timed_out = nav_response.timed_out
-            scrape_response.timed_out_navigating = nav_response.timed_out_navigating
-            scrape_response.timed_out_loading = nav_response.timed_out_loading
-            if nav_response.timed_out_navigating:
-                return scrape_response
-
-            # prefer the tab's final URL (handles redirects) and record it on the ScrapeResponse
-            final_url = nav_response.tab.url if getattr(nav_response, "tab", None) and nav_response.tab.url else url
-            scrape_response.url = self.fix_url(final_url)
-
-            # if it's taking forever to load, get_content() will also take forever to load
-            if scrape_response.timed_out_loading:
-                # fast path: avoid get_content() which can hang when load timed out
-                val = await scrape_response.tab.evaluate("document.documentElement.outerHTML")
-                if isinstance(val, cdp.runtime.ExceptionDetails):
-                    # capture for caller logging; keep html empty string so downstream link extraction is safe
-                    error_obj = nodriver.ProtocolException(val)
-                    logger.warning("failed evaluating outerHTML for %s after load timeout; treating as empty doc", url)
-                    scrape_response.html = ""
-                else:
-                    scrape_response.html = val if isinstance(val, str) else str(val)
-            else:
-                scrape_response.html = await nav_response.tab.get_content()
-            # use the final URL as the base for extracting links
-            scrape_response.links = extract_links(scrape_response.html, final_url)
-            if cloudflare.should_wait(scrape_response.html):
-                logger.info("detected potentially interactable cloudflare challenge in %s", url)
-        except Exception as e:
-            error_obj = e
-        finally:
-            # ensure we remove any fetch handler we installed
-            if fetch_handler is not None:
-                try:
-                    tab.remove_handler(cdp.fetch.RequestPaused, fetch_handler)
-                except Exception:
-                    logger.debug("failed removing fetch handler during scrape cleanup for %s", url)
-                # wait for any in-flight fetch tasks to finish (bounded)
-                if wait_for_pending_fetches:
-                    try:
-                        # copy the set because tasks remove themselves when done
-                        tasks_to_wait = set(pending_tasks)
-                        if tasks_to_wait:
-                            logger.debug("waiting for %d pending fetch tasks for %s", len(tasks_to_wait), url)
-                            done, pending = await asyncio.wait(tasks_to_wait, timeout=2)
-                            if pending:
-                                logger.debug("cancelling %d pending fetch tasks for %s", len(pending), url)
-                                for t in pending:
-                                    try:
-                                        t.cancel()
-                                    except Exception:
-                                        pass
-                                # give cancelled tasks a moment to finish
-                                try:
-                                    await asyncio.wait(pending, timeout=1)
-                                except Exception:
-                                    pass
-                    except Exception:
-                        logger.debug("error while waiting for cancelling pending fetch tasks for %s", url)
-                else:
-                    # best-effort cancel outstanding tasks quickly
-                    for t in list(pending_tasks):
-                        if not t.done():
-                            t.cancel()
-                    # no wait; we are intentionally dropping them
-
-        scrape_response.elapsed = timedelta(seconds=time.monotonic() - start)
-        elapsed_seconds = scrape_response.elapsed.total_seconds()
-        if error_obj is not None:
-            logger.exception("unexpected error during scrape for %s (elapsed=%.2fs): %s", url, elapsed_seconds, error_obj)
-        else:
-            logger.info("successfully finished scrape for %s (elapsed=%.2fs)", url, elapsed_seconds)
-
-        return scrape_response
-    
-
-    async def scrape_bytes(self, 
-        url: str, 
-        tab: nodriver.Tab, 
-        scrape_response: ScrapeResponse,
-        pending_tasks: set[asyncio.Task],
-        *,
-        active_fetch_interceptions: set[str],
-        active_fetch_lock: asyncio.Lock,
-    ):
-        """install fetch interception handlers to capture raw response bytes.
-
-        streams non-text main navigation bodies (e.g. pdfs) into scrape_response.bytes_.
-
-        :param url: navigation url (for main nav tracking).
-        :param tab: active tab.
-        :param scrape_response: response accumulator to populate.
-        :param pending_tasks: set collecting async tasks for cleanup.
-        :param active_fetch_interceptions: dedupe set for in-flight interceptions.
-        :param active_fetch_lock: lock protecting interception state.
-        :return: handler function to unregister later.
-        """
-        await tab.send(cdp.fetch.enable())
-        chunks, total_len = [], None
-        # track interception lifecycle so we never double-continue
-        intercept_states: dict[str, dict] = {}
-        # track main navigation across redirects so we only stream final doc once
-        main_nav_initial_url = url
-        main_nav_current_url = url
-        main_nav_request_id: str | None = None
-        redirect_chain: list[str] = []
-        main_nav_done = False
-        # the original handler logic is placed in an inner coroutine so the
-        # public handler can create a Task and register it in
-        # `self._pending_fetch_tasks`. this ensures tasks are awaited at the
-        # end of the scrape and not left pending when the tab/connection
-        # closes.
-        async def _on_fetch(ev: cdp.fetch.RequestPaused):
-            nonlocal total_len, main_nav_initial_url, main_nav_current_url, main_nav_request_id, main_nav_done
-
-            # ignore late events after we've finalized main navigation
-            if main_nav_done:
-                return
-
-            if ev.response_status_code is None:
-                request_headers = {k.lower(): v for k, v in (ev.request.headers or {}).items()}
-                logger.debug("successfully intercepted %s request for %s", ev.request.method,ev.request.url)
-                scrape_response.intercepted_requests[ev.request.url] = ScrapeRequestIntercepted(
-                    url=ev.request.url,
-                    headers=request_headers,
-                    method=ev.request.method,
-                )
-
-                # remove range header
-                request_headers.pop("range", None)
-
-                # dedupe concurrent handling of the same interception id using per-scrape lock/set
-                req_id = ev.request_id
-                async with active_fetch_lock:
-                    if req_id in active_fetch_interceptions:
-                        logger.debug("skipping duplicate continue_request for %s (id=%s)", ev.request.url, req_id)
-                        return
-                    active_fetch_interceptions.add(req_id)
-
-                try:
-                    if req_id in intercept_states:
-                        logger.debug("duplicate request phase for %s (id=%s) - skipping", ev.request.url, req_id)
-                    else:
-                        is_main_nav = ev.request.url == main_nav_current_url
-                        intercept_states[req_id] = {"phase": "request", "url": ev.request.url, "main_nav": is_main_nav}
-                        if is_main_nav:
-                            main_nav_request_id = req_id
-                        # ask for response interception for everything (simpler) but state machine will gate actions
-                        await tab.send(cdp.fetch.continue_request(
-                            ev.request_id,
-                            headers=[cdp.fetch.HeaderEntry(name=k, value=v) for k,v in request_headers.items()],
-                            intercept_response=True,
-                        ))
-                        intercept_states[req_id]["phase"] = "waiting_response"
-                        logger.debug("continued %s request for %s%s", ev.request.method, ev.request.url, " (main_nav)" if is_main_nav else "")
-                except Exception as exc:
-                    msg = str(exc)
-                    if isinstance(exc, nodriver.ProtocolException):
-                        logger.warning("request phase race for %s (id=%s): %s", ev.request.url, req_id, msg)
-                    else:
-                        logger.exception("failed continuing request %s (id=%s)", ev.request.url, req_id)
-                finally:
-                    async with active_fetch_lock:
-                        active_fetch_interceptions.discard(req_id)
-                return
-            
-            response_headers = {h.name.lower(): h.value for h in ev.response_headers}
-
-            logger.debug("intercepted response for %s", ev.request.url)
-
-            mime = response_headers.get("content-type", "").split(";",1)[0].strip().lower()
-            scrape_response.intercepted_responses[ev.request.url] = ScrapeResponseIntercepted(
-                url=ev.request.url,
-                mime=mime,
-                headers=response_headers,
-                method=ev.request.method,
-            )
-
-            # redirect handling for main navigation: update current url + state, never stream redirect bodies
-            if (
-                ev.response_status_code is not None and 300 <= ev.response_status_code < 400 and
-                intercept_states.get(ev.request_id, {}).get("main_nav")
-            ):
-                loc = response_headers.get("location")
-                if loc:
-                    try:
-                        redirect_url = urljoin(ev.request.url, loc)
-                        redirect_chain.append(redirect_url)
-                        scrape_response.url = redirect_url  # expose latest
-                        logger.info("detected redirect %s -> %s", ev.request.url, redirect_url)
-                        # update tracking so subsequent request is considered main nav
-                        main_nav_current_url = redirect_url
-                        # complete this interception; next request will set new main_nav_request_id
-                        state = intercept_states.get(ev.request_id)
-                        if state: state["phase"] = "done"
-                        await tab.send(cdp.fetch.continue_response(ev.request_id))
-                        logger.debug("continued response for redirect %s", ev.request.url)
-                    except Exception:
-                        logger.exception("failed handling redirect for %s", ev.request.url)
-                else:
-                    logger.debug("redirect status without location for %s", ev.request.url)
-                return
-
-            # we only want the main content
-            # and to save memory, we'll skip the bytes if it's just text
-            text_types = { "text", "javascript", "json", "xml" }
-            if (
-                ev.request.url != main_nav_current_url
-                or any(t in mime for t in text_types)
-            ):
-                # continue response (dedupe + handle ProtocolException)
-                resp_id = ev.request_id
-                async with active_fetch_lock:
-                    if resp_id in active_fetch_interceptions:
-                        logger.debug("skipping duplicate continue_response for %s (id=%s)", ev.request.url, resp_id)
-                        return
-                    active_fetch_interceptions.add(resp_id)
-
-                try:
-                    state = intercept_states.get(resp_id)
-                    if not state or state.get("phase") == "done":
-                        logger.debug("stale response phase for %s (id=%s) - skipping", ev.request.url, resp_id)
-                        return
-                    await tab.send(cdp.fetch.continue_response(ev.request_id))
-                    state["phase"] = "done"
-                    logger.debug("continued response %s with mime %s", ev.request.url, mime)
-                    return
-                except Exception as exc:
-                    msg = str(exc)
-                    if isinstance(exc, nodriver.ProtocolException):
-                        logger.debug("response phase race for %s (id=%s): %s", ev.request.url, resp_id, msg)
-                        return
-                    logger.exception("failed to continue response %s with mime %s", ev.request.url, mime)
-                    return
-                finally:
-                    async with active_fetch_lock:
-                        active_fetch_interceptions.discard(resp_id)
-
-            # take the bytes
-            logger.info("taking response body as stream for %s", ev.request.url)
-            state = intercept_states.get(ev.request_id)
-            if state and state.get("phase") == "done":
-                logger.debug("already completed interception for %s (id=%s) - skipping body", ev.request.url, ev.request_id)
-                return
-            stream = await tab.send(cdp.fetch.take_response_body_as_stream(ev.request_id))
-            buf = bytearray()
-            while True:
-                b64, data, eof = await tab.send(cdp.io.read(handle=stream))
-                buf.extend(base64.b64decode(data) if b64 else bytes(data, "utf-8"))
-                if eof: break
-            await tab.send(cdp.io.close(handle=stream))
-
-            # pretend nothing ever happened
-            # re-serve it to complete the request
-            try:
-                await tab.send(
-                    cdp.fetch.fulfill_request(ev.request_id,
-                        response_code=ev.response_status_code,
-                        response_headers=ev.response_headers,
-                        body=base64.b64encode(buf).decode(),
-                        response_phrase=ev.response_status_text if ev.response_status_text else None,
-                    )
-                )
-                logger.info("successfully fulfilled response %s for %s", ev.request.url, mime)
-            except Exception as exc:
-                msg = str(exc)
-                benign = isinstance(exc, nodriver.ProtocolException) and (
-                    "Invalid InterceptionId" in msg or "Invalid state for continueInterceptedRequest" in msg or "Inspected target navigated or closed" in msg
-                )
-                if benign:
-                    logger.debug("benign race fulfilling %s (id=%s): %s", ev.request.url, ev.request_id, exc)
-                else:
-                    logger.exception("failed to fulfill response %s for %s", ev.request.url, mime)
-
-            # handle 206 chunks if server insists
-            cr = response_headers.get("content-range")
-            if cr and (m := re.compile(r"bytes (\d+)-(\d+)/(\d+)").match(cr)):
-                start, total_len = int(m.group(1)), int(m.group(3))
-                chunks.append((start, buf))
-                if sum(len(b) for _, b in chunks) < total_len:
-                    return
-                buf = bytearray(total_len)
-                for s, b in chunks: buf[s:s+len(b)] = b
-
-            scrape_response.bytes_ = bytes(buf)
-            state = intercept_states.get(ev.request_id)
-            if state:
-                state["phase"] = "done"
-            logger.info("successfully saved bytes for %s", ev.request.url)
-            # mark navigation done so we ignore any stray late events
-            if intercept_states.get(ev.request_id, {}).get("main_nav"):
-                main_nav_done = True
-
-        # wrapper passed to add_handler - schedules the real coroutine as a Task
-        def on_fetch(ev: cdp.fetch.RequestPaused):
-            task = asyncio.create_task(_on_fetch(ev))
-            # register so scrape() can await outstanding tasks before finishing
-            pending_tasks.add(task)
-            task.add_done_callback(lambda t: pending_tasks.discard(t))
-            return None
-
-        tab.add_handler(cdp.fetch.RequestPaused, on_fetch)
-        return on_fetch
-
-    async def wait_for_page_load(self, tab: nodriver.Tab, extra_wait_ms: int = 0):
-        """wait for load event (or immediate if already complete) then optional delay.
-
-        :param tab: target tab.
-        :param extra_wait_ms: additional ms sleep via setTimeout after load.
-        """
-        await tab.evaluate("""
-            new Promise(r => {
-                if (document.readyState === "complete") {
-                    r();
-                } else {
-                    window.addEventListener("load", r);
-                }
-            })
-        """, await_promise=True)
-        logger.debug("successfully finished loading %s", tab.url)
-        if extra_wait_ms:
-            logger.debug("waiting extra %d ms for %s", extra_wait_ms, tab.url)
-            await tab.evaluate(
-                f"new Promise(r => setTimeout(r, {extra_wait_ms}));",
-                await_promise=True
-            )
-
 
     async def get_with_timeout(self, 
-        tab: nodriver.Tab | nodriver.Browser, 
         url: str, 
         *,
         navigation_timeout = 30,
@@ -1089,10 +338,10 @@ class NodriverPlus:
         extra_wait_ms = 0,
         new_tab = False,
         new_window = False
-    ):
+    ) -> CoroutineType[any, any, ScrapeResult]:
         """navigate with separate navigation + load timeouts.
 
-        returns a partial ScrapeResponse (timing + timeout flags + tab ref)
+        returns a partial ScrapeResult (timing + timeout flags + tab ref)
 
         :param tab: existing tab or browser root.
         :param url: target url.
@@ -1102,75 +351,116 @@ class NodriverPlus:
         :param extra_wait_ms: post-load wait for dynamic content.
         :param new_tab: request new tab first.
         :param new_window: request new window/context first.
-        :return: partial ScrapeResponse (timing + timeout flags).
-        :rtype: ScrapeResponse
+        :return: partial ScrapeResult (timing + timeout flags).
+        :rtype: ScrapeResult
         """
-        scrape_response = ScrapeResponse(url, tab, True)
-
-        start = time.monotonic()
-        # prepare target first when we need a new tab/window/context
-        base = tab
-        if new_tab or new_window:
-            try:
-                base = await self.acquire_tab(base=tab if isinstance(tab, nodriver.Tab) else None,
-                    new_window=new_window, new_tab=new_tab, new_context=new_window, initial_url="about:blank")
-            except Exception:
-                logger.exception("failed acquiring tab; falling back to provided target")
-        nav_task = asyncio.create_task(base.get(url))
-        try:
-            # cancelling nav_task will cause throw an InvalidStateError
-            # if the Transaction hasn't finished yet
-            base = await asyncio.wait_for(asyncio.shield(nav_task), timeout=navigation_timeout)
-            scrape_response.tab = base
-        except asyncio.TimeoutError:
-            scrape_response.timed_out_navigating = True
-            scrape_response.elapsed = timedelta(seconds=time.monotonic() - start)
-            logger.warning("timed out getting %s (navigation phase) (elapsed=%.2fs)", url, scrape_response.elapsed.total_seconds())
-            return scrape_response
+        return get_with_timeout(
+            target=self.browser,
+            url=url,
+            navigation_timeout=navigation_timeout,
+            wait_for_page_load=wait_for_page_load,
+            page_load_timeout=page_load_timeout,
+            extra_wait_ms=extra_wait_ms,
+            new_tab=new_tab,
+            new_window=new_window
+        )
+    
 
-        if wait_for_page_load:    
-            load_task = asyncio.create_task(self.wait_for_page_load(base, extra_wait_ms))
-            try:
-                # same thing here
-                await asyncio.wait_for(
-                    asyncio.shield(load_task), 
-                    timeout=page_load_timeout + extra_wait_ms / 1000
-                )
-            except asyncio.TimeoutError:
-                scrape_response.timed_out_loading = True
-                scrape_response.elapsed = timedelta(seconds=time.monotonic() - start)
-                logger.warning("timed out getting %s (load phase) (elapsed=%.2fs)", url, scrape_response.elapsed.total_seconds())
-                # wait for task to actually cancel
-                if not load_task.done():
-                    load_task.cancel()
-                    try:
-                        await load_task
-                    except asyncio.CancelledError:
-                        pass
-                return scrape_response
+    async def get(self,
+        url: str = "about:blank",
+        *,
+        new_tab: bool = False,
+        new_window: bool = True,
+        new_context: bool = True,
+        dispose_on_detach: bool = True,
+        proxy_server: str = None,
+        proxy_bypass_list: list[str] = None,
+        origins_with_universal_network_access: list[str] = None,
+    ):
+        """central factory for new/reused tabs/windows/contexts.
 
-        scrape_response.timed_out = False
-        scrape_response.elapsed = timedelta(seconds=time.monotonic() - start)
-        return scrape_response
+        honors combinations of `new_window`/`new_tab`/`new_context` on `base`.
+        
+        see https://github.com/twarped/nodriver/commit/1dcb52e8063bad359a3f2978b83f44e20dfbca68
 
-    def fix_url(self, url: str):
-        """make the url more like how chrome would make it
+        - **`dispose_on_detach`** — (EXPERIMENTAL) (Optional) If specified, disposes this context when debugging session disconnects.
+        - **`proxy_server`** — (EXPERIMENTAL) (Optional) Proxy server, similar to the one passed to --proxy-server
+        - **`proxy_bypass_list`** — (EXPERIMENTAL) (Optional) Proxy bypass list, similar to the one passed to --proxy-bypass-list
+        - **`origins_with_universal_network_access`** — (EXPERIMENTAL) (Optional) An optional list of origins to grant unlimited cross-origin access to. Parts of the URL other than those constituting origin are ignored.
 
-        :param url: raw url.
-        :return: fixed url
-        :rtype: str
+        :param url: initial navigation (about:blank by default).
+        :param new_window: request a separate window (may create context).
+        :param new_tab: request a new tab in existing window/context.
+        :param new_context: create an isolated context when opening window.
+        :param dispose_on_detach: (EXPERIMENTAL) (Optional) If specified, disposes this context when debugging session disconnects.
+        :param proxy_server: (EXPERIMENTAL) (Optional) Proxy server, similar to the one passed to --proxy-server
+        :param proxy_bypass_list: (EXPERIMENTAL) (Optional) Proxy bypass list, similar to the one passed to --proxy-bypass-list
+        :param origins_with_universal_network_access: (EXPERIMENTAL) (Optional) An optional list of origins to grant unlimited cross-origin access to. Parts of the URL other than those constituting origin are ignored.
+        :return: acquired/created tab
+        :rtype: Tab
         """
-        return urlcanon.google.canonicalize(url).__str__()
+        return get(
+            base=self.browser,
+            url=url,
+            new_tab=new_tab,
+            new_window=new_window,
+            new_context=new_context,
+            dispose_on_detach=dispose_on_detach,
+            proxy_server=proxy_server,
+            proxy_bypass_list=proxy_bypass_list,
+            origins_with_universal_network_access=origins_with_universal_network_access
+        )
 
 
     async def stop(self, graceful = True):
-        """stop browser process (optionally wait for graceful exit).
+        """stop browser and underlying `Manager` process (optionally wait for graceful exit).
 
         :param graceful: wait for underlying process to exit.
         """
-        logger.info("stopping browser")
-        self.browser.stop()
-        if graceful:
-            logger.info("waiting for graceful shutdown")
-            await self.browser._process.wait()
-        logger.info("successfully shutdown browser")
\ No newline at end of file
+        await self.manager.stop()
+        await self.interceptor_manager.stop()
+        await stop(self.browser, graceful)
+
+
+    def enqueue_crawl(self, *args, **kwargs):
+        """enqueue a crawl job using the internal `Manager`.
+
+        auto-starts manager loop on first use if not already running.
+        first positional argument must be the seed url.
+        """
+        if not self.browser:
+            raise RuntimeError("browser not started")
+        # manager expects (base, url, ...)
+        if self.manager._runner_task is None:
+            self.manager.start()
+        self.manager.enqueue_crawl(self.browser, *args, **kwargs)
+
+
+    def enqueue_scrape(self, *args, **kwargs):
+        """enqueue a scrape job using the internal `Manager`.
+
+        auto-starts manager loop on first use if not already running.
+        first positional argument must be the target url.
+        """
+        if not self.browser:
+            raise RuntimeError("browser not started")
+        if self.manager._runner_task is None:
+            self.manager.start()
+        self.manager.enqueue_scrape(self.browser, *args, **kwargs)
+
+
+    async def wait_for_queue(self, timeout: float | None = None):
+        """await completion of all queued manager jobs."""
+        await self.manager.wait_for_queue(timeout)
+
+
+    async def start_manager(self, concurrency: int | None = None):
+        """start the internal `Manager` loop if not already running."""
+        if self.manager._runner_task is None:
+            self.manager.concurrency = concurrency or self.manager.concurrency
+            self.manager.start()
+
+
+    async def stop_manager(self, timeout: float | None = None):
+        """gracefully stop internal manager and export remaining jobs."""
+        return await self.manager.stop(timeout)
\ No newline at end of file
diff --git a/src/nodriverplus/core/scrape_response.py b/src/nodriverplus/core/scrape_result.py
similarity index 52%
rename from src/nodriverplus/core/scrape_response.py
rename to src/nodriverplus/core/scrape_result.py
index 931348d..45a27ea 100644
--- a/src/nodriverplus/core/scrape_response.py
+++ b/src/nodriverplus/core/scrape_result.py
@@ -1,5 +1,3 @@
-import asyncio
-from typing import Callable, Coroutine
 import nodriver
 from datetime import datetime, timedelta
 
@@ -7,9 +5,12 @@ from datetime import datetime, timedelta
 
 mirrors style in `nodriverplus` core: simple models (responses, requests, crawl results)
 and pluggable handler classes that callers can override / monkeypatch.
+
+**TODO**:
+- fix bytes streaming (currently disabled due to cloudflare issues)
 """
 
-class ScrapeResponseIntercepted:
+class InterceptedResponseMeta:
     """captured response metadata during fetch interception.
 
     populated when bytes streaming is enabled; omits body to save memory.
@@ -29,13 +30,13 @@ class ScrapeResponseIntercepted:
         self.headers = headers
         self.method = method
 
-class ScrapeRequestIntercepted:
+class InterceptedRequestMeta:
     """captured request metadata during fetch interception.
 
-    complements ScrapeResponseIntercepted for debugging / audit.
+    complements InterceptedResponseMeta for debugging / audit.
 
     :param url: request url.
-    :param headers: outgoing headers (lowercased keys; range removed).
+    :param headers: outgoing headers
     :param method: http method.
     """
     url: str
@@ -46,11 +47,11 @@ class ScrapeRequestIntercepted:
         self.headers = headers
         self.method = method
 
-class ScrapeResponse:
+class ScrapeResult:
     """primary result object for a single page scrape.
 
     holds html, optional raw bytes (for non-text main resource), link list, timing + timeout flags,
-    headers + mime, and per-request/response interception metadata.
+    headers + mime, redirect chain, and per-request/response interception metadata.
 
     fields are intentionally public and lightly typed for handler mutation.
 
@@ -60,27 +61,35 @@ class ScrapeResponse:
     :param timed_out_navigating: navigation phase timed out.
     :param timed_out_loading: load event phase timed out.
     :param html: captured html ("" when load timeout + outerHTML fallback failed).
-    :param bytes_: raw body bytes (only for non-text types when streamed).
     :param mime: mime of main response.
     :param headers: main response headers (lowercased keys).
-    :param intercepted_responses: map of url->ScrapeResponseIntercepted.
-    :param intercepted_requests: map of url->ScrapeRequestIntercepted.
+    :param intercepted_responses: map of url->InterceptedResponseMeta.
+    :param intercepted_requests: map of url->InterceptedRequestMeta.
+    :param redirect_chain: ordered list of redirect target locations (raw values from Location headers).
     :param elapsed: timedelta duration of the scrape.
+    :param current_depth: mostly used by `crawl()` to pass on to handlers:
+    depth of this scrape in the overall crawl (0 = seed).
     """
+    # bytes_ is disabled due to cloudflare+request interception issues.
+    # :param `bytes_`: raw body bytes (only for non-text types when streamed).
     url: str
     tab: nodriver.Tab
     timed_out: bool
     timed_out_navigating: bool
     timed_out_loading: bool
     html: str
-    bytes_: bytes
+    # bytes_: bytes
     # avoid attribute errors for handlers expecting .links
     links: list[str] = []
     mime: str
     headers: dict
-    intercepted_responses: dict[str, ScrapeResponseIntercepted]
-    intercepted_requests: dict[str, ScrapeRequestIntercepted]
-    elapsed: timedelta | None
+    # disabled for now due to cloudflare+request interception issues.
+    # intercepted_responses: dict[str, InterceptedResponseMeta]
+    # intercepted_requests: dict[str, InterceptedRequestMeta]
+    redirect_chain: list[str]
+    elapsed: timedelta = timedelta(0)
+    current_depth: int = 0
+
     def __init__(self, 
         url: str = None, 
         tab: nodriver.Tab = None, 
@@ -88,134 +97,139 @@ class ScrapeResponse:
         timed_out_navigating: bool = False,
         timed_out_loading: bool = False,
         html: str = None, 
-        bytes_: bytes = None, 
+        # bytes_: bytes = None, 
         mime: str = None, 
         headers: dict = None, 
-        intercepted_responses: dict[str, ScrapeResponseIntercepted] = {},
-        intercepted_requests: dict[str, ScrapeRequestIntercepted] = {},
-        elapsed: timedelta | None = None
+        # intercepted_responses: dict[str, InterceptedResponseMeta] = None,
+        # intercepted_requests: dict[str, InterceptedRequestMeta] = None,
+        redirect_chain: list[str] = None,
+        elapsed: timedelta | None = None,
+        current_depth: int = 0
     ):
+        """
+        :param url: final (or initial) url of the page.
+        :param tab: underlying nodriver.Tab used for the scrape.
+        :param timed_out: overall timeout flag (true if any phase timed out initially).
+        :param timed_out_navigating: navigation phase timed out.
+        :param timed_out_loading: load event phase timed out.
+        :param html: captured html ("" when load timeout + outerHTML fallback failed).
+        :param mime: mime of main response.
+        :param headers: main response headers (lowercased keys).
+        :param redirect_chain: ordered list of redirect target locations (raw values from Location headers).
+        :param elapsed: timedelta duration of the scrape.
+        :param current_depth: mostly used by `crawl()` to pass on to handlers:
+        depth of this scrape in the overall crawl (0 = seed).
+        """
+        # :param `bytes_`: raw body bytes (only for non-text types when streamed).
+        # :param intercepted_responses: map of url->InterceptedResponseMeta.
+        # :param intercepted_requests: map of url->InterceptedRequestMeta.
+
+        # if intercepted_responses is None:
+        #     intercepted_responses = {}
+        # if intercepted_requests is None:
+        #     intercepted_requests = {}
+        if redirect_chain is None:
+            redirect_chain = []
+
         self.url = url
         self.tab = tab
         self.timed_out = timed_out
         self.timed_out_navigating = timed_out_navigating
         self.timed_out_loading = timed_out_loading
         self.html = html
-        self.bytes_ = bytes_
+        # self.bytes_ = bytes_
         self.mime = mime
         self.headers = headers
-        self.intercepted_responses = intercepted_responses
-        self.intercepted_requests = intercepted_requests
+        # self.intercepted_responses = intercepted_responses
+        # self.intercepted_requests = intercepted_requests
+        self.redirect_chain = redirect_chain
         self.elapsed = elapsed
+        self.current_depth = current_depth
 
 
-class ScrapeResponseHandler:
+class ScrapeResultHandler:
     """pluggable hooks invoked during scrape handling inside crawl.
 
     override or pass callables into __init__ to customize behavior (html parsing, bytes processing,
-    timeout handling, link extraction). each method may be sync or async except for `handle`.
+    timeout handling, link extraction). all overridden methods must be async (unless `handle` is
+    overridden to call sync methods).
+
+    `handle()` must be async.
+
+    `result` is mutable, so beware.
     """
 
-    async def timed_out(self, scrape_response: ScrapeResponse) -> list[str] | None:
+    async def timed_out(self, result: ScrapeResult) -> list[str] | None:
         """called when the scrape timed out early (navigation phase).
 
         return list of links (rare) or None to skip expansion.
 
-        :param scrape_response: partial response (likely missing html/bytes).
+        :param result: partial result (likely missing html/bytes). (mutable)
         :return: optional new links.
         """
         pass
 
 
-    async def html(self, scrape_response: ScrapeResponse):
+    async def html(self, result: ScrapeResult):
         """process populated html (and metadata) when navigation succeeded.
 
-        mutate scrape_response as needed (parse, annotate, store state).
+        mutate `result` as needed (parse, annotate, store state).
 
-        :param scrape_response: populated response object.
+        :param result: result object. (mutable)
         """
         pass
 
 
-    async def bytes_(self, scrape_response: ScrapeResponse):
-        """process raw bytes when non-text main response was streamed.
-
-        skip if bytes_ is None. can persist file, hash, etc.
+    # disabled due to cloudflare+request interception issues.
+    # async def bytes_(self, result: ScrapeResult):
+    #     """process raw bytes when non-text main response was streamed.
 
-        :param scrape_response: response object (bytes_ may be None).
-        """
-        pass
+    #     :param result: result object. (mutable)
+    #     """
+    #     pass
 
 
-    async def links(self, scrape_response: ScrapeResponse) -> list[str]:
+    async def links(self, result: ScrapeResult) -> list[str] | None:
         """return list of links for crawler expansion.
 
         default: existing extracted links.
 
-        :param scrape_response: response object.
+        :param result: result object. (mutable)
         :return: list of links to continue crawl with.
         """
-        return scrape_response.links
-    
+        return result.links
+
+
+    async def handle(self, result: ScrapeResult) -> list[str] | None:
+        """main entry point for handling a scrape result.
 
-    async def handle(self, scrape_response: ScrapeResponse) -> list[str] | None:
+        correctly executes each step in order:
+
+        `timed_out()`
+
+        or: `html()` -> `bytes_()` -> `links()`
+        
+        **— currently,** `bytes_()` is disabled due to
+        cloudflare+request interception issues.
+
+        :param result: scrape result object (mutable).
+        :return: list of links to continue crawl with.
+        :rtype: list[str] | None
+        """
         # if the response timed out, break.
         # (or continue I guess)
-        if scrape_response.timed_out:
-            if asyncio.iscoroutinefunction(self.timed_out):
-                return await self.timed_out(scrape_response)
-            else:
-                return self.timed_out(scrape_response)
+        if result.timed_out:
+                return await self.timed_out(result)
 
         # process the response
-        if asyncio.iscoroutinefunction(self.html):
-            await self.html(scrape_response)
-        else:
-            self.html(scrape_response)
-        if scrape_response.bytes_:
-            if asyncio.iscoroutinefunction(self.bytes_):
-                await self.bytes_(scrape_response)
-            else:
-                self.bytes_(scrape_response)
+        await self.html(result)
+        # if response.bytes_:
+        #     await self.bytes_(response)
 
         # return the links for the crawler to follow
-        if asyncio.iscoroutinefunction(self.links):
-            return await self.links(scrape_response)
-        else:
-            return self.links(scrape_response)
+        return await self.links(result)
     
 
-    def __init__(self,
-        timed_out: Callable[[ScrapeResponse], Coroutine[None, None, list[str] | None] | list[str] | None] = None,
-        html: Callable[[ScrapeResponse], Coroutine[None, None, None] | None] = None,
-        bytes_: Callable[[ScrapeResponse], Coroutine[None, None, None] | None] = None,
-        links: Callable[[ScrapeResponse], Coroutine[None, None, list[str]] | list[str]] = None,
-        handle: Callable[[ScrapeResponse], Coroutine[None, None, list[str] | None] | list[str] | None] = None
-    ):
-        """optional injection of hook functions.
-
-        any provided callable replaces the corresponding method 
-        (supports sync or async except for `handle` which must be async).
-
-        :param timed_out: handler for timeout case.
-        :param html: handler after html captured.
-        :param bytes_: handler after bytes captured.
-        :type bytes_: bytes_
-        :param links: link selection handler.
-        :param handle: full override (advanced; skips built-in sequence). 
-        **must** return a list of links for crawl to continue. (empty is the end of the tree)
-        """
-        if timed_out:
-            self.timed_out = timed_out
-        if html:
-            self.html = html
-        if bytes_:
-            self.bytes_ = bytes_
-        if links:
-            self.links = links
-        if handle:
-            self.handle = handle
-
 class FailedLink:
     """record of a link that failed or timed out during crawl.
 
@@ -235,7 +249,8 @@ class FailedLink:
 class CrawlResult:
     """final summary of a crawl run.
 
-    aggregates discovered + successful + failed + timed-out links, timing, and optionally captured ScrapeResponse objects.
+    aggregates discovered + successful + failed + timed-out
+    links, timing, and optionally captured ScrapeResult objects.
 
     :param links: all discovered links (including successful + failures).
     :param successful_links: subset successfully scraped.
@@ -244,7 +259,7 @@ class CrawlResult:
     :param time_start: crawl start timestamp (UTC).
     :param time_end: crawl end timestamp (UTC).
     :param time_elapsed: total duration.
-    :param responses: optional list of every ScrapeResponse captured.
+    :param responses: optional list of every ScrapeResult captured.
     """
     links: list[str]
     successful_links: list[str]
@@ -253,7 +268,7 @@ class CrawlResult:
     time_start: datetime | None
     time_end: datetime | None
     time_elapsed: timedelta | None
-    responses: list[ScrapeResponse] | None
+    responses: list[ScrapeResult] | None
 
     def __init__(self,
         links: list[str] = [],
@@ -263,7 +278,7 @@ class CrawlResult:
         time_start: datetime | None = None,
         time_end: datetime | None = None,
         time_elapsed: timedelta | None = None,
-        responses: list[ScrapeResponse] | None = None
+        responses: list[ScrapeResult] | None = None
     ):
         self.links = links
         self.successful_links = successful_links
@@ -275,23 +290,16 @@ class CrawlResult:
         self.responses = responses
 
 class CrawlResultHandler:
-    """custom hook used by `NodriverPlusManager` when running a crawl
+    """custom hook used by `Manager` when running a crawl
 
     supply a callable to __init__ or subclass and override `handle`.
     """
 
     async def handle(self, result: CrawlResult):
-        """process finished CrawlResult (persist / summarize / metrics).
+        """final hook for when a crawl completes.
+
+        `result` is mutable and is returned by `crawl()`
 
         :param result: crawl summary.
         """
         pass
-
-    def __init__(self, handle: Callable[[CrawlResult], Coroutine[any, any, None] | None] | None = None):
-        """allows for custom handling of crawl results during a queued crawl.
-        (NodriverPlusManager)
-
-        :param handle: optional custom handler for crawl results.
-        """
-        if handle:
-            self.handle = handle
diff --git a/src/nodriverplus/core/tab.py b/src/nodriverplus/core/tab.py
new file mode 100644
index 0000000..c270895
--- /dev/null
+++ b/src/nodriverplus/core/tab.py
@@ -0,0 +1,657 @@
+"""
+**TODO**:
+- move request paused handlers to `TargetInterceptorManager` probably
+"""
+
+import logging
+import asyncio
+import json
+import nodriver
+import time
+import random
+import re
+import base64
+import os
+from pathlib import Path
+from datetime import timedelta, datetime, UTC
+from nodriver import cdp
+from urllib.parse import urlparse
+from ..utils import fix_url, extract_links
+from ..js.load import load_text as load_js
+from .scrape_result import (
+    ScrapeResult, 
+    ScrapeResultHandler, 
+    CrawlResult,
+    CrawlResultHandler, 
+    FailedLink
+)
+# from .pause_handlers import ScrapeRequestPausedHandler
+from .user_agent import UserAgent
+from .browser import get, get_with_timeout
+
+logger = logging.getLogger("nodriverplus.tab")
+
+
+# lazy cv2 + numpy import to keep import cost tiny when unused
+try:  # slim optional dep
+    import cv2  # type: ignore
+    import numpy as np  # type: ignore
+except Exception:  # noqa: broad ok here (missing dep)
+    cv2 = None  # type: ignore
+    np = None  # type: ignore
+
+    
+async def wait_for_page_load(tab: nodriver.Tab, extra_wait_ms: int = 0):
+    """wait for load event (or immediate if already complete) then optional delay.
+
+    :param tab: target tab.
+    :param extra_wait_ms: additional ms sleep via setTimeout after load.
+    """
+    
+    # if the document is already fully loaded, return immediately (about:blank)
+    if await tab.evaluate("document.readyState") == "complete":
+        return
+
+    loop = asyncio.get_running_loop()
+    ev = asyncio.Event()
+
+    def _handler(_event):
+        loop.call_soon_threadsafe(ev.set)
+
+    tab.add_handler([cdp.page.LoadEventFired], handler=_handler)
+    try:
+        await ev.wait()
+        logger.info("successfully finished loading %s", getattr(tab, "url", "<unknown>"))
+        if extra_wait_ms:
+            logger.info("waiting extra %d ms for %s", extra_wait_ms, getattr(tab, "url", "<unknown>"))
+            await asyncio.sleep(extra_wait_ms / 1000)
+    finally:
+        tab.remove_handler([cdp.page.LoadEventFired], handler=_handler)
+
+
+async def get_user_agent(tab: nodriver.Tab):
+    """evaluate js/get_user_agent.js in a tab to extract structured user agent data.
+
+    converts returned json into the UserAgent model.
+
+    :param tab: target tab for the operation.
+    :return: structured user agent data.
+    :rtype: UserAgent
+    """
+    js = load_js("get_user_agent.js")
+    ua_data: dict = json.loads(await tab.evaluate(js, await_promise=True))
+    user_agent = UserAgent.from_json(ua_data)
+    logger.info("successfully retrieved user agent from %s", tab.url)
+    logger.debug(
+        "user agent data retrieved from %s:\n%s", tab.url, json.dumps(ua_data, indent=2)
+    )
+    return user_agent
+
+
+async def crawl(
+    base: nodriver.Tab | nodriver.Browser,
+    url: str,
+    scrape_result_handler: ScrapeResultHandler = None,
+    depth: int | None = 1,
+    crawl_result_handler: CrawlResultHandler = None,
+    *,
+    new_window = False,
+    # scrape_bytes = True,
+    navigation_timeout = 30,
+    wait_for_page_load = True,
+    page_load_timeout = 60,
+    extra_wait_ms = 0,
+    concurrency: int = 1,
+    max_pages: int | None = None,
+    collect_results: bool = False,
+    delay_range: tuple[float, float] | None = None,
+    # request_paused_handler: ScrapeRequestPausedHandler = None,
+    proxy_server: str = None,
+    proxy_bypass_list: list[str] = None,
+    origins_with_universal_network_access: list[str] = None,
+):
+    """customizable crawl API starting at `url` up to `depth`.
+
+    schedules scrape tasks with a worker pool and collects response metadata, errors,
+    links, and timing.
+
+    if `crawl_result_handler` is specified, `crawl_result_handler.handle()` will 
+    be called and awaited before returning the final `CrawlResult`.
+
+    `crawl_result_handler` is nifty if you're crawling with a `Manager`
+    instance.
+
+    - **`proxy_server`** — (EXPERIMENTAL) (Optional) Proxy server, similar to the one passed to --proxy-server
+    - **`proxy_bypass_list`** — (EXPERIMENTAL) (Optional) Proxy bypass list, similar to the one passed to --proxy-bypass-list
+    - **`origins_with_universal_network_access`** — (EXPERIMENTAL) (Optional) An optional list of origins to grant unlimited cross-origin access to. Parts of the URL other than those constituting origin are ignored.
+
+    :param base: target tab or browser instance to run crawl on
+    :param url: root starting point.
+    :param scrape_result_handler: optional `ScrapeResultHandler` to be passed to `scrape()`
+    :param depth: max link depth (0 means single page).
+    :param crawl_result_handler: if specified, `crawl_result_handler.handle()` 
+    will be called and awaited before returning the final `CrawlResult`.
+    :param new_window: isolate crawl in new context+window when True.
+    :param navigation_timeout: seconds for initial navigation phase.
+    :param wait_for_page_load: await full load event.
+    :param page_load_timeout: seconds for load phase.
+    :param extra_wait_ms: post-load settle time.
+    :param concurrency: worker concurrency.
+    :param max_pages: hard cap on processed pages.
+    :param collect_results: store every ScrapeResult object.
+    :param delay_range: (min,max) jitter before first scrape per worker loop.
+    :param tab_close_timeout: seconds to wait closing a tab.
+    :param proxy_server: (EXPERIMENTAL) (Optional) Proxy server, similar to the one passed to --proxy-server
+    :param proxy_bypass_list: (EXPERIMENTAL) (Optional) Proxy bypass list, similar to the one passed to --proxy-bypass-list
+    :param origins_with_universal_network_access: (EXPERIMENTAL) (Optional) An optional list of origins to grant unlimited cross-origin access to. Parts of the URL other than those constituting origin are ignored.
+    :return: crawl summary
+    :rtype: CrawlResult
+    """
+    # :param scrape_bytes: capture bytes stream when possible.
+    # :param request_paused_handler: custom fetch interception handler.
+    # **must be a type**—not an instance—so that it can be initiated later with the correct values attached
+    # :type request_paused_handler: type[ScrapeRequestPausedHandler]
+
+    if scrape_result_handler is None:
+        if not collect_results:
+            logger.warning("no `ScrapeResultHandler` provided and collect_results is False, only errors and links will be captured")
+        else:
+            logger.info("no `ScrapeResultHandler` provided, using default handler functions")
+        scrape_result_handler = ScrapeResultHandler()
+
+    # normalize delay range if provided
+    if delay_range is not None:
+        a, b = delay_range
+        if a > b:
+            delay_range = (b, a)
+        if a < 0 or b < 0:
+            delay_range = None  # disallow negative
+    logger.info(
+        "crawl started for %s (depth=%s concurrency=%s max_pages=%s delay=%s)",
+        url, depth, concurrency, max_pages, delay_range,
+    )
+
+    root_url = fix_url(url)
+    if depth is not None:
+        depth = max(0, depth)
+    concurrency = max(1, concurrency)
+
+    time_start = datetime.now(UTC)
+
+    # queue: (url, current_depth) where root starts at 0
+    queue: asyncio.Queue[tuple[str, int]] = asyncio.Queue()
+    await queue.put((root_url, 0))
+
+    # visited = fully processed
+    # all_links_set = discovered (even if not yet processed)
+    visited: set[str] = set()
+    all_links: list[str] = [root_url]
+    all_links_set: set[str] = {root_url}
+    successful_links: list[str] = []
+    processed_final_urls: set[str] = set()
+    failed_links: list[FailedLink] = []
+    timed_out_links: list[FailedLink] = []
+    # optional heavy list of every response captured
+    responses: list[ScrapeResult] = [] if collect_results else None  # type: ignore
+
+    pages_processed = 0
+    lock = asyncio.Lock()  # protects shared collections when needed
+    # map worker idx -> current url for runtime debugging
+    current_processing: dict[int, str] = {}
+
+    async def should_enqueue(link: str) -> bool:
+        # canonicalize and de-duplicate discovered links; depth checks are handled
+        # by the caller so this function only verifies URL shape and duplication.
+        try:
+            link_canon = fix_url(link)
+        except Exception:
+            return False
+        if link_canon in visited or link_canon in all_links_set:
+            return False
+        parsed = urlparse(link_canon)
+        # throw interesting links away
+        if parsed.scheme not in ("http", "https"):
+            return False
+        async with lock:  # race-safe insertion into discovery sets
+            if link_canon not in all_links_set:
+                all_links_set.add(link_canon)
+                all_links.append(link_canon)
+        return True
+
+    if new_window:
+        # create a dedicated browser context + window; tabs we spawn will stay in this context
+        instance = await get(base, new_window=True, new_context=True)
+    else:
+        instance = base
+
+    # collect tab close tasks so we can await them before returning
+    closing_tasks: list[asyncio.Task] = []
+
+    async def worker(idx: int):
+        nonlocal pages_processed
+        while True:
+            try:
+                # wait for next target
+                current_url, current_depth = await queue.get()
+            except asyncio.CancelledError:
+                break
+            # register current work for debugging/monitoring
+            try:
+                current_processing[idx] = current_url
+            except Exception:
+                current_processing[idx] = str(current_url)
+            if current_url in visited:
+                # clear current marker before finishing item
+                current_processing.pop(idx, None)
+                queue.task_done()
+                continue
+            visited.add(current_url)
+            if max_pages is not None and pages_processed >= max_pages:
+                current_processing.pop(idx, None)
+                queue.task_done()
+                continue
+            error_obj: Exception | None = None
+            scrape_result: ScrapeResult | None = None
+            try:
+                # first scrape (root) is at depth 0
+                is_first_depth = current_depth == 0
+                # simple delay / jitter if specified; skip if first scrape
+                if delay_range is not None and is_first_depth:
+                    wait_time = random.uniform(*delay_range)
+                    logger.info("waiting %.2f seconds before scraping %s", wait_time, current_url)
+                    await asyncio.sleep(wait_time)
+                scrape_result = await scrape(
+                    base=instance,
+                    url=current_url,
+                    # scrape_bytes=scrape_bytes,
+                    scrape_result_handler=scrape_result_handler,
+                    navigation_timeout=navigation_timeout,
+                    wait_for_page_load=wait_for_page_load,
+                    page_load_timeout=page_load_timeout,
+                    extra_wait_ms=extra_wait_ms,
+                    new_tab=True,
+                    # request_paused_handler=request_paused_handler,
+                    proxy_server=proxy_server,
+                    proxy_bypass_list=proxy_bypass_list,
+                    origins_with_universal_network_access=origins_with_universal_network_access,
+                    current_depth=current_depth,
+                )
+                pages_processed += 1
+                # determine final canonical URL (after redirects)
+                final_url = getattr(scrape_result, "url", None) or (scrape_result.tab.url if scrape_result.tab else current_url)
+                final_url = fix_url(final_url)
+
+                links: list[str] = []
+                if scrape_result.timed_out_navigating:
+                    timed_out_links.append(FailedLink(current_url, scrape_result.timed_out_navigating, error_obj))
+                else:
+                    if final_url in processed_final_urls:
+                        logger.debug("skip duplicate final url %s (source %s)", final_url, current_url)
+                    else:
+                        # handler already executed inside scrape(); collect links + finalize bookkeeping
+                        processed_final_urls.add(final_url)
+                        async with lock:
+                            if final_url not in all_links_set:
+                                all_links_set.add(final_url)
+                                all_links.append(final_url)
+                        successful_links.append(final_url)
+                        # adopt links extracted / supplied by handler
+                        links = scrape_result.links or []
+
+                if collect_results and scrape_result:
+                    async with lock:
+                        responses.append(scrape_result)
+
+                # enqueue new links only if we processed this final url just now
+                if final_url in processed_final_urls and not scrape_result.timed_out_navigating:
+                    # if depth is None there is no limit
+                    if (depth is None or current_depth < depth) and links:
+                        for link in links:
+                            if max_pages is not None and pages_processed >= max_pages:
+                                break
+                            if await should_enqueue(link):
+                                await queue.put((fix_url(link), current_depth + 1))
+
+                # close tab (timeout to avoid hang) unless it's the dedicated context tab
+                if scrape_result.tab and not (new_window and scrape_result.tab is instance):
+                    async def _close_tab(t_: nodriver.Tab):
+                        try:
+                            await asyncio.wait_for(t_.close(), timeout=5)
+                        except Exception:
+                            logger.warning("tab close failed or timed out for %s", getattr(t_, 'url', '<unknown>'))
+                    closing_tasks.append(asyncio.create_task(_close_tab(scrape_result.tab)))
+            except Exception as e:
+                failed_links.append(FailedLink(current_url, False, e))
+                logger.exception("unexpected error during crawl for %s", current_url)
+            finally:
+                # clear current marker and mark as done so join can finish
+                current_processing.pop(idx, None)
+                queue.task_done()
+
+    workers = [asyncio.create_task(worker(i)) for i in range(concurrency)]
+
+    # wait until everything is finished
+    await queue.join()
+    for w in workers:
+        w.cancel()
+    # we're trying to cancel everything
+    # so ignore CancelledError
+    for w in workers:
+        try:
+            await w
+        except asyncio.CancelledError:
+            pass
+
+    if new_window and isinstance(instance, nodriver.Tab):
+        # close the dedicated context tab with a timeout to avoid hangs
+        try:
+            t = asyncio.create_task(instance.close())
+            await asyncio.wait_for(t, timeout=5)
+        except asyncio.TimeoutError:
+            logger.warning("timeout closing dedicated context tab (continuing)")
+            t.cancel()
+        except Exception:
+            logger.debug("failed closing dedicated context tab (already closed?)")
+
+    # wait for any outstanding close tasks (bounded wait) to avoid hanging loop
+    if closing_tasks:
+        done, pending = await asyncio.wait(closing_tasks, timeout=6)
+        for p in pending:
+            p.cancel()
+        if pending:
+            logger.warning("cancelled %d lingering tab close task(s)", len(pending))
+
+    time_end = datetime.now(UTC)
+    time_elapsed = time_end - time_start
+    result = CrawlResult(
+        links=all_links,
+        successful_links=successful_links,
+        failed_links=failed_links,
+        timed_out_links=timed_out_links,
+        time_start=time_start,
+        time_end=time_end,
+        time_elapsed=time_elapsed,
+        responses=responses,
+    )
+
+    if crawl_result_handler:
+        await crawl_result_handler.handle(result)
+        result.time_end = datetime.now(UTC)
+        result.time_elapsed = result.time_end - time_start
+
+    logger.info(
+        "successfully finished crawl for %s (pages=%d success=%d failed=%d timed_out=%d elapsed=%.2fs)",
+        url,
+        len(all_links),
+        len(successful_links),
+        len(failed_links),
+        len(timed_out_links),
+        (time_end - time_start).total_seconds(),
+    )
+    return result
+
+
+async def scrape( 
+    base: nodriver.Tab | nodriver.Browser,
+    url: str,
+    # scrape_bytes = True,
+    scrape_result_handler: ScrapeResultHandler | None = None,
+    *,
+    navigation_timeout = 30,
+    wait_for_page_load = True,
+    page_load_timeout = 60,
+    extra_wait_ms = 0,
+    new_tab = False,
+    new_window = False,
+    # request_paused_handler = ScrapeRequestPausedHandler,
+    proxy_server: str = None,
+    proxy_bypass_list: list[str] = None,
+    origins_with_universal_network_access: list[str] = None,
+    current_depth: int = 0,
+):
+    """single page scrape (html + optional bytes + link extraction).
+
+    handles navigation, timeouts, fetch interception, html capture, link parsing and
+    cleanup (tab closure, pending task draining).
+
+    if `scrape_result_handler` is provided, `scrape_result_handler.handle()` will
+    be called exactly once and awaited before returning the final `ScrapeResult`.
+
+    - **`proxy_server`** — (EXPERIMENTAL) (Optional) Proxy server, similar to the one passed to --proxy-server
+    - **`proxy_bypass_list`** — (EXPERIMENTAL) (Optional) Proxy bypass list, similar to the one passed to --proxy-bypass-list
+    - **`origins_with_universal_network_access`** — (EXPERIMENTAL) (Optional) An optional list of origins to grant unlimited cross-origin access to. Parts of the URL other than those constituting origin are ignored.
+
+    :param base: reuse provided tab/browser root or create fresh.
+    :param url: target url.
+    :param scrape_bytes: capture non-text body bytes.
+    :param scrape_result_handler: if specified, `scrape_result_handler.handle()` will be called
+    and awaited before returning the final `ScrapeResult`.
+    :param navigation_timeout: seconds for initial navigation.
+    :param wait_for_page_load: await full load event.
+    :param page_load_timeout: seconds for load phase.
+    :param extra_wait_ms: post-load wait for dynamic content.
+    :param new_tab: request new tab.
+    :param new_window: request isolated window/context.
+    :param proxy_server: (EXPERIMENTAL) (Optional) Proxy server, similar to the one passed to --proxy-server
+    :param proxy_bypass_list: (EXPERIMENTAL) (Optional) Proxy bypass list, similar to the one passed to --proxy-bypass-list
+    :param origins_with_universal_network_access: (EXPERIMENTAL) (Optional) An optional list of origins to grant unlimited cross-origin access to. Parts of the URL other than those constituting origin are ignored.
+    :param current_depth: mostly used by `crawl()` to pass current depth to handlers.
+    :return: html/links/bytes/metadata
+    :rtype: ScrapeResult
+    """
+    # :param request_paused_handler: custom fetch interception handler.
+    # **must be a type**—not an instance—so that it can be initiated later with the correct values attached
+    # :type request_paused_handler: type[ScrapeRequestPausedHandler]
+    
+    start = time.monotonic()
+    url = fix_url(url)
+
+    result = ScrapeResult(url, current_depth=current_depth)
+    parsed_url = urlparse(url)
+
+    # central acquisition
+    tab = await get(
+        base, 
+        new_window=new_window, 
+        new_tab=new_tab,
+        proxy_server=proxy_server,
+        proxy_bypass_list=proxy_bypass_list,
+        origins_with_universal_network_access=origins_with_universal_network_access,
+    )
+    result.tab = tab
+    logger.info("scraping %s", url)
+
+    await tab.send(
+        cdp.network.set_extra_http_headers(
+            headers=cdp.network.Headers({
+                "Referer": f"{parsed_url.scheme}://{parsed_url.netloc}",
+            })
+        )
+    )
+
+    # TODO: figure out why request interception 
+    # causes this error with sandboxed iframes:
+    # my current theory is that intercepting the main page's URL
+    # changes the pages origin or something weird like that.
+    #
+    # Blocked script execution in 'about:blank' because the document's
+    # frame is sandboxed and the 'allow-scripts' permission is not set.
+
+    # # use the stock handler unless a custom one is provided
+    # request_paused_handler = (request_paused_handler or ScrapeRequestPausedHandler)(
+    #     tab, result, url, scrape_bytes
+    # )
+    # await request_paused_handler.start()
+
+    try:
+        nav_response = await get_with_timeout(
+            tab,
+            url,
+            navigation_timeout=navigation_timeout,
+            wait_for_page_load_=wait_for_page_load,
+            page_load_timeout=page_load_timeout,
+            extra_wait_ms=extra_wait_ms,
+        )
+        result.timed_out = nav_response.timed_out
+        result.timed_out_navigating = nav_response.timed_out_navigating
+        result.timed_out_loading = nav_response.timed_out_loading
+
+        if not nav_response.timed_out_navigating:
+            # if it's taking forever to load, get_content() will also take forever to load
+            result.html = await result.tab.evaluate("document.documentElement.outerHTML")
+            if isinstance(result.html, Exception):
+                raise result.html
+            # use the final URL as the base for extracting links
+            result.links = extract_links(result.html, result.url)
+        
+        # run handler + teardown before possibly closing the tab
+        if scrape_result_handler:
+            # run handler only if links not already populated to avoid double execution under crawl()
+            try:
+                result.links = await scrape_result_handler.handle(result)
+            except Exception:
+                logger.exception("error running scrape_result_handler for %s", url)
+        result.elapsed = timedelta(seconds=time.monotonic() - start)
+        elapsed_seconds = result.elapsed.total_seconds()
+        logger.info("successfully finished scrape for %s (elapsed=%.2fs)", url, elapsed_seconds)
+
+        # await request_paused_handler.stop()
+    except Exception:
+        result.elapsed = timedelta(seconds=time.monotonic() - start)
+        elapsed_seconds = result.elapsed.total_seconds()
+        logger.exception(
+            "unexpected error during scrape for %s (elapsed=%.2fs):", url, elapsed_seconds
+        )
+
+    return result
+
+
+async def click_template_image(
+    tab: nodriver.Tab,
+    template: str | os.PathLike,
+    *,
+    x_shift: int = 0,
+    y_shift: int = 0,
+    flash_point: bool = False,
+    save_annotated_screenshot: str | os.PathLike = None,
+    match_threshold: float = 0.5,
+):
+    """find a template in the current page screenshot and somewhere around it
+    (`x_shift` and `y_shift`).
+
+    if the template filename follows this pattern:
+    """\
+    r"- `{name}__x{-?\d+}__y{-?\d+}`,"\
+    """
+
+    then the embedded shifts will be applied unless 
+    `x_shift` or `y_shift` are explicitly set to non-zero values.
+
+    :param tab: target nodriver Tab.
+    :param template: path or filename of the template image.
+    :param x_shift: horizontal shift in css pixels to apply to the computed click point.
+    :param y_shift: vertical shift in css pixels to apply to the computed click point.
+    :param save_annotated_screenshot: if set, save an annotated screenshot with the matched template.
+    :param flash_point: if True, flash the click location after clicking.
+    :param match_threshold: only issue a click if the template match exceeds this threshold (0.0-1.0).
+    """
+
+    # resolve template path
+    tpl_path = Path(template)
+
+    # parse embedded shift hints unless explicitly overridden by params
+    # pattern: {name}__  (both optional)
+    x_group = re.match(r".*__x(-?\d+).*", tpl_path.name)
+    y_group = re.match(r".*__y(-?\d+).*", tpl_path.name)
+    if x_group:
+        if x_shift == 0:
+            x_shift = int(x_group.group(1))
+    if y_group:
+        if y_shift == 0:
+            y_shift = int(y_group.group(1))
+
+    # capture screenshot + dpr
+    png_bytes = None
+    dpr = float(await tab.evaluate("window.devicePixelRatio||1"))
+    await tab.send(cdp.page.enable())
+    data = await tab.send(cdp.page.capture_screenshot(format_="png", from_surface=True))
+    png_bytes = base64.b64decode(data)
+
+    scr = cv2.imdecode(np.frombuffer(png_bytes, dtype=np.uint8), cv2.IMREAD_COLOR)
+    template_im = cv2.imread(str(tpl_path))
+    match = cv2.matchTemplate(scr, template_im, cv2.TM_CCOEFF_NORMED)
+    _min_v, _max_v, _min_l, max_l = cv2.minMaxLoc(match)
+    # matchTemplate returns a normalized score in _max_v; convert to percentage for logging
+    match_pct = float(_max_v) * 100.0
+    xs, ys = max_l
+    th, tw = template_im.shape[:2]
+    xe, ye = xs + tw, ys + th
+    cx_img = (xs + xe) // 2
+    cy_img = (ys + ye) // 2
+    # apply shifts (shift already in image pixel space; adjust for dpr afterwards)
+    cx_shifted = cx_img + int(x_shift * dpr)
+    cy_shifted = cy_img + int(y_shift * dpr)
+    h, w = scr.shape[:2]
+    cx_shifted = max(0, min(cx_shifted, w - 1))
+    cy_shifted = max(0, min(cy_shifted, h - 1))
+    # optionally annotate and save an annotated screenshot before clicking
+    if save_annotated_screenshot:
+        # draw a bright red rectangle around the matched template and a bright green dot at click
+        # scr is a BGR image
+        rect_thickness = max(2, int(round(3 * (dpr or 1.0))))
+        dot_radius = max(3, int(round(4 * (dpr or 1.0))))
+        # rectangle: top-left (xs,ys), bottom-right (xe,ye)
+        cv2.rectangle(scr, (int(xs), int(ys)), (int(xe), int(ye)), (0, 0, 255), thickness=rect_thickness)
+        # dot: filled circle at clicked image coords
+        cv2.circle(scr, (int(cx_shifted), int(cy_shifted)), dot_radius, (0, 255, 0), thickness=-1)
+        out_path = Path(save_annotated_screenshot)
+        # ensure parent dir exists
+        out_path.parent.mkdir(parents=True, exist_ok=True)
+        # write annotated image
+        cv2.imwrite(str(out_path), scr)
+
+    if match_pct / 100.0 < match_threshold:
+        logger.debug(
+            "skipping template %s: match=%.1f%% img=(%d,%d) css=(%.1f,%.1f) dpr=%.2f shift=(%d,%d)",
+            tpl_path.name,
+            match_pct,
+            cx_shifted,
+            cy_shifted,
+            cx_shifted / (dpr or 1.0),
+            cy_shifted / (dpr or 1.0),
+            dpr,
+            x_shift,
+            y_shift,
+        )
+        return False
+
+    # convert to css coords
+    css_x = cx_shifted / (dpr or 1.0)
+    css_y = cy_shifted / (dpr or 1.0)
+    await tab.mouse_click(css_x, css_y)
+    logger.debug(
+        "successfully clicked best match coords for %s: match=%.1f%% img=(%d,%d) css=(%.1f,%.1f) dpr=%.2f shift=(%d,%d)",
+        tpl_path.name,
+        match_pct,
+        cx_shifted,
+        cy_shifted,
+        css_x,
+        css_y,
+        dpr,
+        x_shift,
+        y_shift,
+    )
+    if flash_point:
+        await tab.flash_point(int(css_x), int(css_y))
+    return True
+
+
+__all__ = [
+    "wait_for_page_load",
+    "get",
+    "get_with_timeout",
+    "get_user_agent",
+    "crawl",
+    "scrape",
+    "click_template_image",
+]
\ No newline at end of file
diff --git a/src/nodriverplus/core/user_agent.py b/src/nodriverplus/core/user_agent.py
index 777f9ff..9ffe818 100644
--- a/src/nodriverplus/core/user_agent.py
+++ b/src/nodriverplus/core/user_agent.py
@@ -1,101 +1,13 @@
-"""user agent models + helpers.
+"""user agent container using cdp.emulation.UserAgentMetadata directly.
 
-wraps structured ua + metadata returned from a page script and provides
-serialization helpers (including js-friendly bool coercion) for patching
-cdp domains (Network / Emulation) and runtime navigator.* surfaces.
+we removed the local mirror to avoid drift; conversion now happens in JS so
+metadata can be fed into emulation.UserAgentMetadata.from_json with no tweaks.
 """
 
-class JSObject:
-    """tiny helper mixin for js-friendly serialization tweaks."""
-    def fix_bools_in_obj(self, obj):
-        """recursively coerce python bools into lowercase js bool literals.
+from nodriver import cdp
 
-        used when embedding a python-built dict into a js string so we don't
-        end up with capitalized True/False tokens in runtime patches.
 
-        :param obj: arbitrary python structure (dict/list/scalar).
-        :return: same structure with bools replaced by "true"/"false".
-        """
-        if isinstance(obj, bool):
-            return "true" if obj else "false"
-        elif isinstance(obj, dict):
-            return {k: self.fix_bools_in_obj(v) for k, v in obj.items()}
-        elif isinstance(obj, list):
-            return [self.fix_bools_in_obj(v) for v in obj]
-        else:
-            return obj
-
-class UserAgentMetadata(JSObject):
-    """structured client hints / metadata describing platform + versions.
-
-    mirrors chrome userAgentData surfaces (platform/version/brands/etc.).
-
-    values are passed directly into Network / Emulation overrides and also
-    injected into runtime patches so `navigator.userAgentData` matches. 
-    (reduces fingerprint)
-    """
-    platform: str
-    platform_version: str
-    architecture: str
-    model: str
-    mobile: bool
-    brands: list[dict[str, str]]
-    full_version_list: list[dict[str, str]]
-    full_version: str
-    bitness: str
-    wow64: bool
-    form_factors: list[str]
-
-    def __init__(self,
-        platform: str,
-        platform_version: str,
-        architecture: str,
-        model: str,
-        mobile: bool,
-        brands: list[dict[str, str]],
-        full_version_list: list[dict[str, str]],
-        full_version: str,
-        bitness: str,
-        wow64: bool,
-        form_factors: list[str],
-    ):
-        self.platform = platform
-        self.platform_version = platform_version
-        self.architecture = architecture
-        self.model = model
-        self.mobile = mobile
-        self.brands = brands
-        self.full_version_list = full_version_list
-        self.full_version = full_version
-        self.bitness = bitness
-        self.wow64 = wow64
-        self.form_factors = form_factors
-
-    def to_json(self, fix_bools = False) -> dict[str, str | bool | list[str] | list[dict[str, str]]]:
-        """serialize to dict matching cdp expectations.
-
-        :param fix_bools: when True convert python bools to js-friendly strings.
-        :return: json-serializable metadata dict.
-        :rtype: dict
-        """
-        obj = {
-            "platform": self.platform,
-            "platformVersion": self.platform_version,
-            "architecture": self.architecture,
-            "model": self.model,
-            "mobile": self.mobile,
-            "brands": self.brands,
-            "fullVersionList": self.full_version_list,
-            "fullVersion": self.full_version,
-            "bitness": self.bitness,
-            "wow64": self.wow64,
-            "formFactors": self.form_factors,
-        }
-        if fix_bools:
-            obj = self.fix_bools_in_obj(obj)
-        return obj
-
-class UserAgent(JSObject):
+class UserAgent:
     """primary user agent container (raw UA string + platform + metadata).
 
     also exposes acceptLanguage and an appVersion if specified. 
@@ -104,25 +16,49 @@ class UserAgent(JSObject):
     used for both protocol overrides and runtime
     patch injection inside pages/workers.
     """
-    user_agent: str
-    platform: str
-    language: str
-    metadata: UserAgentMetadata | None
-    app_version: str
 
     def __init__(self, 
         user_agent: str,
         platform: str,
-        language: str,
-        metadata: UserAgentMetadata = None,
+        accept_language: str,
+        metadata: cdp.emulation.UserAgentMetadata | None = None,
         app_version: str = None
     ):
         self.user_agent = user_agent
         self.platform = platform
-        self.language = language
+        self.accept_language = accept_language
         self.metadata = metadata
         self.app_version = app_version or user_agent.removeprefix("Mozilla/")
 
+    @classmethod
+    def from_json(cls, data: dict):
+        """construct a UserAgent from a dict produced by get_user_agent.js.
+
+        expects keys: user_agent, platform, language, metadata (camelCase for metadata
+        sub-keys matching emulation.UserAgentMetadata.from_json).
+        silently ignores extra keys.
+        """
+        meta_raw = data.get("metadata")
+        metadata = cdp.emulation.UserAgentMetadata.from_json(meta_raw) if meta_raw else None
+        user_agent = data.get("userAgent", "")
+        return cls(
+            user_agent=user_agent,
+            platform=data.get("platform", ""),
+            accept_language=data.get("acceptLanguage", ""),
+            metadata=metadata,
+            app_version=data.get("appVersion") or user_agent.removeprefix("Mozilla/"),
+        )
+
+    def _fix_bools(self, obj):
+        # coerce python bools into lowercase js literals
+        if isinstance(obj, bool):
+            return "true" if obj else "false"
+        if isinstance(obj, dict):
+            return {k: self._fix_bools(v) for k, v in obj.items()}
+        if isinstance(obj, list):
+            return [self._fix_bools(v) for v in obj]
+        return obj
+
     def to_json(self, include_app_version = False, fix_bools = False) -> dict[str, str | dict[str, str | bool | list[str] | list[dict[str, str]]]]:
         """serialize ua info for protocol overrides / patches.
 
@@ -134,12 +70,13 @@ class UserAgent(JSObject):
         obj = {
             "userAgent": self.user_agent,
             "platform": self.platform,
-            "acceptLanguage": self.language,
+            "acceptLanguage": self.accept_language,
         }
         if self.metadata:
-            obj["userAgentMetadata"] = self.metadata.to_json(fix_bools=fix_bools)
+            # cdp metadata already exposes the proper camelCase structure
+            obj["userAgentMetadata"] = self.metadata.to_json()
         if include_app_version:
             obj["appVersion"] = self.app_version
         if fix_bools:
-            obj = self.fix_bools_in_obj(obj)
+            obj = self._fix_bools(obj)
         return obj
\ No newline at end of file
diff --git a/src/nodriverplus/js/apply_stealth.js b/src/nodriverplus/js/apply_stealth.js
deleted file mode 100644
index dc544cf..0000000
--- a/src/nodriverplus/js/apply_stealth.js
+++ /dev/null
@@ -1,226 +0,0 @@
-(() => {
-    // --- 1) One-time toString masker ---
-    const _fnToString = Function.prototype.toString;
-    const _mask = new WeakMap();
-
-    Function.prototype.toString = new Proxy(_fnToString, {
-        apply(target, thisArg, args) {
-            if (_mask.has(thisArg)) return _mask.get(thisArg);
-            return Reflect.apply(target, thisArg, args);
-        }
-    });
-
-    function maskNative(fn, src) {
-        // src e.g. 'function item() { [native code] }' or 'function get plugins() { [native code] }'
-        _mask.set(fn, src);
-        try { Object.defineProperty(fn, "length", { value: fn.length, configurable: true }); } catch { }
-        return fn;
-    }
-
-    // --- 2) Define helpers ---
-    function defineData(obj, prop, value, { enumerable = false, writable = false } = {}) {
-        Object.defineProperty(obj, prop, { value, enumerable, writable, configurable: true });
-    }
-
-    function defineMethod(obj, prop, fn, { enumerable = false } = {}) {
-        defineData(obj, prop, maskNative(fn, `function ${prop}() { [native code] }`), { enumerable });
-    }
-
-    function defineIterator(obj, generatorFn, name = "values") {
-        defineData(obj, Symbol.iterator, maskNative(generatorFn, `function ${name}() { [native code] }`));
-    }
-
-    function defineNativeGetter(obj, prop, val, { enumerable = false } = {}) {
-        const getter = function () { return val; };
-        Object.defineProperty(obj, prop, {
-            get: maskNative(getter, `function get ${prop}() { [native code] }`),
-            configurable: true,
-            enumerable,
-        });
-    }
-
-    const navProto = Object.getPrototypeOf(navigator);
-
-    // hide webdriver flag
-    defineNativeGetter(navProto, "webdriver", undefined);
-
-    // spoof languages
-    defineNativeGetter(navProto, "language", "en-US");
-    defineNativeGetter(navProto, "languages", ["en-US", "en"]);
-
-    // minimal chrome object
-    if (!window?.chrome) {
-        window.chrome = {
-            app: {},
-            runtime: {
-                connect: function () { return { onMessage: { addListener: function () { } } }; },
-                sendMessage: function () { },
-                getManifest: function () { return undefined; },
-                id: undefined
-            },
-            webstore: undefined
-        };
-    }
-
-    // battery API for desktop
-    if (!navigator.getBattery) {
-        const batteryManager = {
-            charging: false,
-            chargingTime: Infinity,
-            dischargingTime: Infinity,
-            level: 1
-        };
-        defineMethod(navProto, "getBattery", () => Promise.resolve(batteryManager));
-    }
-
-    // permissions API - always deny
-    if (navigator.permissions && navigator.permissions.query) {
-        const origQuery = navigator.permissions.query;
-        defineMethod(navigator.permissions, "query", (params) => {
-            return Promise.resolve({ state: "denied", onchange: null });
-        });
-    }
-
-    // screen orientation
-    if (!screen.orientation) {
-        defineNativeGetter(screen, "orientation", {
-            type: "landscape-primary",
-            angle: 0,
-            onchange: null
-        });
-    }
-
-    // PDF plugin support
-    const pdfPlugins = [
-        {
-            // Chrome PDF Plugin -> x-google-chrome-pdf
-            name: "Chrome PDF Plugin",
-            filename: "internal-pdf-viewer",
-            description: "Portable Document Format",
-            mimes: [
-                {
-                    type: "application/x-google-chrome-pdf",
-                    suffixes: "pdf",
-                    description: "Portable Document Format",
-                },
-            ],
-        },
-        {
-            // Chrome PDF Viewer -> application/pdf
-            name: "Chrome PDF Viewer",
-            filename: "mhjfbmdgcfjbbpaeojofohoefgiehjai",
-            description: "",
-            mimes: [
-                {
-                    type: "application/pdf",
-                    suffixes: "pdf",
-                    description: "",
-                },
-            ],
-        },
-    ];
-
-    // --- Build native-ish Plugin/MimeType objects ---
-    function makeMimeType(mt, pluginRef) {
-        const o = {};
-        defineData(o, "type", mt.type);
-        defineData(o, "suffixes", mt.suffixes);
-        defineData(o, "description", mt.description || "");
-        defineData(o, "enabledPlugin", pluginRef);
-        return o;
-    }
-
-    function makePlugin(def) {
-        const p = {};
-
-        // fields = DATA props
-        defineData(p, "name", def.name);
-        defineData(p, "filename", def.filename);
-        defineData(p, "description", def.description || "");
-
-        const mimes = def.mimes.map(mt => makeMimeType(mt, p));
-
-        // index slots = DATA, enumerable
-        mimes.forEach((mt, i) => defineData(p, i, mt, { enumerable: true }));
-        // named by type = DATA, non-enumerable
-        mimes.forEach(mt => defineData(p, mt.type, mt));
-        defineData(p, "length", mimes.length);
-
-        return { plugin: p, mimes };
-    }
-
-    function makePluginArray(pluginObjs) {
-        const arr = {};
-        pluginObjs.forEach((plug, i) => defineData(arr, i, plug, { enumerable: true }));
-        pluginObjs.forEach(plug => defineData(arr, plug.name, plug));
-        defineData(arr, "length", pluginObjs.length);
-        return arr;
-    }
-
-    function makeMimeTypeArray(allMimes) {
-        const order = ["application/pdf", "application/x-google-chrome-pdf"];
-        const ordered = allMimes.slice().sort((a, b) => order.indexOf(a.type) - order.indexOf(b.type));
-        const arr = {};
-        ordered.forEach((mt, i) => defineData(arr, i, mt, { enumerable: true }));
-        ordered.forEach(mt => defineData(arr, mt.type, mt));
-        defineData(arr, "length", ordered.length);
-        return arr;
-    }
-
-    // Build
-    const built = pdfPlugins.map(makePlugin);
-    const pluginObjs = built.map(b => b.plugin);
-    const allMimeObjs = built.flatMap(b => b.mimes);
-
-    const pluginsArray = makePluginArray(pluginObjs);
-    const mimeTypesArray = makeMimeTypeArray(allMimeObjs);
-
-    const realPlugins = Object.getOwnPropertyDescriptor(Navigator.prototype, "plugins").get.call(navigator);
-    const realPluginProto = realPlugins.length ? Object.getPrototypeOf(realPlugins[0]) : Object.prototype;
-    const realMimeProto = realPlugins.length && realPlugins[0].length ? Object.getPrototypeOf(realPlugins[0][0]) : Object.prototype;
-    const realArrayProto = Object.getPrototypeOf(realPlugins);
-
-    Object.setPrototypeOf(pluginsArray, realArrayProto);
-    pluginObjs.forEach(p => Object.setPrototypeOf(p, realPluginProto));
-    allMimeObjs.forEach(m => Object.setPrototypeOf(m, realMimeProto));
-
-    // Attach to navigator — these MUST be getters (that's how Chrome/Edge do it)
-    Object.defineProperty(navProto, "plugins", {
-        get: maskNative(function getPlugins() { return pluginsArray; }, "function get plugins() { [native code] }"),
-        configurable: true
-    });
-    Object.defineProperty(navProto, "mimeTypes", {
-        get: maskNative(function getMimeTypes() { return mimeTypesArray; }, "function get mimeTypes() { [native code] }"),
-        configurable: true
-    });
-    Object.defineProperty(navigator, "pdfViewerEnabled", {
-        get: maskNative(function getPdfViewerEnabled() { return true; }, "function get pdfViewerEnabled() { [native code] }"),
-        configurable: true
-    });
-    defineData(navigator, "pdfViewerEnabled", true);
-
-    // canvas fingerprint protection
-    const getImageData = CanvasRenderingContext2D.prototype.getImageData;
-    defineMethod(CanvasRenderingContext2D.prototype, "getImageData", function (x, y, w, h) {
-        const imageData = getImageData.apply(this, arguments);
-
-        // skip 1x1 transparent checks
-        if (w <= 1 && h <= 1 && x === 0 && y === 0) {
-            return imageData;
-        }
-
-        // add minimal noise
-        const data = imageData.data;
-        for (let i = 0; i < data.length; i += 4) {
-            const noise = Math.random() < 0.1 ? (Math.random() < 0.5 ? -1 : 1) : 0;
-            data[i] = Math.max(0, Math.min(255, data[i] + noise));
-            data[i + 1] = Math.max(0, Math.min(255, data[i + 1] + noise));
-            data[i + 2] = Math.max(0, Math.min(255, data[i + 2] + noise));
-        }
-
-        return imageData;
-    });
-
-    // cap error stack trace
-    Error.stackTraceLimit = 10;
-})();
diff --git a/src/nodriverplus/js/apply_stealth_worker.js b/src/nodriverplus/js/apply_stealth_worker.js
deleted file mode 100644
index 71e35c5..0000000
--- a/src/nodriverplus/js/apply_stealth_worker.js
+++ /dev/null
@@ -1,50 +0,0 @@
-(() => {
-    // --- 1) One-time toString masker ---
-    const _fnToString = Function.prototype.toString;
-    const _mask = new WeakMap();
-
-    Function.prototype.toString = new Proxy(_fnToString, {
-        apply(target, thisArg, args) {
-            if (_mask.has(thisArg)) return _mask.get(thisArg);
-            return Reflect.apply(target, thisArg, args);
-        }
-    });
-
-    function maskNative(fn, src) {
-        // src e.g. 'function item() { [native code] }' or 'function get plugins() { [native code] }'
-        _mask.set(fn, src);
-        try { Object.defineProperty(fn, "length", { value: fn.length, configurable: true }); } catch { }
-        return fn;
-    }
-
-    // --- 2) Define helpers ---
-    function defineData(obj, prop, value, { enumerable = false, writable = false } = {}) {
-        Object.defineProperty(obj, prop, { value, enumerable, writable, configurable: true });
-    }
-
-    function defineMethod(obj, prop, fn, { enumerable = false } = {}) {
-        defineData(obj, prop, maskNative(fn, `function ${prop}() { [native code] }`), { enumerable });
-    }
-
-    function defineIterator(obj, generatorFn, name = "values") {
-        defineData(obj, Symbol.iterator, maskNative(generatorFn, `function ${name}() { [native code] }`));
-    }
-
-    function defineNativeGetter(obj, prop, val, { enumerable = false } = {}) {
-        const getter = function () { return val; };
-        Object.defineProperty(obj, prop, {
-            get: maskNative(getter, `function get ${prop}() { [native code] }`),
-            configurable: true,
-            enumerable,
-        });
-    }
-
-    const navProto = Object.getPrototypeOf(navigator);
-
-    // hide webdriver flag
-    defineNativeGetter(navProto, "webdriver", false);
-
-    // spoof languages
-    defineNativeGetter(navProto, "language", "en-US");
-    defineNativeGetter(navProto, "languages", ["en-US", "en"]);
-})();
\ No newline at end of file
diff --git a/src/nodriverplus/js/get_user_agent.js b/src/nodriverplus/js/get_user_agent.js
index 33cecb7..45331a4 100644
--- a/src/nodriverplus/js/get_user_agent.js
+++ b/src/nodriverplus/js/get_user_agent.js
@@ -8,23 +8,23 @@
             'model', 'platformVersion', 'uaFullVersion', 'wow64'
         ]);
         metadata = {
-            platform: highEntropy.platform || '',
-            platform_version: highEntropy.platformVersion || '',
+            platform: uaData.platform || '',
+            platformVersion: highEntropy.platformVersion || '',
             architecture: highEntropy.architecture || '',
             model: highEntropy.model || '',
-            mobile: highEntropy.mobile || false,
-            brands: highEntropy.brands || [],
-            full_version_list: highEntropy.fullVersionList || [],
-            full_version: highEntropy.uaFullVersion || '',
+            mobile: uaData.mobile || false,
+            brands: uaData.brands || [],
+            fullVersionList: highEntropy.fullVersionList || [],
+            fullVersion: highEntropy.uaFullVersion || '',
             bitness: highEntropy.bitness || '',
             wow64: highEntropy.wow64 || false,
-            form_factors: highEntropy.formFactors || [],
+            formFactors: highEntropy.formFactors || [],
         };
     }
     return JSON.stringify({ 
-        user_agent: ua, 
+        userAgent: ua, 
         platform: navigator.platform,
-        language: navigator.language,
+        acceptLanguage: navigator.language,
         metadata: metadata, 
     });
 })()
\ No newline at end of file
diff --git a/src/nodriverplus/js/load.py b/src/nodriverplus/js/load.py
index ea599fd..db7be95 100644
--- a/src/nodriverplus/js/load.py
+++ b/src/nodriverplus/js/load.py
@@ -1,7 +1,7 @@
 import logging
 from pathlib import Path
 
-logger = logging.getLogger(__name__)
+logger = logging.getLogger("nodriverplus.js.load")
 
 def load_text(filename: str, encoding: str = "utf-8") -> str:
     # resolve path relative to this file to avoid relying on cwd
diff --git a/src/nodriverplus/js/patch_user_agent.js b/src/nodriverplus/js/patch_user_agent.js
index 8872f6a..e19e44e 100644
--- a/src/nodriverplus/js/patch_user_agent.js
+++ b/src/nodriverplus/js/patch_user_agent.js
@@ -1,36 +1,53 @@
 (() => {
-    // --- 1) One-time toString masker ---
-    const _fnToString = Function.prototype.toString;
+    // assumes single application per global; no global guards or assignments
+    const _originalFnToString = Function.prototype.toString;
     const _mask = new WeakMap();
 
-    Function.prototype.toString = new Proxy(_fnToString, {
+    const _fnToStringProxy = new Proxy(_originalFnToString, {
         apply(target, thisArg, args) {
             if (_mask.has(thisArg)) return _mask.get(thisArg);
             return Reflect.apply(target, thisArg, args);
         }
     });
+    try { Function.prototype.toString = _fnToStringProxy; } catch (e) { /* if locked, continue */ }
+    // mask the proxy itself to prevent introspection
+    maskNative(_fnToStringProxy, 'function toString() { [native code] }');
+
+    // define shared native functions to avoid infinite recursion
+    const _nativeToString = function () { return 'function toString() { [native code] }'; };
+    maskNative(_nativeToString, 'function toString() { [native code] }');
+    const _nativeToLocaleString = function () { return 'function toLocaleString() { [native code] }'; };
+    maskNative(_nativeToLocaleString, 'function toLocaleString() { [native code] }');
 
     function maskNative(fn, src) {
         // src e.g. 'function item() { [native code] }' or 'function get plugins() { [native code] }'
+        if (_mask.has(fn)) return fn; // avoid re-masking
         _mask.set(fn, src);
         try { Object.defineProperty(fn, "length", { value: fn.length, configurable: true }); } catch { }
+        // recursively mask toString properties
+        try {
+            Object.defineProperty(fn, 'toString', {
+                value: _nativeToString,
+                configurable: true,
+                writable: true,
+                enumerable: false
+            });
+            // also mask toLocaleString
+            Object.defineProperty(fn, 'toLocaleString', {
+                value: _nativeToLocaleString,
+                configurable: true,
+                writable: true,
+                enumerable: false
+            });
+        } catch {}
         return fn;
     }
 
-    // --- 2) Define helpers ---
-    function defineData(obj, prop, value, { enumerable = false, writable = false } = {}) {
-        Object.defineProperty(obj, prop, { value, enumerable, writable, configurable: true });
-    }
-
-    function defineMethod(obj, prop, fn, { enumerable = false } = {}) {
-        defineData(obj, prop, maskNative(fn, `function ${prop}() { [native code] }`), { enumerable });
-    }
-
-    function defineIterator(obj, generatorFn, name = "values") {
-        defineData(obj, Symbol.iterator, maskNative(generatorFn, `function ${name}() { [native code] }`));
-    }
-
     function defineNativeGetter(obj, prop, val, { enumerable = false } = {}) {
+        // avoid redefining the getter if already defined to our masked getter
+        const existing = Object.getOwnPropertyDescriptor(obj, prop);
+        if (existing && typeof existing.get === 'function' && _mask.has(existing.get)) return;
+
         const getter = function () { return val; };
         Object.defineProperty(obj, prop, {
             get: maskNative(getter, `function get ${prop}() { [native code] }`),
@@ -39,7 +56,7 @@
         });
     }
 
-    /* example placeholder replacement:
+    /* example //uaPatch// placeholder replacement:
     const uaPatch = {
         'userAgent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
         'appVersion': '5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
@@ -51,7 +68,7 @@
             'platformVersion': '19.0.0', 
             'architecture': 'x86', 
             'model': '', 
-            'mobile': False, 
+            'mobile': false, 
             'brands': [
                 { 
                     'brand': 'Not;A=Brand', 'version': '99' 
@@ -72,7 +89,7 @@
             ], 
             'fullVersion': '139.0.7258.139', 
             'bitness': '64', 
-            'wow64': False, 
+            'wow64': false, 
             'formFactors': [
                 'Desktop'
             ] 
@@ -87,5 +104,7 @@
     defineNativeGetter(navProto, "userAgent", uaPatch.userAgent);
     defineNativeGetter(navProto, "appVersion", uaPatch.appVersion);
     defineNativeGetter(navProto, "platform", uaPatch.platform);
+
     defineNativeGetter(navProto, "language", uaPatch.acceptLanguage);
+    defineNativeGetter(navProto, "languages", [...uaPatch.acceptLanguage.split(",")]);
 })();
\ No newline at end of file
diff --git a/src/nodriverplus/utils/__init__.py b/src/nodriverplus/utils/__init__.py
index dfb59f1..805f65e 100644
--- a/src/nodriverplus/utils/__init__.py
+++ b/src/nodriverplus/utils/__init__.py
@@ -12,6 +12,7 @@ keep surface tiny + predictable.
 from .pdf import pdf_to_html
 from .markdown import html_to_markdown
 from .html import extract_links
+from .urls import fix_url
 
 class ByteType:
     name: str | None
@@ -27,7 +28,7 @@ def get_type(bytes_: bytes):
 
     recognizes pdf magic header and utf-8 decodable text; else unsupported.
 
-    :param bytes_: the byte content to analyze
+    :param `bytes_`: the byte content to analyze
     """
     if bytes_.startswith(b"%PDF-"):
         return ByteType("pdf", True)
@@ -66,4 +67,5 @@ __all__ = [
     "ByteType",
     "get_type",
     "to_markdown",
+    "fix_url",
 ]
\ No newline at end of file
diff --git a/src/nodriverplus/utils/html.py b/src/nodriverplus/utils/html.py
index a6450fa..87b44d3 100644
--- a/src/nodriverplus/utils/html.py
+++ b/src/nodriverplus/utils/html.py
@@ -21,7 +21,7 @@ from urllib.parse import urljoin
 
 from bs4 import BeautifulSoup, Tag, NavigableString
 
-logger = logging.getLogger(__name__)
+logger = logging.getLogger("nodriverplus.utils.html")
 
 # constants
 INLINE_TAGS = {
diff --git a/src/nodriverplus/utils/markdown.py b/src/nodriverplus/utils/markdown.py
index c3241a0..48e3975 100644
--- a/src/nodriverplus/utils/markdown.py
+++ b/src/nodriverplus/utils/markdown.py
@@ -20,7 +20,7 @@ from html_to_markdown import convert_to_markdown
 
 from .html import parse_style_attribute, normalize_html
 
-logger = logging.getLogger(__name__)
+logger = logging.getLogger("nodriverplus.utils.markdown")
 
 
 def _style_to_emphasis_converter(*, 
diff --git a/src/nodriverplus/utils/pdf.py b/src/nodriverplus/utils/pdf.py
index 856252e..54dfc1c 100644
--- a/src/nodriverplus/utils/pdf.py
+++ b/src/nodriverplus/utils/pdf.py
@@ -20,12 +20,11 @@ import logging
 import re
 import base64
 from dataclasses import dataclass
-import fitz
 from bs4 import BeautifulSoup, Tag
 from .html import normalize_html
 
 
-logger = logging.getLogger(__name__)
+logger = logging.getLogger("nodriverplus.utils.pdf")
 
 DATA_URI_PATTERN = re.compile(r'src="(data:image/[^;]+;base64,)\s*(.*?)"', flags=re.DOTALL)
 BULLET_PATTERN = re.compile(r'^[\u2022\u25E6\u2219*\-–]\s+')  # • ◦ ∙ * - –
@@ -323,6 +322,11 @@ def pdf_to_html(pdf_bytes: bytes, url: str, embed_images = True):
     :return: cleaned html (no null bytes) ready for markdown.
     :rtype: str
     """
+    try:
+        import fitz # type: ignore
+    except ImportError:
+        raise ImportError("pymupdf is required for pdf support: uv add pymupdf")
+
     doc = fitz.open(stream=pdf_bytes, filetype="pdf")
     parts: list[str] = []
     for i, page in enumerate(doc):
diff --git a/src/nodriverplus/utils/urls.py b/src/nodriverplus/utils/urls.py
new file mode 100644
index 0000000..6412b92
--- /dev/null
+++ b/src/nodriverplus/utils/urls.py
@@ -0,0 +1,10 @@
+import urlcanon
+
+def fix_url(url: str):
+    """make the url more like how chrome would make it
+
+    :param url: raw url.
+    :return: fixed url
+    :rtype: str
+    """
+    return urlcanon.google.canonicalize(url).__str__()
diff --git a/tests/test_crawl_dedup.py b/tests/test_crawl_dedup.py
new file mode 100644
index 0000000..43f08f4
--- /dev/null
+++ b/tests/test_crawl_dedup.py
@@ -0,0 +1,42 @@
+import asyncio
+import pytest
+
+from nodriverplus import NodriverPlus, ScrapeResultHandler
+
+import logging
+logging.basicConfig(level=logging.INFO)
+
+pytestmark = [
+    pytest.mark.integration,
+    pytest.mark.browser,
+    pytest.mark.crawl,
+    pytest.mark.asyncio,
+]
+
+class Duper(ScrapeResultHandler):
+    async def links(self, result):
+        return ["https://example.com", "https://example.com"]
+
+async def _run_crawl(url: str, depth: int = 4):
+    ndp = NodriverPlus()
+    await ndp.start(headless=True)
+    result = await ndp.crawl(url, scrape_result_handler=Duper(), depth=depth, concurrency=2)
+    await ndp.stop()
+    return result
+
+async def collect_crawl(url: str = "https://example.com"):
+    return await _run_crawl(url)
+
+async def test_crawl_deduplication():
+    result = await collect_crawl()
+
+    # ensure no duplicate discovered links
+    assert len(result.links) == len(set(result.links)), "duplicate URLs found in result.links"
+    # ensure no duplicate successful links
+    assert len(result.successful_links) == len(set(result.successful_links)), "duplicate URLs found in successful_links"
+
+    # sanity: any link marked failed/timed out shouldn't also appear in successful twice
+    dup_success = [u for u in result.successful_links if result.successful_links.count(u) > 1]
+    assert not dup_success, f"duplicates in successful_links: {dup_success}"
+
+asyncio.run(test_crawl_deduplication())
\ No newline at end of file
diff --git a/tests/test_main.py b/tests/test_main.py
deleted file mode 100644
index 5f32096..0000000
--- a/tests/test_main.py
+++ /dev/null
@@ -1,79 +0,0 @@
-import asyncio
-import pytest
-
-from nodriverplus import (
-    CrawlResult, 
-    CrawlResultHandler,
-    NodriverPlus, 
-    NodriverPlusManager, 
-    ScrapeResponseHandler, 
-    ScrapeResponse, 
-)
-from nodriverplus.utils import to_markdown, get_type
-
-
-@pytest.mark.suite
-@pytest.mark.browser
-@pytest.mark.network
-@pytest.mark.manager
-@pytest.mark.crawl
-@pytest.mark.scrape
-@pytest.mark.bytes_
-@pytest.mark.stealth
-@pytest.mark.markdown
-@pytest.mark.asyncio
-async def test_crawl_with_bytes_and_stealth_and_manager():
-    has_html = False
-    is_bytes = False
-    bytes_conversion_successful = False
-    manager_success = False
-
-    # kick up a headless nodriver instance
-    # to test stealth
-    ndp = NodriverPlus()
-    await ndp.start(headless=True)
-
-    manager = NodriverPlusManager(ndp)
-    manager.start()
-
-    def handle_html(response: ScrapeResponse):
-        nonlocal has_html
-        # check if it generated HTML
-        if response.html:
-            has_html = True
-
-    def handle_bytes(response: ScrapeResponse):
-        nonlocal is_bytes, bytes_conversion_successful
-        bytes_type = get_type(response.bytes_)
-        print("bytes_type: ", bytes_type.__dict__)
-        # check if it scraped the bytes/pdf
-        if not bytes_type.supported:
-            return
-        is_bytes = True
-        # check if supported bytes to markdown works
-        markdown = to_markdown(response.bytes_)
-        bytes_conversion_successful = isinstance(markdown, str)
-
-    # attach the handlers to a new ScrapeResponseHandler
-    scrape_response_handler = ScrapeResponseHandler(html=handle_html, bytes_=handle_bytes)
-
-    def handle_result(_: CrawlResult):
-        nonlocal manager_success
-        manager_success = True
-
-    crawl_result_handler = CrawlResultHandler(handle=handle_result)
-    await manager.enqueue_crawl(
-        "https://investors.lockheedmartin.com/static-files/b5548c6b-71f9-4b58-b171-20accb1e8713",
-        scrape_response_handler,
-        crawl_result_handler=crawl_result_handler,
-        new_window=True
-    )
-    await manager.wait_for_queue(60)
-    # stop the nodriver instance
-    await manager.stop()
-    await ndp.stop()
-
-    assert has_html
-    assert is_bytes
-    assert bytes_conversion_successful
-    assert manager_success
\ No newline at end of file
